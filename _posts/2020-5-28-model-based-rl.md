---
layout: post
title: 모델 기반 강화학습에 대한 단상
comments: true
permalink: /model-based-rl/
---

강화학습에서는 행위자가 살고있는 세상이 동작하는 원리를 모델이라고 부른다. 행위자의 행동이 상황을 어떻게 변화시키는지 이해하고 있으면 모델을 알고 있다는 개념이다. 모델을 알고 있으면 무엇이 좋은가? 현재 시점에서 가능한 모든 미래들을 고려하여 그 중 최선일 것으로 사료되는 시나리오를 따라가면 된다. 단순한 트리 서치를 하면 된다는 말이다. 

당연하게도 대부분의 상황에서 scalable하게 적용할 수 있는 아이디어는 아니다. (그 닥터 스트레인지조차 고작 천 4백만개 언저리에서 포기하고 말았다.) 그런 상황에서 신경망의 도움을 받아 tree search를 하자는 이야기가 알파고가 차용한 MCTS (Monte Carlo Tree Search)이다. 바둑은 모델을 완전히 알고 있지 않은가? 플레이어가 착점하면 바둑판의 그 위치에 돌이 생기는 자명한 이치가 바로 바둑의 모델이다. 

인간의 학습, 내지는 의사 결정 과정에서 모델이 사용되는가? 물론이다. 가령 높은 곳에서 물체를 놓으면 얼마간 낙하한다는 사실과, 물체와 바닥이 충돌하는 짧은 순간 운동량의 교환이 일어난다는 사실을 이해한다. 덕분에 고가의 핸드폰을 굳이 깨 보지 않아도 케이스를 씌워 보호해야 한다는 판단이 가능하다. 연애 편지를 쓸 때 이를 읽는 대의 마음을 헤아려 수십번씩 지웠다 썼다 한다. 연애편지를 보내서 반응을 관찰하는 과정을 백만번씩 반복한다면 쓸만한 편지를 만들 수야 있겠지만 그럴 필요가 없다. 편지를 읽은 연인의 반응이 어떨 것인지에 대한 모델이 있기 때문에. (물론 그렇게 한다면 더 이상 편지를 건넬 연인도 없어지고 말 것이다.) 그 뿐인가, 인류는 끊임없는 지적 관찰을 통해 우주가 억겁의 세월을 거치며 어떻게 변할 것인지에 대해서도 어렴풋이 이해하고 나아갈 방향을 논한다. 이러한 모델을 만들어 나가는 것이 지능의 원천이라 주장하는 학자가 있을 만큼, 인간의 학습에 모델이 개입하는지 묻는것은 부자연스럽다. 오히려 모델이 존재하지 않는 강화학습 방법론들이 어색하게 느껴져야 하는 것이다.

그러나 소위 Model-Free와 Model-Based 방법론의 경계는 철학적으로 따져 보았을때 애매한 경향이 있다. 가령 모두의 친구인 Gym 패키지에 구현되어있는 CartPole 환경을 생각해보자. 컴퓨터 안에서 무한히 많이 시도해 볼 수 있기 때문에 어떤 Model-Free 방법론을 사용해도 막대기는 선다. 그런데 연구자의 목적에 대해 생각해보자. 연구자는 정말 컴퓨터 안의 CartPole을 세우고 싶었을 수 있고, 가상 세계에서 CartPole을 세우는 정책을 학습해 이를 현실 세계의 CartPole에 적용해 보고 싶었을 수도 있다. 전자의 경우에는 Model-Free, 후자의 경우에는 Model-Based이며 모델은 CartPole 환경이 되는 것이다.

모델이 어떤 행위를 해야할지 직접적으로 결정해주는건 아니므로 모델을 알고 있는 상황에서도 인간은 학습을 한다. 말하자면  모델은 시뮬레이션을 가능하게끔 해 주는 도구이고, 시뮬레이션을 통해 학습하는 매커니즘은 따로 있어야 한다는 것이다. 이런 맥락에서 Model-Based와 Model-Free 방법론은 자연스럽게 결합된다. 우선 실제 환경과 상호작용을 한 데이터로 시뮬레이터를 만들고, 그 시뮬레이터를 이용해 데이터를 무한히 만들어 그것으로 의사결정을 하는 신경망을 학습하면 된다. 이러한 방법론은 Richard Sutton에 의해 Dyna라는 이름으로 처음 제안되었는데 당연하게도 높은 효율성을 자랑한다. 

물론 신경망을 이용하겠다고 결정한 시점에서 모델은 부정확하고, 학습 주체는 모델이 높은 보상이 있을 것이라 틀리게 판단한 False Positive의 영역으로 빨려 들어갈 위험이 크다. 이를 해결하기 위해 시뮬레이션을 짧게 하는 MBPO (시뮬레이션을 길게 하면 모델의 오차가 누적되어 기하급수적으로 늘어나기 때문이다.), 시뮬레이터를 여러개 만들어 마치 다른 환경인 양 간주해 메타러닝을 이용해 각 환경에서 잘 행위하도록 하게끔 학습하는 MB-MPO와 같은 방법론들이 제안되곤 했다. 

비단 학습 뿐만이 아니다. Exploration과 Exploitation의 딜레마를 근본적으로 해결하기 위해서는 결국 학습 주체가 짧은 일회성 학습을 통해 접근할 수 없는 방대한 Prior가 필요하다. DQN의 $$\epsilon$$ -greedy나 DDPG의 OU noise는 좋지 않다.