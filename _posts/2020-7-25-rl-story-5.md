---
layout: post
title: 강화 학습 이야기 5
comments: true
permalink: /rl-story-5/
description: A story about maximization bias in q-learning.
---

Q-Learning 계열의 방법론들은 최적의 정책이 할 행동을 상상한다. 그렇게 상상한 $$a'$$를 가지고 시간차 학습을 한다. $$Q(s, a)$$와 $$r+\gamma Q(s', a')$$가 비슷해지도록 학습해 나간다. $$(s,$$ $$a,$$ $$r,$$ $$s')$$는 진짜 데이터, $$a'$$는 상상한 데이터이기에 Off-Policy라는 성질이 발생한다. 

그러나 한 가지 문제가 떠오른다. 최적의 정책이 할 행동을 상상하는 주체는 학습 중인 가치 함수 $$\hat{Q}$$이다. 그러니 당연하게도 정확하지 않다. 이 오차가 어떤 문제를 야기하는지 따져보자.
 

## 스톡데일 패러독스

학습 중인 $$\hat{Q}$$를 근거로 가장 좋은 $$a'$$를 선택한다.

$$a' = \mathrm{argmax}_a \hat{Q}(s', a)$$

그렇게 선택한 $$a'$$로 $$r+\gamma \hat{Q}(s', a')$$를 계산해 이것이 마치 정답인 양 $$\hat{Q}(s,a)$$와 가까워지게 만든다. 함정은 $$a' = \mathrm{argmax}_a \hat{Q}(s', a)$$에 있다. 한 가지 예시를 들어보자. 가령 가능한 행동들이 $$a_1,$$ $$a_2,$$ $$a_3$$ 세 가지이고, 최적 정책의 가치 함수가 $$s'$$에서 아래와 같다고 해 보자. 

$$Q^*(s', a_1)=2.6$$

$$Q^*(s', a_2)=2$$

$$Q^*(s', a_3)=2.5$$

그러니 $$s'$$에서 최적의 정책은 $$a_1$$만을 할 테다. 그 때의 가치는 2.6이다. 그런데 문제는 우리가 $$Q^*$$를 알지 못한다는데 있다. 아직 학습 중이기 때문일 수도 있지만 신경망을 이용해 $$Q^*$$를 근사하려 하고 있는 상황이기 때문에 [본질적으로 부정확](https://en.wikipedia.org/wiki/Uncertainty_quantification#Aleatoric_and_epistemic_uncertainty)할 수 밖에 없다. 가령 약간의 오차가 개입했다고 해 보자.

$$\hat{Q}(s', a_1)=2.59$$

$$\hat{Q}(s', a_2)=1.7$$

$$\hat{Q}(s', a_3)=2.64$$

$$\hat{Q}$$를 근거로 판단한 최적의 행동은 $$a_3$$이고, 그 때의 추정 가치는 $$2.64$$이다. 그러나 오차가 없었다면 최적의 행동은 $$a_1$$이고, 그 때의 가치는 $$2.6$$이라 추정했어야 한다. 이는 $$\hat{Q}(s,a)$$의 정답 역할을 할 $$r+\gamma \hat{Q}(s', a')$$를 참값보다 크게 추정하는 잘못된 판단으로 이어진다. 항상 정답을 참값보다 크게 추정하니 $$\hat{Q}$$가 전반적으로 참값 $$Q^*$$에 비해 높게 학습되는 편향을 야기한다. 

$$a_1$$과 $$a_2$$의 가치는 참값보다 낮게 추정했다. 전체적인 오차는 낮은 방향으로 작용한 셈이다.  그럼에도 불구하고 편향이 발생한다는 사실이 재밌다. 이러한 편향을 Maximization Bias라 부른다. 불공평한 녀석이다.

이러한 편향 탓에 학습한 $$\hat{Q}$$와 참값 $$Q^{*}$$이 달라진다. 그러나 생각보다 큰 문제는 아닐 수 있다. $$Q^{*}$$가 아닌 $$\pi^{*}$$를 추정하는게 우리의 목적이기 때문이다. 행위자에게는 행동의 근거가 될 정책만 있으면 충분하다. 물론 $$Q^{*}$$를 정확히 추정하면 좋다. 그러나 참값보다 일관적으로 높게 학습한다면, 예컨데 어떤 상수 $$C$$에 대해 $$Q^{*} + C$$를 학습하는 셈이라면 상관없다. 결국 가치 함수로부터 유도되는 정책은 같다.

$$\pi^{*} = \mathrm{argmax} _a Q^{*} (s, a) +C=\mathrm{argmax} _a Q^{*} (s, a) $$

당연하게도 굳이 이렇게 일관적으로 잘못할 이유가 없다. 그러니 학습한 가치 함수로부터 유도되는 정책이 최적 정책과 달라진다. 따라서 Q-Learning 기반의 방법론들은 이러한 편향을 해결해 줄 처방이 필요하다.

{% include image.html url="/images/rl-story-5-1.png" description="van Hasselt et al., 2015" %}

위 도식은 가치 함수가 추정한 가치, 아래 도식은 그러한 가치 함수로부터 정책을 유도해 행동했을 때 실제로 얻은 누적 보상을 나타낸다. 말했듯이 최적 정책의 가치 함수를 참값보다 높게 측정한다는 사실 자체가 큰 문제는 아닐 수 있다. 그러나 그런 현상이 시작되는 순간부터 실제 성능은 감소하기 시작한다. 낙관주의의 비극이다.

이제 등장할 Double DQN의 경우에는 이러한 편향이 현저히 적어진다. 편향이 적을 뿐 아니라 실제 환경에서 얻는 누적 보상도 높아지고 학습이 안정화된다. 편향을 해결하면 달콤한 보상이 따르리라는 암시렸다.

## 쌍성계

편향이 왜 발생하는지 생각해보자. $$\hat{Q}$$는 무엇을 근거로 $$a_3$$이 최적의 행동이라고 판단했는가? 말장난 같지만 $$\hat{Q}$$ 자신이다. 그러니 $$\hat{Q}$$는 억울하다. 최적의 행동을 말하래서 말했고 그 근거를 대래서 댔다. 편향이 발생하는건 $$\hat{Q}$$의 잘못이 아니다.

비유를 들어보자. 가령 두 신경망을 학습시킨다고 해 보자. 신경망이 작동하게끔 하기 위해 우선은 학습 데이터에 끼워 맞춰야 한다. 이미 학습 데이터에 맞추었기 때문에 당연히 일반적인 데이터보다 학습 데이터에 더 잘 맞을테다. 그러니 학습 데이터를 지표로 사용한다면 실제보다 낙관적인 결론에 이른다. 그러니 학습 데이터와 상호 배타적인 검증 데이터를 들고와 모형을 선택한다.

거창한 비유를 들었으나 사실 당연한 이야기다. 놀이공원에서 롤러코스터를 타는 사람들의 키 평균은 한국인 평균 키보다 높다. 애초에 키가 작은 사람은 롤러코스터를 탈 수 없기 때문이다. 한국인 평균 키의 추정치가 필요한 자리에 이를 사용한다면 [편향](https://en.wikipedia.org/wiki/Sampling_bias)이 생긴다.

마찬가지다. $$\hat{Q}$$에게 최적의 행동을 물었다면 그것으로 끝내야 한다. 그 행동의 가치를 다시금 $$\hat{Q}$$에게 물으면 필연적으로 참값보다 높은 값을 이야기하는 경향이 발생한다. $$\hat{Q}$$가 최적이라고 판단했던 행동의 가치를 객관적으로 일러줄 제 2의 가치 함수가 필요하다.

<details id='inside'>
<summary>Talk is cheap. Show me the code. ┓ </summary>
<div markdown="1">

```python
import gym
import torch
import torch.nn as nn
import numpy as np
from collections import deque
import random
import copy

env = gym.make("CartPole-v1")

STATE_DIM = env.observation_space.shape[0]
HIDDEN_DIM = 200
ACTION_DIM = env.action_space.n
LEARNING_RATE = 0.001
GAMMA = 0.99
EPSILON = 0.03
TAU = 0.005
BATCH_SIZE = 512
BUFFER_SIZE = 10000

def to_tensor(array):
    if array.ndim == 1:
        return torch.tensor(array, dtype=torch.float32).unsqueeze(0)
    else:
        return torch.tensor(array, dtype=torch.float32)

class ReplayBuffer(object):
    def __init__(self, maxlen=10000):
        self.memory = deque(maxlen=maxlen)
    
    def store(self, state, action, next_state, reward, done):
        self.memory.append([state, action, next_state, reward, done])

    def sample(self, batch_size):
        batch_indices = random.sample(range(len(self)), batch_size)
        batch = [self.memory[idx] for idx in batch_indices]
        batch = {'states': torch.tensor([data[0] for data in batch], dtype=torch.float32),
                 'actions': torch.tensor([data[1] for data in batch], dtype=torch.long).unsqueeze(-1),
                 'next_states': torch.tensor([data[2] for data in batch], dtype=torch.float32),
                 'rewards': torch.tensor([data[3] for data in batch], dtype=torch.float32).unsqueeze(-1),
                 'dones': torch.tensor([data[4] for data in batch], dtype=torch.float32).unsqueeze(-1)}
        return batch

    def __len__(self):
        return len(self.memory)

q1 = nn.Sequential(nn.Linear(STATE_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, ACTION_DIM))

q2 = nn.Sequential(nn.Linear(STATE_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, ACTION_DIM))

q1_optimizer = torch.optim.AdamW(q1.parameters(), lr = LEARNING_RATE)
q2_optimizer = torch.optim.AdamW(q2.parameters(), lr = LEARNING_RATE)

buffer = ReplayBuffer(BUFFER_SIZE)

for i in range(1, 10000):
    state = env.reset()
    done = False

    counter = 0
    while not done:
        counter += 1
        if random.random() < EPSILON:
            chosen_action = random.choice(range(2))
        else:
            chosen_action = q1(to_tensor(state)).argmax().item()
            
        next_state, reward, done, _ = env.step(chosen_action)

        buffer.store(state, chosen_action, next_state, reward, done*1)

        state = next_state

    if len(buffer) < BATCH_SIZE:
        continue
    
    batch = buffer.sample(BATCH_SIZE)
    q1_expected_state_action_values = q1(batch['states']).gather(1, batch['actions'])
    q2_expected_state_action_values = q2(batch['states']).gather(1, batch['actions'])

    augmented_next_actions = q1(batch['next_states']).argmax(1, keepdim=True)
    target = batch['rewards'] + q2(batch['next_states']).gather(1, augmented_next_actions) * GAMMA * (1-batch['dones'])

    loss = (q1_expected_state_action_values - target.detach()).pow(2).mean() +\
           (q2_expected_state_action_values - target.detach()).pow(2).mean()
            
    q1_optimizer.zero_grad()
    q2_optimizer.zero_grad()

    loss.backward()

    q1_optimizer.step()
    q2_optimizer.step()
    """
    for param, param_target in zip(q.parameters(), q_target.parameters()):
        param_target.data.copy_((1-TAU) * param_target.data+ TAU * param.data)
    """
    if i % 100 == 0:
        performance = 0
        for j in range(10):
            state = env.reset()
            done = False
            while not done:
                chosen_action = q1(to_tensor(state)).argmax().item()
                next_state, reward, done, _ = env.step(chosen_action)
                state = next_state
                performance += reward
        print(f"{i}th Trial -> {performance/10}")
```
    
</div>
</details>

$$Q_1$$에게 최적의 행동을 고르게 두고, $$Q_2$$로 그 행동에 대한 가치를 추정하게 만든다. 그렇게 계산한 정답으로 $$Q_1$$과 $$Q_2$$를 동시에 학습시킨다. 생각보다 문제가 간단히 해결된다. 원한다면 $$Q_1$$과 $$Q_2$$이 역할을 번갈아 맡도록 구현해도 된다. 자유다.

지난 이야기 말미에 등장했던 Target Network를 잊었다. 원래의 가치 함수를 천천히 따라오는 Target Network로 정답을 계산해 학습 과정을 안정적으로 만들자고 했다. 이 또한 구현해주자. 두 신경망 모두의 Target Network를 만들어 주어야 한다. 네 신경망이 조화를 이루며 작동한다. 다소 부담스럽다.

<details id='inside'>
<summary>Talk is cheap. Show me the code. ┓ </summary>
<div markdown="1">

```python
import gym
import torch
import torch.nn as nn
import numpy as np
from collections import deque
import random
import copy

env = gym.make("CartPole-v1")

STATE_DIM = env.observation_space.shape[0]
HIDDEN_DIM = 200
ACTION_DIM = env.action_space.n
LEARNING_RATE = 0.001
GAMMA = 0.99
EPSILON = 0.03
TAU = 0.005
BATCH_SIZE = 512
BUFFER_SIZE = 10000

def to_tensor(array):
    if array.ndim == 1:
        return torch.tensor(array, dtype=torch.float32).unsqueeze(0)
    else:
        return torch.tensor(array, dtype=torch.float32)

class ReplayBuffer(object):
    def __init__(self, maxlen=10000):
        self.memory = deque(maxlen=maxlen)
    
    def store(self, state, action, next_state, reward, done):
        self.memory.append([state, action, next_state, reward, done])

    def sample(self, batch_size):
        batch_indices = random.sample(range(len(self)), batch_size)
        batch = [self.memory[idx] for idx in batch_indices]
        batch = {'states': torch.tensor([data[0] for data in batch], dtype=torch.float32),
                 'actions': torch.tensor([data[1] for data in batch], dtype=torch.long).unsqueeze(-1),
                 'next_states': torch.tensor([data[2] for data in batch], dtype=torch.float32),
                 'rewards': torch.tensor([data[3] for data in batch], dtype=torch.float32).unsqueeze(-1),
                 'dones': torch.tensor([data[4] for data in batch], dtype=torch.float32).unsqueeze(-1)}
        return batch

    def __len__(self):
        return len(self.memory)

q1 = nn.Sequential(nn.Linear(STATE_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, ACTION_DIM))

q1_target = copy.deepcopy(q1)

q2 = nn.Sequential(nn.Linear(STATE_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, ACTION_DIM))

q2_target = copy.deepcopy(q2)

q1_optimizer = torch.optim.AdamW(q1.parameters(), lr = LEARNING_RATE)
q2_optimizer = torch.optim.AdamW(q2.parameters(), lr = LEARNING_RATE)

buffer = ReplayBuffer(BUFFER_SIZE)

for i in range(1, 10000):
    state = env.reset()
    done = False

    counter = 0
    while not done:
        counter += 1
        if random.random() < EPSILON:
            chosen_action = random.choice(range(2))
        else:
            chosen_action = q1(to_tensor(state)).argmax().item()
            
        next_state, reward, done, _ = env.step(chosen_action)

        buffer.store(state, chosen_action, next_state, reward, done*1)

        state = next_state

    if len(buffer) < BATCH_SIZE:
        continue
    
    batch = buffer.sample(BATCH_SIZE)
    q1_expected_state_action_values = q1(batch['states']).gather(1, batch['actions'])
    q2_expected_state_action_values = q2(batch['states']).gather(1, batch['actions'])

    augmented_next_actions = q1_target(batch['next_states']).argmax(1, keepdim=True)
    target = batch['rewards'] + q2_target(batch['next_states']).gather(1, augmented_next_actions) * GAMMA * (1-batch['dones'])

    loss = (q1_expected_state_action_values - target.detach()).pow(2).mean() +\
           (q2_expected_state_action_values - target.detach()).pow(2).mean()
            
    q1_optimizer.zero_grad()
    q2_optimizer.zero_grad()

    loss.backward()

    q1_optimizer.step()
    q2_optimizer.step()
    
    for param, param_target in zip(q1.parameters(), q1_target.parameters()):
        param_target.data.copy_((1-TAU) * param_target.data+ TAU * param.data)

    for param, param_target in zip(q2.parameters(), q2_target.parameters()):
        param_target.data.copy_((1-TAU) * param_target.data+ TAU * param.data)

    if i % 100 == 0:
        performance = 0
        for j in range(10):
            state = env.reset()
            done = False
            while not done:
                chosen_action = q1(to_tensor(state)).argmax().item()
                next_state, reward, done, _ = env.step(chosen_action)
                state = next_state
                performance += reward
        print(f"{i}th Trial -> {performance/10}")
```
    
</div>
</details>



Q-Learning에서 발생하는 이런 편향은 [오래 전](https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf)부터 [알려져 있던](https://papers.nips.cc/paper/3964-double-q-learning) 현상이다.  DQN에서도 마찬가지로 이런 현상이 관측되었고, 얼마 가지 않아 딥마인드의 연구자들에 의해 [Double DQN](https://arxiv.org/abs/1509.06461)이라는 이름의 개선체가 제안되었다. 이 때 제안된 방법론은 위의 구현체보다는 단순하다. $$Q_2$$를 그냥 $$Q_1$$의 Target Network로 간주한다. 서로 완전히 독립적이지는 않지만 어쨌든 다른 신경망이니 제 2의 신경망까지 도입하진 말자는 아이디어다.

그렇게 할 요량이라면 지난 이야기의 DQN 구현체에서 정말로 단 한 단어만 지우면 된다. 일반적으로 Double DQN이라고 부르는건 아래의 구현체다. 그러나 더 일반적인 구현체를 먼저 만들어본 이유가 있다. 이후 이야기할 TD3, 내지는 SAC의 구현체와 얼개가 같기 때문이다.

<details id='inside'>
<summary>Talk is cheap. Show me the code. ┓ </summary>
<div markdown="1">

```python
import gym
import torch
import torch.nn as nn
import numpy as np
from collections import deque
import random
import copy

env = gym.make("CartPole-v1")

STATE_DIM = env.observation_space.shape[0]
HIDDEN_DIM = 200
ACTION_DIM = env.action_space.n
LEARNING_RATE = 0.001
GAMMA = 0.99
EPSILON = 0.03
TAU = 0.005
BATCH_SIZE = 512
BUFFER_SIZE = 10000

def to_tensor(array):
    if array.ndim == 1:
        return torch.tensor(array, dtype=torch.float32).unsqueeze(0)
    else:
        return torch.tensor(array, dtype=torch.float32)

class ReplayBuffer(object):
    def __init__(self, maxlen=10000):
        self.memory = deque(maxlen=maxlen)
    
    def store(self, state, action, next_state, reward, done):
        self.memory.append([state, action, next_state, reward, done])

    def sample(self, batch_size):
        batch_indices = random.sample(range(len(self)), batch_size)
        batch = [self.memory[idx] for idx in batch_indices]
        batch = {'states': torch.tensor([data[0] for data in batch], dtype=torch.float32),
                 'actions': torch.tensor([data[1] for data in batch], dtype=torch.long).unsqueeze(-1),
                 'next_states': torch.tensor([data[2] for data in batch], dtype=torch.float32),
                 'rewards': torch.tensor([data[3] for data in batch], dtype=torch.float32).unsqueeze(-1),
                 'dones': torch.tensor([data[4] for data in batch], dtype=torch.float32).unsqueeze(-1)}
        return batch

    def __len__(self):
        return len(self.memory)

q = nn.Sequential(nn.Linear(STATE_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, ACTION_DIM))

q_target = copy.deepcopy(q)

optimizer = torch.optim.AdamW(q.parameters(), lr = LEARNING_RATE)
buffer = ReplayBuffer(BUFFER_SIZE)

for i in range(1, 10000):
    state = env.reset()
    done = False

    counter = 0
    while not done:
        counter += 1
        if random.random() < EPSILON:
            chosen_action = random.choice(range(2))
        else:
            chosen_action = q(to_tensor(state)).argmax().item()
            
        next_state, reward, done, _ = env.step(chosen_action)

        buffer.store(state, chosen_action, next_state, reward, done*1)

        state = next_state

    if len(buffer) < BATCH_SIZE:
        continue
    
    batch = buffer.sample(BATCH_SIZE)
    expected_state_action_values = q(batch['states']).gather(1, batch['actions'])

    augmented_next_actions = q(batch['next_states']).argmax(1, keepdim=True)
    target = batch['rewards'] + q_target(batch['next_states']).gather(1, augmented_next_actions) * GAMMA * (1-batch['dones'])
    loss = (expected_state_action_values - target.detach()).pow(2).mean()
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    for param, param_target in zip(q.parameters(), q_target.parameters()):
        param_target.data.copy_((1-TAU) * param_target.data+ TAU * param.data)

    if i % 100 == 0:
        performance = 0
        for j in range(10):
            state = env.reset()
            done = False
            while not done:
                chosen_action = q(to_tensor(state)).argmax().item()
                next_state, reward, done, _ = env.step(chosen_action)
                state = next_state
                performance += reward
        print(f"{i}th Trial -> {performance/10}")
```
    
</div>
</details>

## Unplugged (작성중)

우리의 목표인 오프라인 강화학습에서는 이러한 편향이 더 아프게 다가온다. 대부분의 오프라인 강화학습 방법론들은 Q-Learning 계열의 방법론들을 골자로 한다. Q-Learning은 Off-Policy이니 오프라인 강화학습을 위한 응당 합리적인 선택지인 듯 보인다. 그럼 Q-Learning을 바로 적용하면 안 되는걸까?

그러니 미지의 정책 $$\pi^{\beta}$$가 만든 순서쌍 $$(s,$$ $$a,$$ $$r,$$ $$s')$$들을 이용한다. 일반적으로 수집하기 간단한 데이터다. 
