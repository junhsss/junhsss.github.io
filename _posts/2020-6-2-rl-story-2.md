---
layout: post
title: 강화 학습 이야기 2
comments: true
permalink: /rl-story-2/
description: A story about the monte carlo estimation of value functions.
---

도대체 이게 무슨 탁상 공론인가? 미래에 받을 수 있는 누적 보상의 기댓값이라니. 가능한 미래들을 전부 열거해 보기라도 하겠다는 말인가? [지난 이야기]({% post_url 2020-6-1-rl-story-1 %})를 읽으며 이런 생각이 들었다면 정상이다. 가치 함수가 바로 그렇게 정의되는 양이다. 정책 $$\pi$$가 어떤 행동을 해야 하는지 알려준다면 가치 함수는 그러한 행동 이후에 얼마만큼의 누적 보상을 기대할 수 있는지 알려준다. 정의를 되짚어보자.

- 상태 $$s$$에서 행위 $$a$$를 했을 때 미래에 받을 수 있는 누적 보상의 기댓값을 $$Q(s, a)$$,
- 상태 $$s$$에 놓여 있을 때 미래에 받을 수 있는 누적 보상의 기댓값을 $$V(s)$$라 한다.

이 정의에서 미래라는 말은 모호하다. 누가 만드는 미래인가? 같은 동네에서 나고 자랐더라도 내가 만드는 미래와 옆집 철수가 만드는 미래는 다르다. 그래서 가치 함수를 정의할 땐 행위자의 정책 $$\pi$$가 개입한다. 윗 첨자로 어떤 정책을 따를 때의 가치 함수인지 알려 주어야 명확하다. $$Q^{\pi}(s, a)$$, $$V^{\pi}(s)$$ 이렇게 쓴다.

$$Q^{\pi}(s, a)=\mathbb{E}_{\pi} \left[  \sum_{t'=t} r_{t'} \mid s_t = s, a_t = a \right]$$

$$V^{\pi}(s) =\mathbb{E}_{\pi} \left[  \sum_{t'=t} r_{t'} \mid s_t = s \right]$$

상황과 행동의 가치 함수 $$Q^{\pi}$$의 경우를 생각해 보자. 시점 $$t$$에서 상황 $$s$$에 놓여 있다. 그 때 $$a$$라는 행동을 하라는 조건이 주어졌다. 거기서부터 행위자는 정책 $$\pi$$를 따라 무수히 많은 미래들을 만들어본다. (이후로는 행위자와 정책 $$\pi$$를 동일시하겠다.) 그렇게 만들어진 여러 미래들에서 계산되는 누적 보상의 평균을 $$Q^{\pi}$$로 정한다고 받아들이면 된다. 상황의 가치 함수 $$V^{\pi}$$도 마찬가지다. 상황 $$s$$에서 정책 $$\pi$$를 따라 무수히 많은 미래를 만들고 누적 보상의 평균을 계산한다. 다만 처음 행동까지 정책 $$\pi$$에게 맡겨 버린다는 차이가 있다.

$$s$$와 $$a$$는 조건으로 주어지는 데이터라는 점을 짚고 넘어가야 한다. 정책 $$\pi$$를 고수할 때 현실적으로 맞닥뜨릴 일이 없는 $$s$$와 $$a$$에 대해서도 이론상 $$Q^{\pi}(s, a)$$ 는 정의된다. 강원도 토박이 철수($$\pi$$)가 세렝게티 한복판($$s$$)에서 삼겹살을 구워먹는게($$a$$) 얼마나 현명한 일인지($$Q^{\pi}(s, a)$$) 짐작이야 해 볼 수 있다는 말이다. 물론 $$Q^{\pi}(s, a)$$를 추정해보고 싶다면 세렝게티 삼겹살 파티에 철수를 데려다 놓고 미래를 관찰해보아야 한다. 현재로서는 유일한 방법이다. 타임머신이 있는 경우에는 여러번 관찰해보면 더 좋다.

만약 신경망으로 $$Q^{\pi}(s, a)$$를 표현하고 싶다면 간단하다. $$s$$와 $$a$$를 입력으로, 이후 수많은 미래에서 계산한 누적 보상들의 평균을 정답으로 두고 학습시키면 된다. 미래를 여러 번 관측할 수 있다면 단순한 지도학습에 불과하다. 그러나 인과율을 존중하자. 미래를 여러 번 관측할 수는 없다. 그러니 현실적으로는 단 하나의 미래에서 얻은 누적 보상을 정답으로 사용할 수 밖에 없다. 정말로 그렇게 구현한다. 그래도 작동을 퍽 잘 한다. (사족: 약간은 추상적인 이야기지만 어차피 평균 제곱 오차가 작아지도록 학습시키면 가능한 값들이 이루는 분포의 기댓값을 학습하는 셈이니 괜찮다. [이 교재](https://web.stanford.edu/~hastie/ElemStatLearn/)의 18 페이지를 참고하자.)

정책 $$\pi$$를 따를 때의 가치 함수 $$Q^{\pi}$$를 알고 싶은 상황이다. 그러니 정답을 계산할 때 필요한 미래는 반드시 $$\pi$$가 만든 미래여야만 한다는 사실에 유의하자. 다른 정책 $$\pi^{\beta}$$가 만든 미래를 이용해 정답을 계산한다면 $$Q^{\pi^{\beta}}$$를 알게 되는 셈이다. 아무런 의미가 없다.

그렇다면 행위자와 정책 $$\pi$$가 어떻게 미래를 만들어 가는지 따져볼 필요가 있다. 위에서 이야기했듯이 $$s_t$$에서 $$a_t$$를 하는건 조건이다. 행위자를 $$s_t$$로 끌어와 강제로 $$a_t$$를 시켜본다. $$s_t$$에서 $$a_t$$를 했을 때 어떤 미래 $$s_{t+1}$$를 마주하게 될는지는 모른다. 그러나 적어도 행위자의 소관은 아니다. 행위자는 이미 행동을 억지로 해 버린 상황이다. 행위자는 손을 놓고 보상 $$r_t$$와 함께 $$s_{t+1}$$를 겸허히 맞이한다. 그 이후나 되어야 비로소 행위자의 이성이 개입한다. 행위자는 정책 $$\pi$$에 따라 $$a_{t+1}$$를 결정한다. 왜 행위자가 $$a_{t+1}$$를 결정하는가? $$Q^{\pi}(s_t, a_t)$$의 정의는 $$s_t$$에서 $$a_t$$를 했을 때, 정책 $$\pi$$를 따라가면 미래에 받을 수 있는 누적 보상의 기댓값이라고 했었다. 정의에 충실하게 $$s_t$$와 $$a_t$$ 이후로 정책 $$\pi$$에 따라 행동하고 있다.

$$Q^{\pi}(s_t, a_t)$$를 학습시키기 위해 만들어질 데이터를 시간 순서에 따라 써 보자. 

$$s_t$$ $$\rightarrow$$ $$a_t$$ $$\rightarrow$$ $${\color{#642EFE} {r_t}}$$ $$\rightarrow$$ $$s_{t+1}$$ $$\rightarrow$$ $$a_{t+1}$$ $$\rightarrow$$ $${\color{#642EFE} {r_{t+1}}}$$ $$\rightarrow$$ $$s_{t+2}$$ $$\rightarrow$$ $$\dots$$

$$s_t$$와 $$a_t$$에서 시작하는 하나의 미래만 고려해 정답을 만들기로 했었다. 그러니 입력은 $$s_t$$와 $$a_t$$, 정답은 $$\sum_{t'=t}r_{t'}$$로 잡고 신경망을 훈련시키면 충분하다. 


<details>
<summary>Talk is cheap. Show me the code. ┓ </summary>
<div markdown="1">

```python
import gym
import torch
import torch.nn as nn

env = gym.make("CartPole-v1")

STATE_DIM = env.observation_space.shape[0]
ACTION_DIM = env.action_space.n

HIDDEN_DIM = 200
LEARNING_RATE = 0.0005

def to_tensor(array):
    if array.ndim == 1:
        return torch.tensor(array, dtype=torch.float32).unsqueeze(0)
    else:
        return torch.tensor(array, dtype=torch.float32)

def calculate_returns(rewards):
    R = 0
    returns = []
    for r in rewards[::-1]:
        R = R + r
        returns.append(R)
    return returns[::-1]

policy = nn.Sequential(nn.Linear(STATE_DIM, HIDDEN_DIM),
                       nn.LeakyReLU(),
                       nn.Linear(HIDDEN_DIM, HIDDEN_DIM),
                       nn.LeakyReLU(),
                       nn.Linear(HIDDEN_DIM, ACTION_DIM),
                       nn.Softmax(dim=1))

# Note that it's handy if we implement Q in the form of Q(s)[a] when the action space is discrete. 
# We could still implement Q as Q(s, a), but it's unnecessary. (one-hot encode actions or whatever.)
# This sutblety will be elaborated further later. It's a key difference between DQN-like and DDPQ-like algorithms.
q = nn.Sequential(nn.Linear(STATE_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, ACTION_DIM))

# We need separate optimizers for actor and critic respectively.
optimizer_actor = torch.optim.AdamW(policy.parameters(), lr = LEARNING_RATE)
optimizer_critic = torch.optim.AdamW(q.parameters(), lr = 2*LEARNING_RATE)

track_performance = 0
for i in range(1, 10000):
    STATE_MEMORY = []
    ACTION_MEMORY = []
    REWARD_MEMORY = []

    state = env.reset()
    done = False
    # Deactivate PyTorch autograd engine as we don't need to compute gradient here.
    with torch.no_grad(): 
        while not done:
            action_probs = policy(to_tensor(state))
            sampled_action = torch.multinomial(action_probs, 1).item()
            next_state, reward, done, _ = env.step(sampled_action)

            STATE_MEMORY.append(state)
            ACTION_MEMORY.append(sampled_action)
            REWARD_MEMORY.append(reward)

            state = next_state

    STATE_TENSORS = torch.tensor(STATE_MEMORY, dtype=torch.float32)
    ACTION_TENSORS = torch.tensor(ACTION_MEMORY, dtype=torch.long).unsqueeze(-1)
    RETURN_TENSORS = torch.tensor(calculate_returns(REWARD_MEMORY),\
                                  dtype=torch.float32).unsqueeze(-1)

    """
    Critic (State-action value) learning phase
    """
    # Q(s)[a] ≈ cumulative rewards after a in s.
    expected_state_action_values = q(STATE_TENSORS).gather(1, ACTION_TENSORS)
    single_true_cumulative_rewards = RETURN_TENSORS
    critic_loss = (expected_state_action_values - single_true_cumulative_rewards).pow(2).mean()

    optimizer_critic.zero_grad()
    critic_loss.backward()
    optimizer_critic.step()

    """
    Actor (Policy) learning phase
    """
    likelihoods = policy(STATE_TENSORS).gather(1, ACTION_TENSORS)
    log_likelihoods = torch.log(likelihoods) 

    # Log-likelihoods are now weighted by the state-action values calculated from q. i.e. Q(s)[a]
    with torch.no_grad():
        assigned_credits = q(STATE_TENSORS).gather(1, ACTION_TENSORS)

    weighted_log_likelihoods = log_likelihoods * assigned_credits

    pseudo_loss = -weighted_log_likelihoods.mean()
    optimizer_actor.zero_grad()
    pseudo_loss.backward()
    optimizer_actor.step()


    track_performance += sum(REWARD_MEMORY)

    if i % 100 == 0:
        print(f"{i}th Trial -> {track_performance/100}")
        track_performance = 0
```
    
</div>
</details>

## 천지 개벽

$$Q^{\pi}(s_t, a_t)$$를 추정하고 싶다면 이 길고 긴 사슬과 같은 데이터는 행위자가 만들었어야 한다고 강조했다. 그러나 행위자가 데이터에 개입한 흔적은 $$a_{t+1}$$부터 드러난다. 그러니 그 전까지는 행위자와 무관한 데이터다. 만약 $$Q^{\pi}$$를 학습시키기 위한 어떤 방법론이 $$a_{t+1}$$ 이후의 데이터를 요구하지 않는다면 이론 상 행위자와 무관한 데이터로도 학습이 가능하다. 대단히 중요한 관찰이다. 천지가 개벽할 일이다. 정책 $$\pi$$의 가치 함수를 $$\pi$$의 흔적이 없는 데이터로도 알아낼 수 있다니. 철수가 만드는 가치를 영희의 행동만으로 알아낼 수 있다니! 영리한 독자라면 의구심이 들어야 정상이다. 이 이야기가 등장하려면 조금 더 기다려야 한다. 그러니 잠깐 잊고 원래 이야기를 이어가자.

## 배우 수업

[지난 이야기]({% post_url 2020-6-1-rl-story-1 %})에서 말했듯이 가치 함수 $$Q^{\pi}$$를 알고 있다면 행위자의 정책 $$\pi$$를 교정할 때 각 행동들의 기여도를 더 합리적으로 판단할 수 있다. 이를 알고리즘으로 써 보자.

1. 정책 $$\pi$$를 이용해 데이터를 모은다.
2. 정책 $$\pi$$의 가치 함수 $$Q^{\pi}$$를 추정한다.
3. 추정한 가치함수 $$Q^{\pi}$$를 근거로 정책 $$\pi$$의 행동을 교정한다.

그런데 3단계에서 정책 $$\pi$$가 조금이라도 변하는 순간 가치 함수 $$Q^{\pi}$$는 무용지물이 된다. 정책이 변했으니 가치 함수를 다시 찾아야 한다. 절망적이다. 그래도 경사 하강법은 정책을 크게 바꾸지 않으니 다행이다. 덕분에 $$\pi$$와 $$Q^{\pi}$$를 한 번씩 번갈아 학습시키기만 해도 충분하다. 바뀌기 전 정책에 대한 가치 함수를 초깃값 삼아 바뀐 후의 정책에 대한 가치 함수를 찾는 셈이다. 이런 미묘한 부분들까지 꼼꼼히 짚고 넘어가야 안 헷갈린다.

정책 $$\pi$$와 가치 함수 $$Q^{\pi}$$을 표현하는데는 독립된 두 신경망을 사용한다. 그러니 태생적으로 불안정 할 수 밖에 없다. 지난 이야기에서 논했던대로 $$V^{\pi}$$ 까지 있다면 더 나은 신뢰 할당을 할 수 있다. $$V^{\pi}$$를 표현하는 세 번째 신경망을 도입하면 된다. 

<details id='inside'>
<summary>Talk is cheap. Show me the code. ┓ </summary>
<div markdown="1">

```python
"""
Note that we NEVER do this in practice. 
Just a pedagogical practice. :)
"""
import gym
import torch
import torch.nn as nn

env = gym.make("CartPole-v1")

STATE_DIM = env.observation_space.shape[0]
ACTION_DIM = env.action_space.n

HIDDEN_DIM = 200
LEARNING_RATE = 0.0001

def to_tensor(array):
    if array.ndim == 1:
        return torch.tensor(array, dtype=torch.float32).unsqueeze(0)
    else:
        return torch.tensor(array, dtype=torch.float32)

def calculate_returns(rewards):
    R = 0
    returns = []
    for r in rewards[::-1]:
        R = R + r
        returns.append(R)
    return returns[::-1]

# We now need 3 neural nets and optimizers. 
policy = nn.Sequential(nn.Linear(STATE_DIM, HIDDEN_DIM),
                       nn.LeakyReLU(),
                       nn.Linear(HIDDEN_DIM, HIDDEN_DIM),
                       nn.LeakyReLU(),
                       nn.Linear(HIDDEN_DIM, ACTION_DIM),
                       nn.Softmax(dim=1))

q = nn.Sequential(nn.Linear(STATE_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, ACTION_DIM))

v = nn.Sequential(nn.Linear(STATE_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, 1))

optimizer_actor = torch.optim.AdamW(policy.parameters(), lr = LEARNING_RATE)
optimizer_critic_q = torch.optim.AdamW(q.parameters(), lr = 5*LEARNING_RATE)
optimizer_critic_v = torch.optim.AdamW(v.parameters(), lr = 5*LEARNING_RATE)

track_performance = 0
for i in range(1, 10000):
    STATE_MEMORY = []
    ACTION_MEMORY = []
    REWARD_MEMORY = []

    state = env.reset()
    done = False
    with torch.no_grad():
        while not done:
            action_probs = policy(to_tensor(state))
            sampled_action = torch.multinomial(action_probs, 1).item()
            next_state, reward, done, _ = env.step(sampled_action)

            STATE_MEMORY.append(state)
            ACTION_MEMORY.append(sampled_action)
            REWARD_MEMORY.append(reward)

            state = next_state

    STATE_TENSORS = torch.tensor(STATE_MEMORY, dtype=torch.float32)
    ACTION_TENSORS = torch.tensor(ACTION_MEMORY, dtype=torch.long).unsqueeze(-1)
    RETURN_TENSORS = torch.tensor(calculate_returns(REWARD_MEMORY), dtype=torch.float32).unsqueeze(-1)

    """
    Critic Q Learning Phase
    """
    expected_state_action_values = q(STATE_TENSORS).gather(1, ACTION_TENSORS)
    single_true_cumulative_rewards = RETURN_TENSORS
    critic_q_loss = (expected_state_action_values - single_true_cumulative_rewards).pow(2).mean()
    
    optimizer_critic_q.zero_grad()
    critic_q_loss.backward()
    optimizer_critic_q.step()

    """
    Critic V Learning Phase
    """
    # Almost identical
    expected_state_action_values = v(STATE_TENSORS)
    single_true_cumulative_rewards = RETURN_TENSORS
    critic_v_loss = (expected_state_action_values - single_true_cumulative_rewards).pow(2).mean()

    optimizer_critic_v.zero_grad()
    critic_v_loss.backward()
    optimizer_critic_v.step()

    """
    Actor (Policy) Learning Phase
    """
    likelihoods = policy(STATE_TENSORS).gather(1, ACTION_TENSORS)
    log_likelihoods = torch.log(likelihoods) 

    with torch.no_grad():
        assigned_credits = q(STATE_TENSORS).gather(1, ACTION_TENSORS) - v(STATE_TENSORS)

    weighted_log_likelihoods = log_likelihoods * assigned_credits

    pseudo_loss = -weighted_log_likelihoods.mean()
    optimizer_actor.zero_grad()
    pseudo_loss.backward()
    optimizer_actor.step()


    track_performance += sum(REWARD_MEMORY)

    if i % 100 == 0:
        print(f"{i}th Trial -> {track_performance/100}")
        track_performance = 0
```
    
</div>
</details>

그러나 $$Q^{\pi}$$와 $$V^{\pi}$$를 표현하는 신경망을 따로 가지고 있는다면 낭비다. 다소 부정확할지라도 $$Q^{\pi}$$로 $$V^{\pi}$$를 표현하거나 $$V^{\pi}$$를 $$Q^{\pi}$$로 표현해 방법론을 단순하게 만들어야 좋다. 이런 맥락에서는 $$Q^{\pi}$$를 $$V^{\pi}$$의 관점으로 표현한다. 이렇게 말이다.

$$Q^{\pi}(s_t, a_t) \approx r_t + V^{\pi}(s_{t+1})$$

좌변과 우변은 다른 양이다. $$s_t$$에서 $$a_t$$를 했을지라도 놓이게 되는 다음 상태 $$s_{t+1}$$은 달라질 수 있기 때문이다. 정확히는 이렇게 기술해야 옳다.

$$Q^{\pi}(s_t, a_t) = \mathbb{E}_{s_{t+1}} \left[ r_t + V^{\pi}(s_{t+1})\right] $$

독자들 스스로 이 등식과 근사식의 함의를 고찰해보길 바란다. $$Q^{\pi}$$와 $$V^{\pi}$$의 정의를 상기하며 두 함수가 어떤 관계로 엮여있을지 찬찬히 생각해보면 된다. $$V^{\pi}$$를 $$Q^{\pi}$$의 기댓값으로 표현할 수도 있다. 이런 관점은 이미 [지난 이야기]({% post_url 2020-6-1-rl-story-1 %})에서 등장했었다.


<details id='inside'>
<summary>Talk is cheap. Show me the code. ┓ </summary>
<div markdown="1">

```python
import gym
import torch
import torch.nn as nn

env = gym.make("CartPole-v1")

STATE_DIM = env.observation_space.shape[0]
ACTION_DIM = env.action_space.n

HIDDEN_DIM = 200
LEARNING_RATE = 0.0001

def to_tensor(array):
    if array.ndim == 1:
        return torch.tensor(array, dtype=torch.float32).unsqueeze(0)
    else:
        return torch.tensor(array, dtype=torch.float32)

def calculate_returns(rewards):
    R = 0
    returns = []
    for r in rewards[::-1]:
        R = R + r
        returns.append(R)
    return returns[::-1]

policy = nn.Sequential(nn.Linear(STATE_DIM, HIDDEN_DIM),
                       nn.LeakyReLU(),
                       nn.Linear(HIDDEN_DIM, HIDDEN_DIM),
                       nn.LeakyReLU(),
                       nn.Linear(HIDDEN_DIM, ACTION_DIM),
                       nn.Softmax(dim=1))

v = nn.Sequential(nn.Linear(STATE_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, HIDDEN_DIM),
                  nn.LeakyReLU(),
                  nn.Linear(HIDDEN_DIM, 1))

optimizer_actor = torch.optim.AdamW(policy.parameters(), lr = LEARNING_RATE)
optimizer_critic = torch.optim.AdamW(v.parameters(), lr = 5*LEARNING_RATE)

track_performance = 0
for i in range(1, 10000):
    # Note that we now need two more memory lists.
    STATE_MEMORY = []
    NEXT_STATE_MEMORY = []
    ACTION_MEMORY = []
    REWARD_MEMORY = []
    DONE_MEMORY = []

    state = env.reset()
    done = False
    while not done:
        action_probs = policy(to_tensor(state))
        sampled_action = torch.multinomial(action_probs, 1).item()
        next_state, reward, done, _ = env.step(sampled_action)

        STATE_MEMORY.append(state)
        NEXT_STATE_MEMORY.append(next_state)
        ACTION_MEMORY.append(sampled_action)
        REWARD_MEMORY.append(reward)
        DONE_MEMORY.append(done * 1)

        state = next_state

    # Concatenation to tensors. I know. It's ugly :(
    STATE_TENSORS = torch.tensor(STATE_MEMORY, dtype=torch.float32)
    NEXT_STATE_TENSORS = torch.tensor(NEXT_STATE_MEMORY, dtype=torch.float32)
    ACTION_TENSORS = torch.tensor(ACTION_MEMORY, dtype=torch.long).unsqueeze(-1)
    REWARD_TENSORS = torch.tensor(REWARD_MEMORY, dtype=torch.float32).unsqueeze(-1)
    RETURN_TENSORS = torch.tensor(calculate_returns(REWARD_MEMORY), dtype=torch.float32).unsqueeze(-1)
    DONE_TENSORS = torch.tensor(DONE_MEMORY, dtype=torch.float32).unsqueeze(-1)

    """
    Critic V Learning Phase
    """
    expected_state_action_values = v(STATE_TENSORS)
    single_true_cumulative_rewards = RETURN_TENSORS
    critic_loss = (expected_state_action_values - single_true_cumulative_rewards).pow(2).mean()

    optimizer_critic.zero_grad()
    critic_loss.backward()
    optimizer_critic.step()

    """
    Actor (Policy) Learning Phase
    """
    likelihoods = policy(STATE_TENSORS).gather(1, ACTION_TENSORS)
    log_likelihoods = torch.log(likelihoods)

    # We approximate Q(s)[a] (Current State-Action Value) with r+V(s') (Reward + Next State Value)
    # Note that it's not accurate Q value even when V is 100% accurate.
    # Simple but crucial implementation trick is presented
    #   : "Truncate next state value when it's terminal state."
    # There's no life (hence no values) after death, at least in the CartPole environment.
    assignmented_credits = REWARD_TENSORS + v(NEXT_STATE_TENSORS)*(1-DONE_TENSORS) - v(STATE_TENSORS)

    weighted_log_likelihoods = log_likelihoods * assignmented_credits

    pseudo_loss = -weighted_log_likelihoods.mean()
    optimizer_actor.zero_grad()
    pseudo_loss.backward()
    optimizer_actor.step()


    track_performance += sum(REWARD_MEMORY)

    if i % 100 == 0:
        print(f"{i}th Trial -> {track_performance/100}")
        track_performance = 0
```
    
</div>
</details>

정리해보자. 괄호 안에 무엇을 넣어야 더 나은 기여도 할당이 가능할지 생각해 보았다.

$$\sum_t (\ \ \ ) \log \pi_{\theta}(a_t|s_t)$$

첫 번째로는 누적 보상의 기댓값인 $$Q^{\pi}(s_t, a_t)$$가 있었고, 두 번째로는 행위의 좋고 나쁨을 상대적으로 고려할 수 있도록 $$V^{\pi}(s_t)$$와의 차이를 계산한 $$Q^{\pi}(s_t, a_t)$$ $$- V^{\pi}(s_t)$$가 있었다. 두 번째 양을 Advantage라 부르고 이 양을 이용한 Policy Gradient 방법론에는 Advantage Actor-Critic (A2C)라는 이름이 붙어있다. A2C를 CPU의 여러 쓰레드에서 병렬적 + 비동기적으로 돌리자는 아이디어가 그 유명한 [A3C](https://arxiv.org/abs/160201783) 되시겠다.

$$A^{\pi}(s_t, a_t) = Q^{\pi}(s_t, a_t)  - V^{\pi}(s_t)$$

신경망을 $$Q^{\pi}$$ 따로 $$V^{\pi}$$ 따로 사용할 수 없으니 Advantage의 근사로서는 일반적으로 $$r_t + V^{\pi}(s_{t+1}) - V^{\pi}(s_{t})$$를 사용한다고 했다. 이를 조금 더 일반화하여 여러 방식으로 Advantage를 표현할 수 있다. 관심있는 독자는 [GAE](https://arxiv.org/abs/1506.02438)를 읽어보면 좋다. 이후 맥락에서 $${\hat{A_t}}$$라는 기호가 나온다면 무언가 한 가지 방법으로 Advantage를 추정한 값이라 이해하면 된다.

가치 함수를 도입했다고 지난 이야기의 말미에 등장했던 문제점들이 해결되는건 아니다. 받는 보상이 없으면 정답도 항상 $$0$$이다. 그러면 가치 함수는 $$0$$을 뱉는 방법을 학습한다. 진정한 무가치 함수로 거듭난다. 쇼펜하우어가 따로 없다. 혹시 무가치 함수가 필요하다면 시도해 보자. 