---
layout: post
title: 강화 학습 이야기 1
comments: true
permalink: /rl-story-1/
description: A story about policy gradient methods.
---

자, 신경망을 학습시켜 보자. 먼저 호랑이의 형상이 담겨있는 픽셀 덩어리를 먹인다. 먹은 덩어리가 호랑이라는 범주에 속한다고 얼마나 확신하고 있는지 엿본다. 그 수치가 높아지도록 파라미터를 살짝 건드려준다. 그러면 끝이다. 먹어보았던 것이 무엇인지 알게 되도록 지도해주는 셈이다. 그래서 지도 학습이다. 먹어보지 않은 것까지 알게되는 마법이 일어나기를 바란다면 정화수를 떠놓고 비는 수 밖엔 없다. 정말이다.

강화 학습에서는 이런 신탁을 내려줄 미지의 지적 존재를 가정하지 않는다. 확신이 얼마나 타당했는지 학습 주체 스스로 판단해야 한다. 무엇이 옳고 그른지 판단할 근거가 아예 없다면 행동을 교정할 수 있을 턱이 없다. 그래서 보상이라는 신호가 주어진다. 

강화 학습은 시간이 존재하는 상황에서의 의사 결정을 논하는 분야다. 잠깐 강화 학습의 언어를 복습하자. 먼저 학습의 주체인 행위자가 있다. 행위자는 자신이 놓인 상황 $$s$$에 기반해 어떤 행동 $$a$$가 최적일지 판단한다. 행동의 결과로 상황이 $$s$$에서 $$s'$$로 변하며 보상 $$r$$을 받는다. 보상은 행동이 그 시점에서 얼마나 좋고 나빴는지 알려준다. 이 과정이 반복된다. 행위자는 누적될 보상이 가장 커지도록 행동하는 법을 배워야 한다. 

최적의 의사 결정을 논하기 위해 만든 자못 [단순해 보이는 모형](https://en.wikipedia.org/wiki/Markov_decision_process)이다. 모든 의사 결정 문제를 이 언어로 표현할 수 있으리라는 보장은 없어 보인다. 가령 유아의 발달 과정을 설명할 수 있는 [보상 체계가 있을까](https://arxiv.org/abs/1802.06070)? 그럴 수도 있고 아닐 수도 있다. [가설](http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html)에 불과하나 적어도 아직까지는 유용하다.

호랑이를 보고 솜사탕이라고 판단했는가? 괜찮다. 솜사탕은 달콤하니 보상을 +1만큼 주자. 잠시 후 다시 보니 형체가 조금 더 가까워졌다. 이제는 호랑이가 아니라 고양이 같다. 고양이는 귀여우니 보상을 +10만큼 주자. 그러나 잘못된 판단이 누적되면 언젠가는 호랑이의 식사가 되는 미래를 마주한다. 그 순간의 보상이 아닌 시간에 따른 누적 보상이 커지도록 행동하는 법을 배워야 하는 이유이다.

배우도록 장려하는 방법은 간단하다. 행위자의 행동이 높은 누적 보상을 가져왔다면 당근을, 그렇지 못했다면 채찍을 주어 행동 정책을 교정한다. 경험을 이용해 정책을 교정할 올바른 방향을 판단한다. 그래서 이런 철학을 공유하는 방법론들에 Policy Gradient라는 이름이 붙어있다.

정답이 제공되어 있다고 가정하자. 다시 말해 상황 $$s$$에서 행동 $$a$$가 올바른 행위라는 사실을 인간이 알려주었다고 하자. 그렇다면 이 문제는 전형적인 지도 학습이다. 첫 문단에서 말했듯 로그 가능도 $$\log \pi_{\theta} (a \mid s)$$를 높이는 방향으로 신경망을 학습시키면 간단하다. (사족: [현실적으로는 불충분](http://proceedings.mlr.press/v15/ross11a/ross11a.pdf)하다. 이런 상황을 강화 학습의 맥락에서는 모방 학습이라 부른다.) 만약 이러한 판단들이 순차적으로 제공되었다면 로그 가능도들을 시간에 따라 전부 더해주면 된다. 이 양이 늘어나도록 행위자의 행동을 교정한다. 

$$\sum_t  \log \pi_{\theta} (a_t \mid s_t)$$

그러나 강화 학습에서는 $$s_t$$에서 $$a_t$$를 하도록 판단한 주체가 인간이 아니라 행위자이다. 그러니 이런 방법론을 사용할 수 없다. 대신 자연스럽게 수정해 볼 수는 있다. $$s_t$$에서 했던 $$a_t$$가 정답은 아니지만 얼마나 옳고 그른 행동인지 정량화할 수 있다고 하자. 그렇게 정량화한 결과를 가중치의 형태를 빌어 행위자에게 일러준다. 행위자는 이 신호를 근거로 행동을 교정한다. 가중치가 양수라면 해당 행동을 강화하고 음수라면 약화한다. 그래서 강화 학습이다.

$$\sum_t (\ \ \ ) \log \pi_{\theta}(a_t|s_t)$$

행위자의 행동들은 순차적으로 누적되어 하나의 미래를 만들어 낸다. 미래는 좋을 수도 있고 나쁠수도 있다. $$a_t$$가 그 미래에 얼마나 기여했는지 따지는건 쉬운 문제가 아니다. 미래는 아주 좋았지만 $$a_t$$ 자체는 나빴을 수 있으며 그 반대일 수도 있다. 어쩌면 $$s_t$$에서의 $$a_t$$가 좋은 미래를 만드는데 결정적인 요인이었을수도 있다. [인과 관계](https://en.wikipedia.org/wiki/Causal_inference)를 [추론하는 문제](https://arxiv.org/abs/1605.03661)가 뜨거운 주제인 이유가 있다. $$s_t$$에서의 $$a_t$$가 이후 미래에 얼마나 기여했는지 판단하는 문제를 강화 학습에서는 기여도 할당 문제라 부른다. 

행동 $$a_t$$ 이후로 받은 보상을 모두 더해 행동 교정의 신호로 제공해볼 수 있다. 

$$\sum_t (\sum_{t'=t} r_{t'}) \log \pi_{\theta}(a_t|s_t)$$

그러나 순진해도 너무 순진하다. 백번을 잘 하다가도 한 번 큰 실수를 저질러 버리면 좋은 행동들까지 약화되어 버린다. 인과 관계를 고려하기 귀찮은 자의 알고리즘이다. 그래도 말은 된다. 지도 학습의 얼개를 유지한 채로 보상을 이용한다. 영리하다.

의사 결정을 순차적으로 해 나가는 상황에서 행위자가 했던 행동들을 일단은 정답이라 간주한다. 얻은 보상의 합으로 그러한 행동들이 얼마나 타당했는지 사후적으로 판단하여 행동 교정의 신호로 사용한다. 단순하면서도 직관적인 이 방법론은 REINFORCE라는 이름으로 1992년 처음 제안되었고 오늘날 Policy Gradient 방법론들의 효시가 되었다. 일견 말이 되는 방법론처럼 보인다. 특정한 환경에서는 실제로 작동을 잘 한다. 가령 [NAS](https://arxiv.org/abs/1611.01578)가 처음 제안되었을 때도 이 간단한 방법론으로 재미를 톡톡히 보았다.

<details>
<summary>Talk is cheap. Show me the code. ┓ </summary>
<div markdown="1">

이해를 돕기 위해 앞으로 소개할 구현체들의 전체적인 짜임새를 크게 수정하지는 않을 것이다. 여러 방법론들 간 어떤 부분들이 비슷하고 다른지 따져 가며 읽어 보기를 권장한다. 환경을 바꾸어 가며 시도해 보아도 좋을 것이다.

```python
"""
Note that there will be large degree of redundancy around every code.
It would be beneficial if you try to distinguish between codes.
All hyperparameters will be tuned minimally.
"""
import gym
import torch
import torch.nn as nn

env = gym.make("CartPole-v1")

# Dimension of the observation space = 4
STATE_DIM = env.observation_space.shape[0] 
# Number of possible actions, assuming discrete action spaces = 2 (left, right)
ACTION_DIM = env.action_space.n 

HIDDEN_DIM = 123
LEARNING_RATE = 0.0001

def to_tensor(array):
    if array.ndim == 1:
        return torch.tensor(array, dtype=torch.float32).unsqueeze(0)
    else:
        return torch.tensor(array, dtype=torch.float32)

def calculate_returns(rewards):
    """
    Calculate culumatve rewards.
    Usage : [1, 3, 1, 1, 2] -> [8, 5, 4, 3, 2]
    """
    R = 0
    returns = []
    for r in rewards[::-1]:
        R = R + r
        returns.append(R)
    return returns[::-1]

# Stochastic policy that outputs a probability distribution over possible actions
policy = nn.Sequential(nn.Linear(STATE_DIM, HIDDEN_DIM),
                       nn.LeakyReLU(),
                       nn.Linear(HIDDEN_DIM, HIDDEN_DIM),
                       nn.LeakyReLU(),
                       nn.Linear(HIDDEN_DIM, ACTION_DIM),
                       nn.Softmax(dim=1)) 

optimizer = torch.optim.AdamW(policy.parameters(), lr = LEARNING_RATE)

track_performance = 0
for i in range(1, 10000):
    # Note that these memories are initialized everytime when the loop starts.
    # It's because your policy can only leverage the data collected by itself.
    # This makes policy gradient algorithms sample-inefficient. 
    # We will definitely fix this.
    STATE_MEMORY = []
    ACTION_MEMORY = []
    REWARD_MEMORY = []

    # Interacting with the environment. (i.e. Generating a single trajectory.)
    state = env.reset()
    done = False
    while not done:
        # Calculates probability distribution over the current state.
        action_probs = policy(to_tensor(state)) 

        # Sample action from the probability distribution.
        sampled_action = torch.multinomial(action_probs, 1).item()
        next_state, reward, done, _ = env.step(sampled_action)

        STATE_MEMORY.append(state)
        ACTION_MEMORY.append(sampled_action)
        REWARD_MEMORY.append(reward)

        state = next_state

    # Concatenation to tensors.
    STATE_TENSORS = torch.tensor(STATE_MEMORY, dtype=torch.float32)
    ACTION_TENSORS = torch.tensor(ACTION_MEMORY, dtype=torch.long).unsqueeze(-1)
    RETURN_TENSORS = torch.tensor(calculate_returns(REWARD_MEMORY), dtype=torch.float32).unsqueeze(-1)

    # Calculate log-likelihood as if it were a supervised learning problem.
    likelihoods = policy(STATE_TENSORS).gather(1, ACTION_TENSORS)
    log_likelihoods = torch.log(likelihoods) 

    # Cumulative-rewards weighted version of log likelihoods.
    # Without this part, It will simply be reduced to a supervised learning problem.
    weighted_log_likelihoods = log_likelihoods * RETURN_TENSORS 

    # Optimization as usual.
    pseudo_loss = -weighted_log_likelihoods.mean()
    optimizer.zero_grad()
    pseudo_loss.backward()
    optimizer.step()

    # Keeping track of the performace of the algorithm.
    track_performance += sum(REWARD_MEMORY)

    if i % 100 == 0:
        print(f"{i}th Trial -> {track_performance/100}")
        track_performance = 0
```

</div>
</details>

## 닥터 스트레인지

여기까지는 좋아보이지만 현실적으로 여러 문제가 있다. REINFORCE에서는 어떤 행동이 얼마나 좋았는지 판단하기 위해 해당 행동 이후에 받은 보상을 전부 더하고 있다. 만약 타임머신을 타고 그 행동을 했던 직후로 돌아갔다고 해 보자. 정확히 같은 행동을 했더라도 미래는 달라진다. 그것도 아주 크게. 발생 가능한 미래는 무한히 많고 우리가 관측한 미래는 그 중 하나에 불과하다. 그래서 행동 교정의 신호로 제공하기에 누적 보상이라는 신호는 매우 불안정하다. 만약 타임머신이 있어서 특정 행동 이후의 미래를 수백, 수천번 관찰할 수 있다면 각 미래들에서 받는 누적 보상들의 평균을 행동 교정의 신호로 제공하면 된다. 그것이 가치 함수이다. 물론 타임머신이 있다 한들 어느 세월에 그러고 있겠느냐만은 아무튼 가능하다면야 그것이 최선이다. 이렇게 써 보자.

$$\sum_t Q(s_t, a_t) \log \pi_{\theta}(a_t|s_t)$$

$$Q(s, a)$$는 상황 $$s$$에서 행동 $$a$$를 하고난 후 얻을 수 있는 누적 보상의 기댓값을 의미한다. 기댓값이라는 말에 익숙하지 않은 독자가 많으리라 생각한다. 한없이 많이 시도해본 후 평균을 계산한 양이라고 생각하면 된다. 행동 $$a$$ 직후에 발생할 수 있는 모든 미래들을 고려하겠다는 말이다. 좋은 행동이라면 대부분의 미래에서 높은 누적 보상을, 나쁜 행동이라면 낮은 누적 보상을 안겨줘야 한다. 이러한 양을 알 수 있다면 더욱 안정적인 기여도 할당이 가능하다.

그러나 아직 어색하다. 가령 발을 헛디뎌 넘어지는 상황에 놓여있다고 해 보자. 내가 어떤 행동을 하든 넘어지는 미래를 피할 수는 없다. 자연의 법칙을 거스를 수는 없는 법이다. 그러니 어떤 행동에 대해서도 $$Q(s,\ \cdot )$$가 낮다. 그러나 아무리 넘어지는 상황에 놓여있더라도 행동의 우열을 가릴 수는 있다. 손으로 곧 다가올 충격을 완화할 채비를 하는건 좋은 행동이고, 신나서 가속도를 붙이는 행동은 정신 빠진 행동이다. 좋고 나쁨은 상대적이다. 다른 행동들과 비교해야 안다.

$$\sum_t \left[ Q(s_t, a_t)-V(s_t) \right] \log \pi_{\theta}(a_t|s_t)$$

$$V(s)$$는 $$Q(s,a)$$의 행동에 대한 기댓값을 의미한다. $$s$$에서 행동을 한없이 많이 해보고  $$Q(s, a)$$의 평균을 계산했다고 생각하면 된다. 이제야 무언가 제대로 돌아가는 느낌이다. 어떤 행동이 다른 행동들에 비해 좋았어야만 강화된다. 행위를 하는 주체와 행동을 교정하는 주체가 다르다. 그러한 방법론들을 Actor-Critic이라 부른다. 맞다. $$Q(s,a)$$와 $$V(s)$$를 모르니 아직은 신선 놀음이다. 걱정 마시라. [다음 포스팅]({% post_url 2020-6-2-rl-story-2 %})부터 지겹도록 다룰 예정이다.

이해를 돕기 위해 인과를 비틀어 설명했다. 좀 더 엄밀한 접근이 궁금한 독자들은 [이 글](https://arxiv.org/abs/1906.10652)을 읽어보자. 딥마인드 연구자들의 짬바를 느낄 수 있다. 참 잘 쓴 글이다.

## 여는말

비슷한 이야기들을 몇 꼭지 더 쓸 계획이다. 강화 학습과 인간의 발달이 어떤 면에서 비슷하고 다른지 의식적으로 따져 가며 읽혀지기를 바란다. 풍부한 관찰과 사색을 가능케 하는 관점이다. 이미 널리 알려져 있는 이야기들을 굳이 재생산하려는 이유들 중 하나이다.

당근과 채찍으로 행동이 강화되고 소거된다. 강화의 원리를 따르는 조작적 조건 형성이라 말한다. 행동주의 심리학은 인간의 행동을 강화의 원리로 설명한다. 대표적인 행동주의자 스키너는 그렇게 [비둘기에게 탁구를 가르쳤다.](https://www.youtube.com/watch?v=vGazyH6fQQ4) 당근과 채찍과 행동만이 존재하는 건조한 세계관이다. 의식이라는 유령이 끼어들 틈이 없다. 그래서 행동주의 심리학은 의식을 배제한다. 탁구를 배우는 비둘기와 존재를 논하는 철학자의 본질이 같다.

이러한 관점은 젊은 노암 촘스키를 [자극해](https://youtu.be/zobBTuX03D8) 인지 혁명의 시대를 불러온다. 인간은 단순히 자극에 반응하는 블랙 박스가 아니다. 정보를 능동적으로 받아들이고 처리한다. 의식이 있다. [의식의 실재성은 중요하지 않다](https://www.youtube.com/watch?v=f-08IkK0UxM). 그러나 의식이 대단히 유용한 모형이라는 사실은 인정해야 한다. 단적인 증거가 있다. 연결주의라 부르는 인지 과학의 한 분파가 있다. 뉴런의 작용을 계산 모형으로 만들어 인간의 정보 처리 과정을 설명하려 한다. 연결주의의 다른 이름은 친숙하다. 딥러닝이다. [딥러닝의 대가들이 의식에 집착하는](https://slideslive.com/38922304/from-system-1-deep-learning-to-system-2-deep-learning) 이유가 있다. 의식의 존재를 가정하면 더 나은 알고리즘을 디자인 할 수 있다.

위에서 이야기했던 방법론들에는 행동주의 심리학의 정신이 녹아들어 있다. 무의식적인 시행착오를 반복할 따름이다. 행위자는 일단 보상을 받아 보아야 학습이 가능하다. 무엇이 좋고 나쁜 행동인지 알려면 겪어 보아야만 한다. 전부 찍어 먹어 보는 수 밖엔 없다. 선험적 지식이 없다면 필연적으로 우연에 기대야만 한다. 무적처럼 보이는 강화 학습 방법론들이 얼마나 우연에 의존하고 있는지 안다면 깜짝 놀랄거다. 복잡한 의사결정에 따른 행동을 반복해야만 보상을 간신히 받을 수 있는 환경에서는 문제가 더욱 심화된다. 아직 미숙한 행위자가 아무리 행동해봐야 절대로 보상을 받을 수 없다. 학습이 불가능하다.

행위자가 우연의 장벽을 넘고 넘어 높은 보상을 얻는 방법을 알아냈다고 하자. 또 다른 문제가 드러난다. 훨씬 높은 보상을 얻을 수 있는 낙원이 없다는 보장이 없다. 심지어 여태까지 최악의 보상을 주었던 길을 헤쳐 나가야만 닿을 수 있는 낙원이라면 더 끔찍해진다. 유토피아를 꿈꾸며 최악이었던 행동들을 의도적으로 해 보아야 하는가? 행위자가 자해의 성향을 보이는건 난감하다.

오래되고 골치아픈 문제이다. [이 딜레마를 가장 단순하게 만들어 놓은 문제](https://en.wikipedia.org/wiki/Multi-armed_bandit)가 있다. 제2차 세계 대전 당시 이 문제가 쓰인 삐라를 독일 과학자들에게 뿌려 지적 사보타주를 꾀하던 역사가 있을 정도니 말 다 했다.

이러한 일련의 물음들은 인간이 겪는 문제일 수도, 아닐 수도 있다. 보상을 받아보고 사후 분석을 거쳐 행동 교정의 신호로 이용하는 방식은 수동적이며 게으르다. 인간은 환경으로부터 보상을 받아보지 않고도 좋고 나쁨을 판단해 행동할 수 있다. 찍어먹어 보아야만 된장인줄 아는건 아니라는 말이다. 인간은 계획을 세워 행동한다. 세상이 동작하는 매커니즘을 얼추 이해하고 있어야 가능한 일이다. 이는 추후 이야기할 모델 기반 강화학습이 필연적임을 시사한다. 도메인 지식을 가지고 [보상 체계를 단계적으로 세심하게 디자인](https://people.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf)해 볼 수도 있고, [호기심](https://pathak22.github.io/noreward-rl/)을 [모델링](https://arxiv.org/abs/1808.04355)해 [내재적인 보상](http://www.cs.cornell.edu/~helou/IMRL.pdf)의 신호로 이용해 볼 수도 있다. 하지만 임시변통일 뿐 본질적인 해결책은 아니다. 의식의 역할이 필요하다.

그러나 제아무리 인간이라도 아예 듣지도 보지도 못한 무언가를 알 수는 없는 법이다. 그러한 상황에서는 인간도 찍어 먹어보아야 안다. 아무리 그래도 전부 찍어 먹어볼 수는 없다. 인생은 짧다. 체계적인 찍어먹기를 위해서는 경험 외적의 지식이 필요하다. 개체의 수준에서 종의 수준으로 논의를 확장해야 한다. [개체의 빠른 학습은 종이 오랜 시간 쌓아 온 선험적 지식에 의존한다](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30061-0). 인류의 선험적 지식은 문화의 형태로 존재하기도 하고 진화를 거쳐 유전자의 형태로 존재하기도 한다. 이에 상응하는 알고리즘이 필요하다. 단순히 생각하면 [문화는 데이터베이스로](http://proceedings.mlr.press/v48/santoro16.pdf), [유전자는 신경망 가중치의 초깃값 따위로](https://arxiv.org/abs/1703.03400) 구현하면 될 법도 하다. 이런 아이디어들을 메타 학습이라고 부른다. 아주 핫하다.

이 이야기의 끝맺음은 정해져 있다. 환경과 상호 작용이 불가능한 상황에서 누군가가 만들어 놓은 데이터로 학습하는 방법론에 대한 이야기다. 찍어 먹기는 아예 금기로 간주한다. 대단히 까다로운 상황이다. 그러나 [언어](https://github.com/openai/gpt-3)와 [시각](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)에 내렸던 딥러닝의 은총이 강화 학습에도 내리게 하기 위해서는 반드시 풀어야 할 문제이다. 찍어 먹어볼 것을 전제하는 강화 학습은 Scalable하지 않다. 최근에서야 이러한 방법론들의 [벤치마크를 위한 데이터베이스](https://arxiv.org/abs/2004.07219)를 구성하기 시작했다. 언제나 좋은 신호탄이다. [계산이 우리를 자유케 하리라](https://newsight.tistory.com/302).
