<!DOCTYPE html>
<html>
  <head>
    <title>강화 학습 이야기 3</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="가치 함수에는 강력한 재귀적 구조가 존재한다. 흙먼지 아래에 방치하기엔 아까운 구조다. 심미적 관점에서도 틀리다. 가치 함수의 아름다움을 존중하는 알고리즘이 필요하다.
" />
    <meta property="og:description" content="가치 함수에는 강력한 재귀적 구조가 존재한다. 흙먼지 아래에 방치하기엔 아까운 구조다. 심미적 관점에서도 틀리다. 가치 함수의 아름다움을 존중하는 알고리즘이 필요하다.
" />
    
    <meta name="author" content="JunHyoung Ryu's Blog" />

    
    <meta property="og:title" content="강화 학습 이야기 3" />
    <meta property="twitter:title" content="강화 학습 이야기 3" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="JunHyoung Ryu's Blog - zero-calorie ideas." href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="/images/jekyll-logo.png" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">JunHyoung Ryu's Blog</a></h1>
            <p class="site-description">zero-calorie ideas.</p>
          </div>

          <nav>
            <a href="/about" class="about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>강화 학습 이야기 3</h1>


  <div class="entry">
    <p>가치 함수에는 강력한 재귀적 구조가 존재한다. 흙먼지 아래에 방치하기엔 아까운 구조다. 심미적 관점에서도 틀리다. 가치 함수의 아름다움을 존중하는 알고리즘이 필요하다.</p>

<p>상황에 대한 가치함수 \(V^{\pi}\)를 생각해보자. 행위자가 상황 \(s\)에서 행동 \(a\)를 하면 보상 \(r\)를 받고 \(s'\)라는 상황에 놓인다. 원래 상황의 가치 \(V^{\pi}(s)\)와 바뀐 상황의 가치 \(V^{\pi}(s')\) 사이에는 어떤 관계가 존재할까? 두 상태가 시간적으로 매우 가까우므로 크게 다르다면 어색하다. 현재 상황에서 보상 \(r\)을 받고 다음 상황 \(s'\)로 넘어갔으니 이렇게 쓸 수 있다.</p>

\[V^{\pi}(s) \approx r + V^{\pi}(s')\]

<p>아침에 가만히 앉아 오늘부터 삶의 마지막까지 누릴 수 있는 행복의 총량을 추산해 본다. 하루를 열심히 살고 다음날 아침에 가만히 앉아 같은 양을 추산한다. 전자를 \(V(s)\), 후자를 \(V(s')\)라 생각하자. 시간적 관계를 고려해 보자. \(V(s)\)는 하루동안 누렸던 행복 \(r\)에 \(V(s')\)를 더한 양과 비슷해야 말이 된다.</p>

<p>그러나 세상 일은 뜻대로 되지 않는다. 길을 걷다 우연히 귀여운 고양이와 마주칠 수도 있고, 간발의 차로 지하철을 놓칠 수도 있는 노릇이다. 타임머신이 있어 하루를 여러번 살 수 있다고 해도 매번 다른 하루를 마주하게 된다. 머릿속의 행복 추산기가 정확하다면, 그리고 타임머신이 있다면 오늘 추산한 행복 \(V(s)\)는 하루를 수 없이 살아보고 계산한 \(r + V(s')\) 들의 평균과 일치해야 한다.</p>

\[V^{\pi}(s) = \mathbb{E}_{s'} \left[ r + V^{\pi}(s') \right]\]

<p>그럼에도 불구하고 \(V^{\pi}(s)\)와 하나의 미래에 대한 \(r + V^{\pi}(s')\)는 비슷해야 한다. 하루가 아무리 다사다난해봐야 얼마나 달라질 수 있겠는가. (사족: 물론 이 진술은 일반적으로 참이 아니다. 이런 논의가 불편하다면 단순히 기댓값을 하나의 표본으로 추정했다고 생각해도 좋다. 분산이 얼마나 클지는 모르지만 적어도 불편추정량이다. 물론 기댓값이 아닌 전체 분포를 논하는 방식이 훨씬 낫다. 이런 분야를 Distributional RL이라 부르고 2020년 현재 SOTA로 간주되는 방법론들이 이에 속한다.)</p>

<h2 id="사슬-끊기">사슬 끊기</h2>

<p>이 수식에는 벨만 기댓값 방정식이라는 이름이 붙어있다. 가치 함수의 정의를 변형해 얻을 수 있다.</p>

\[V^{\pi}(s) =\mathbb{E}_{\pi} \left[  \sum_{t'=t} r_{t'} \mid s_t = s \right]\]

\[=\mathbb{E}_{\pi} \left[  r_t + \sum_{t'=t+1} r_{t'} \mid s_t = s \right]\]

\[= \mathbb{E}_{\pi} \left[ r + V^{\pi}(s_{t+1}) \mid s_t = s \right]\]

<p>당연한 과정인 양 넘어가면 안 된다. \(V^{\pi}(s)\)를 표현하는 언어가 달라진다. 정의에서는 \(s_t=s\)에서 시작하는 미래에서 받는 보상의 총 합으로 \(V^{\pi}(s)\)를 표현한다. 인과의 사슬이 길다.</p>

<p>\(s_t\) \(\rightarrow\) \(a_t\) \(\rightarrow\) \({\color{#642EFE} {r_t}}\) \(\rightarrow\) \(s_{t+1}\) \(\rightarrow\) \(a_{t+1}\) \(\rightarrow\) \({\color{#642EFE} {r_{t+1}}}\) \(\rightarrow\) \(s_{t+2}\) \(\rightarrow\) \(\dots\)</p>

<p>그러나 마지막 식에서는 \(s_t\)에서 시작하는 짧은 미래를 논한다. 이것도 말이 된다. 가치 함수의 정의는 미래에 받을 보상의 총 합이다. \(s_{t+1}\) 이후로 받는 보상의 총 합은 \(V^{\pi}(s_{t+1})\)이 알려 준다. 그러니 \(s_t\)에서 받을 보상은 \(r_t\)에 \(V^{\pi}(s_{t+1})\)를 더하면 된다.</p>

<p>\(s_t\) \(\rightarrow\) \(a_t\) \(\rightarrow\) \({\color{#642EFE} {r_t}}\) \(\rightarrow\) \({\color{#642EFE} {s_{t+1}}}\)</p>

<p>상황 \(s\)에서의 가치를 판단하고 싶다면 우선 한 단계 미래에서 처할 수 있는 상황들을 고려한다. 바로 다음에 처할 수 있는 상황들의 가치를 참조하면 현재 상황 \(s\)의 가치를 표현할 수 있다. 당연하다.</p>

<p>그러나 잠깐만, 이는 순환 논리가 아닌가? 현재 상황의 가치를 몰라 이를 판단하고 싶은 상황인데 미래에 처하게 되는 상황의 가치인들 알고 있겠는가?</p>

<figure class="image">
    <img src="/images/funcoolsexy.gif" alt="'그것이 가치 함수이니까.'" />
    <figcaption class="caption">'그것이 가치 함수이니까.'</figcaption>
  </figure>

<p>맞는 말이다. 가치 함수를 어떻게 구해야 할 지 알려주는 식은 아니다. 가치 함수들의 값이 무엇일지는 모른다. 그러나 각 상태들의 가치 함수들은 서로가 서로를 표현할 수 있도록 얽혀 있어야 한다. 이 제약이 가치 함수의 공간에 강한 구조를 부여한다.</p>

<h2 id="벨만의-고뇌">벨만의 고뇌</h2>

<p>정의에 따라 가치 함수를 알아내기 위해서는 타임 머신을 이용해 무한히 많은 미래를 관측해 보아야 한다. 마찬가지로 타임머신이 있다면 벨만 기댓값 방정식을 이용해 가치 함수를 알아낼 수 있다. 방정식이라는 이름이 붙어 있는 이유다.</p>

<p>\(n\)개의 상황이 있을 수 있다고 하자. 벨만 기댓값 방정식의 좌변에 \(n\)개의 상황 중 하나를 넣어 보자. 우변은 기댓값이다. \(s\) 다음에 놓일 수 있는 상황 \(s'\)를 무수히 많이 관찰해 \(r+V^{\pi}(s')\)의 평균을 계산하면 표현할 수 있다. 그러면 많아 봐야 \(n\)개의 미지수들로 이루어진 1차 방정식을 하나 얻는다. 같은 과정을 반복하면 \(n\)개의 방정식을 얻는다. 미지수가 \(n\)개, 식이 \(n\)개이니 해를 쉽게 구할 수 있다.</p>

<p>그러나 \(50\)년대 연구자 리처드 벨만에게는 계산 자원이 부족했으리라. 선형 방정식을 풀고 싶다면 역행렬을 계산해야 한다. 그러나 역행렬 계산의 복잡도는 일반적으로 \(O(n^3)\) 정도다. 자꾸 메모리가 터지니 성질이 났나보다. 공감이 간다. 그 유명한 차원의 저주라는 단어가 벨만의 입에서 처음 나왔다. 그래서 제안한 근사적 해법이 다이나믹 프로그래밍이다. 다이나믹 프로그래밍이라는 이름이 멋지지 않은가? 맞다. <a href="https://pubsonline.informs.org/doi/pdf/10.1287/opre.50.1.48.17791">간지나라고 지은 이름이라고 한다</a>. 알고리즘 과목에 등장하는 다이나믹 프로그래밍의 어원이다.</p>

<p>우선 아는게 없으니 모든 상태의 가치 함수를 \(0\)으로 초기화 한다. 타임 머신을 이용해 \(r+V^{\pi}(s')\)들의 평균을 구하고 그걸 \(V^{\pi}(s)\)에 할당하자. 물론 초기 \(V^{\pi}(s')\)들은 모두 \(0\)일 테다. 그러나 \(r\)들이 \(0\)이 아니라면 \(V^{\pi}(s)\)는 \(0\)에서 벗어난다. 이 과정을 모든 상태에서 반복하면 언젠가는 옳은 가치 함수로 수렴한다. 이런 알고리즘을 다이나믹 프로그래밍이라고 부른다.</p>

<p>계산 문제를 차치해도 비선형 방정식은 단순히 역행렬을 곱해 풀 수 없다. 그런 경우에도 다이나믹 프로그래밍을 쓸 수 있다. 이후 이야기 할 벨만 최적 방정식은 비선형 방정식이다. 다이나믹 프로그래밍으로 풀어야 한다.</p>

<p>다이나믹 프로그래밍은 타임 머신의 존재를 가정한다. (사족: 기댓값을 계산할 수 있다고 가정한다.) 으레 그렇듯이 타임 머신이 없다면 관측된 하나의 미래만을 고려하는게 일반적이다. 이를 시간차 학습이라고 부른다.</p>

<p>이 단락은 역사적 배경에 불과하다. 일부러 자세히 쓰진 않았다. 그러니 잊어도 좋다. 일반적으로 쓰는 방법론은 시간차 학습이다. 시간차 학습이 등장한 배경을 소개하고 싶었다.</p>

<h2 id="자아-성찰">자아 성찰</h2>

<p>첫 단락에서 이어진다. \(V^{\pi}(s)\)와 \(r + V^{\pi}(s')\)는 비슷해야 한다.</p>

\[V^{\pi}(s) \approx r + V^{\pi}(s')\]

<p>머릿속의 행복 추산기가 아직 미숙한 상태라고 가정하자. \(V^{\pi}(s)=12\)이었고 하루가 지난 오늘 다시 따져 보았을 때 \(r=1\), \(V^{\pi}(s')=10\) 라고 해 보자. \(12&gt;1+10\) 이니 모르긴 몰라도 어제의 나는 너무 순진했다. 오늘 생각해 보았을 때, 이 각박한 세상에서 어제의 나는 더 비관적이어야 할 필요가 있다. \(V^{\pi}(s)\)가 약간 작아지도록 행복 추산기를 살짝 건드린다. 내일의 내가 오늘의 나를 교정한다.</p>

<p>행복 추산기가 미숙한 상태라고 가정했다. 그러니 오늘 추산한 \(V^{\pi}(s')=10\) 또한 정확하지 않다. 그럼에도 이를 신뢰하여 교정한다는 점이 재밌다. 이러한 아이디어는 강화 학습에서 시간차 학습 (Temporal Difference Learning) 이라는 이름으로 등장한다. 개인적으로는 자기 참조 학습 (Self-Referential Learning) 이라는 이름이 더 적절하지 않을까 싶다. 리처드 서튼의 말을 빌리자면 시간차 학습은 강화학습에서 등장하는 가장 신박하고 중요한 아이디어란다. 맞는 말이다. 가치 함수가 나오는 맥락이라면 시간차 학습은 무조건 등장한다.</p>

<p>일찍이 마빈 민스키는 본인의 저서 <a href="http://aurellem.org/society-of-mind/som-17.1.html">마음의 사회</a> (1991) 에서 인간의 발달 과정은 자기 자신을 가르치는 과정의 반복이라는 이야기를 한 바 있다. 완전히 다른 주제이긴 하지만, 같은 이야기가 Knowledge Distillation의 맥락에서도 등장한다는게 재밌다. 최근까지 Imagenet 분류의 <a href="https://arxiv.org/abs/1911.04252">왕좌를 차지했던 방법론</a>에서도 자가 학습의 아이디어를 차용하고 있다.</p>

<p>시간차 학습을 염두에 두었다면 이렇게 쓰는게 더 낫겠다.</p>

\[V^{\pi}(s) \leftarrow r + V^{\pi}(s')\]

<p>신경망으로 가치 함수를 표현하려는 상황이라면 아래 오차가 줄어들도록 파라미터를 건드리면 된다. 유의할 점은 학습 과정에서 \(r + V^{\pi}(s')\)는 상수로 취급한다는 점이다. PyTorch에서는 <code class="language-plaintext highlighter-rouge">detach</code>를 붙여 계산 그래프를 끊어주거나 <code class="language-plaintext highlighter-rouge">no_grad</code> 컨텍스트 매니저를 이용해 아예 계산 그래프를 만들지 못하게 해야한다. 내일의 내가 오늘의 나를 교정해야 한다. 오늘의 내가 내일의 나를 교정하면 과거의 망령이 된다.</p>

\[\left[ V^{\pi}(s) - (r + V^{\pi}(s')) \right]^2\]

<p>\(V^{\pi}\)를 표현하는 신경망을 시간차 학습으로 추정하기 위해 필요한 데이터는 \((s,\ r,\ s')\) 이다. 가치 함수는 정책 \(\pi\)에 종속되는 양이라고 했다. 이 데이터 어디에서 정책에 대한 종속성을 확인할 수 있는가? 헷갈리게도 데이터에 직접적으로 드러나진 않는다. 데이터를 수집하는 과정을 생각해보자. 행위자는 상황 \(s\)에서 정책 \(\pi\)에 따라 \(a\)를 결정한다. \(s\)에서 행위자의 판단으로 \(a\)을 했기 때문에 \(r\)을 받고 \(s'\)에 놓인다. 결국 \(s'\)는 정책의 영향을 받아 관측된 데이터이다. 강조한다. \((s,\ r,\ s')\)는 가치 함수를 추정하고 싶은 정책 \(\pi\)로 모은 데이터여야만 한다. 만약 그렇지 않았다면? \(\pi\)가 아닌 이 데이터를 모은 미지의 정책 \(\pi ^{\beta}\)에 대한 가치 함수를 추정하는 꼴이 되어버린다. 가치 함수를 추정하기 위한 데이터를 모을 때 정책이 어디에서 어떻게 개입하는지 명확히 이해하자. <a href="/rl-story-2/">이전 이야기</a>에서도 강조했다.</p>

<p>가치 함수를 신경망 등을 이용해 표현하려는 상황에서 시간차 학습이 발산할 수 있다는 사실은 <a href="https://www.mit.edu/~jnt/Papers/J063-97-bvr-td.pdf">오래전부터 알려져 있던 사실</a>이다. <a href="https://arxiv.org/abs/1812.02648">어떤 이유</a>로 <a href="https://arxiv.org/abs/1903.08894">그렇게 되는지</a>는 나중에 이야기 해 볼 생각이다. 그러나 일반적으로 가치 함수를 추정할 때는 거의 항상 시간차 학습을 이용한다. 먼 미래가 아닌 단 한 단계의 미래만을 고려하기 때문에 분산이 낮은 덕이다. (사족: 반드시 한 단계 미래와의 관계만을 볼 필요는 없다. 정의를 조금 더 만지면 일반적으로 \(n\)단계 미래와의 관계를 보도록 변형할 수 있다. 자주 쓰이는 테크닉이다.)</p>

<h2 id="파블로프의-개">파블로프의 개</h2>

<p>시간차 학습에서 등장하는 오차의 순서를 바꾸어 써 보면 재밌는 해석이 가능하다.</p>

\[\left[ V^{\pi}(s) - (r + V^{\pi}(s')) \right]^2\]

\[= \left[ (V^{\pi}(s) - V^{\pi}(s')) - r) \right]^2\]

<p>내가 기대한 자극 \(V^{\pi}(s) - V^{\pi}(s')\)과 실제로 받은 자극 \(r\)의 간극이 줄어들도록 나의 기대를 바꾸어 나간다. 전형적인 조건 형성이다. 조건 형성은 자극의 의외성에 달렸다. 받는 자극이 더 의외일수록 조건 형성은 더 강하게 일어난다. 심리학의 레스콜라-바그너 모형은 이 논리로 조건 형성을 설명한다. 그래서 시간차 학습과 놀랍도록 닮아 있다. 삶의 경험이 늘어나면 이 오차는 작아진다. 모든 자극을 예상할 수 있다. 삶이 지루해진다.</p>

<p>A2C에서는 계산 효율을 위해 \(Q^{\pi}(s,a)\)를 \(r+V(s')\)로 근사한다. \(r\) \(-(V^{\pi}(s)\) \(-\) \(V^{\pi}(s'))\)라는 신호를 이용해 행동을 교정하는 셈이다. 실제로 받은 자극이 내가 기대한 자극보다 높을 때 행동이 강화된다. 놀라움을 향한 인간의 욕구를 잘 설명한다. 행동주의 심리학의 강화 원리에 대한 조금은 다른 해석이다.</p>

<p>같은 원리가 미시적인 세계에서도 드러난다는게 놀랍다. 다음 상태의 가치 함수에 보상을 더한 정보가 이전 상태의 가치 함수로 전파된다. 도파민이 전파되는 <a href="https://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC6721851&amp;blobtype=pdf">원리</a>라고 <a href="https://science.sciencemag.org/content/275/5306/1593">한다</a>. 기대한 자극과 실제 받은 자극이 다르면 다를수록 더 빠른 전파가 일어날테다. 중뇌의 작은 부분에서 생성되는 도파민이 멀리까지 전파될 수 있는 이유이다. <a href="https://arxiv.org/abs/2006.01001?fbclid=IwAR31J3lMmwJXYmAu2KBxcJ0Q0546Wfmx0IJCgkseoNyhjExaecuQUa3jOyw">신경 과학</a>에서는 <a href="https://www.jneurosci.org/content/jneuro/16/5/1936.full.pdf">오래 전부터 알려져 있던 사실</a>들이다. <a href="http://www.princeton.edu/~ndaw/dt.pdf">읽어</a>보면 <a href="https://www.pnas.org/content/108/Supplement_3/15647">재밌</a>는 <a href="https://deepmind.com/blog/article/Dopamine-and-temporal-difference-learning-A-fruitful-relationship-between-neuroscience-and-AI">이야기들</a>이다.</p>

<h2 id="카르페-디엠">카르페 디엠</h2>

<p>감가율 \(\gamma\)를 소개하고 마치자. 가치 함수는 먼 미래까지 받는 보상의 누계를 고려하는 양이다. 한없이 커질 수 있다. 현재 시점으로부터 멀어질수록 받은 보상의 중요도를 낮추어 더하면 이 문제를 해결할 수 있다. 현재 시점에서 한 단계씩 멀어질수록 \(\gamma\)를 곱한다. 그렇다면 가치함수의 정의와 벨만 기대 방정식은 아래와 같이 수정할 수 있다. 위에서 만든 알고리즘에 이를 반영하고 싶다면 다음 상태의 가치가 등장할 때마다 \(\gamma\)를 곱해 수정하면 된다.</p>

\[V^{\pi}(s) =\mathbb{E}_{\pi} \left[  \sum_{t'=t} \gamma^{t'-t} r_{t'} \mid s_t = s \right]\]

\[=\mathbb{E}_{\pi} \left[  r_t + \sum_{t'=t+1} \gamma^{t'-t} r_{t'} \mid s_t = s \right]\]

\[= \mathbb{E}_{\pi} \left[ r + \gamma V^{\pi}(s') \mid s_t = s \right]\]

<p>\(\gamma\)는 \(0.99\)처럼 \(1\)에 가까운 값으로 설정한다. 직관적으로는 \(\frac{1}{1-\gamma}\)만큼 떨어진 미래까지의 보상만 고려한다고 받아들이면 된다. \(\gamma\)는 원론적으로 사람이 정해주는 값이 아닌 문제 정의의 일부다. 그러나 현실적으로는 하이퍼 파라미터 취급을 받아 이리저리 굴려지는 안타까운 녀석이다. 먼 미래까지 고려하는 문제보다 가까운 미래까지 고려하는 문제가 쉽다. 그래서 가끔 구현체를 보다 보면 \(\gamma\)를 \(0.5\)처럼 낮은 값으로 설정해 놓곤 점점 늘려나가는 통밥을 볼 때도 있다. 먼 미래를 무시할 수 있게 해주는 특성상 <a href="https://arxiv.org/abs/1506.02438">분산 감소를 위한 하이퍼 파라미터 취급을 받는 경우</a>도 있고, 메타 학습으로 아예 <a href="https://arxiv.org/abs/1805.09801">적절한 값을 학습 당하는 경우</a>도 있다. 57개의 Atari 2600 게임에서 모두 인간을 깨부쉈다는 Deepmind의 <a href="https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark">Agent57</a>에도 이런 아이디어가 구현되어 있다. 다음 이야기부터는 \(\gamma\)가 등장하면 \(0.99\) 쯤으로 고정된 상수라고 생각하자.</p>

  </div>

  <div class="date">
    20년 6월의 기록
  </div>


  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'junhsss-github-io';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>


  
    <div id="disqus_thread"></div>
    <script>

      /**
      *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
      *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

      var disqus_config = function () {
        this.page.url = "http://localhost:4000/rl-story-3/";
        this.page.identifier = "강화 학습 이야기 3";
      };

      (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://junhsss-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                                
  
</article>

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    </div>
    <!--
    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          



<a href="https://github.com/barryclark/jekyll-now"><i class="svg-icon github"></i></a>




<a href="https://www.twitter.com/jekyllrb"><i class="svg-icon twitter"></i></a>



        </footer>
      </div>
    </div>
    -->

    

  </body>
</html>

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>