I"нц<p>лЏ„лЊЂмІґ мќґкІЊ л¬ґмЉЁ нѓЃмѓЃ кіµлЎ мќёк°Ђ? лЇёлћм—ђ л°›мќ„ м€ мћ€лЉ” л€„м Ѓ ліґмѓЃмќ кё°лЊ“к°’мќґлќјл‹€. к°ЂлЉҐн•њ лЇёлћл“¤мќ„ м „л¶Ђ м—ґк±°н•ґ ліґкё°лќјлЏ„ н•кІ л‹¤лЉ” л§ђмќёк°Ђ? <a href="/rl-story-1/">м§Ђл‚њ мќґм•јкё°</a>лҐј мќЅмњјл©° мќґлџ° мѓќк°Ѓмќґ л“¤м—€л‹¤л©ґ м •мѓЃмќґл‹¤. к°Ђм№ н•Ём€к°Ђ л°”лЎњ к·ёл ‡кІЊ м •мќлђлЉ” м–‘мќґл‹¤. м •м±… \(\pi\)к°Ђ м–ґл–¤ н–‰лЏ™мќ„ н•ґм•ј н•лЉ”м§Ђ м•Њл ¤м¤Ђл‹¤л©ґ к°Ђм№ н•Ём€лЉ” к·ёлџ¬н•њ н–‰лЏ™ мќґн›„м—ђ м–јл§€л§ЊнЃјмќ л€„м Ѓ ліґмѓЃмќ„ кё°лЊЂн•  м€ мћ€лЉ”м§Ђ м•Њл ¤м¤Ђл‹¤. м •мќлҐј лђм§љм–ґліґмћђ.</p>

<ul>
  <li>мѓЃнѓњ \(s\)м—ђм„њ н–‰мњ„ \(a\)лҐј н–€мќ„ л•Њ лЇёлћм—ђ л°›мќ„ м€ мћ€лЉ” л€„м Ѓ ліґмѓЃмќ кё°лЊ“к°’мќ„ \(Q(s, a)\),</li>
  <li>мѓЃнѓњ \(s\)м—ђ л†“м—¬ мћ€мќ„ л•Њ лЇёлћм—ђ л°›мќ„ м€ мћ€лЉ” л€„м Ѓ ліґмѓЃмќ кё°лЊ“к°’мќ„ \(V(s)\)лќј н•њл‹¤.</li>
</ul>

<p>мќґ м •мќм—ђм„њ лЇёлћлќјлЉ” л§ђмќЂ лЄЁнён•л‹¤. л€„к°Ђ л§Њл“њлЉ” лЇёлћмќёк°Ђ? к°™мќЂ лЏ™л„¤м—ђм„њ л‚кі  мћђлћђлЌ”лќјлЏ„ л‚ґк°Ђ л§Њл“њлЉ” лЇёлћм™Ђ м†м§‘ мІ м€к°Ђ л§Њл“њлЉ” лЇёлћлЉ” л‹¤лҐґл‹¤. к·ёлћм„њ к°Ђм№ н•Ём€лҐј м •мќн•  л•ђ н–‰мњ„мћђмќ м •м±… \(\pi\)к°Ђ к°њмћ…н•њл‹¤. мњ— мІЁмћђлЎњ м–ґл–¤ м •м±…мќ„ л”°лҐј л•Њмќ к°Ђм№ н•Ём€мќём§Ђ м•Њл ¤ мЈјм–ґм•ј лЄ…н™•н•л‹¤. \(Q^{\pi}(s, a)\), \(V^{\pi}(s)\) мќґл ‡кІЊ м“ґл‹¤.</p>

\[Q^{\pi}(s, a)=\mathbb{E}_{\pi} \left[  \sum_{t'=t} r_{t'} \mid s_t = s, a_t = a \right]\]

\[V^{\pi}(s) =\mathbb{E}_{\pi} \left[  \sum_{t'=t} r_{t'} \mid s_t = s \right]\]

<p>мѓЃн™©кіј н–‰лЏ™мќ к°Ђм№ н•Ём€ \(Q^{\pi}\)мќ кІЅмљ°лҐј мѓќк°Ѓн•ґ ліґмћђ. м‹њм ђ \(t\)м—ђм„њ мѓЃн™© \(s\)м—ђ л†“м—¬ мћ€л‹¤. к·ё л•Њ \(a\)лќјлЉ” н–‰лЏ™мќ„ н•лќјлЉ” мЎ°к±ґмќґ мЈјм–ґмЎЊл‹¤. к±°кё°м„њл¶Ђн„° н–‰мњ„мћђлЉ” м •м±… \(\pi\)лҐј л”°лќј л¬ґм€нћ€ л§ЋмќЂ лЇёлћл“¤мќ„ л§Њл“¤м–ґліёл‹¤. (мќґн›„лЎњлЉ” н–‰мњ„мћђм™Ђ м •м±… \(\pi\)лҐј лЏ™мќјм‹њн•кІ л‹¤.) к·ёл ‡кІЊ л§Њл“¤м–ґм§„ м—¬лџ¬ лЇёлћл“¤м—ђм„њ кі„м‚°лђлЉ” л€„м Ѓ ліґмѓЃмќ нЏ‰к· мќ„ \(Q^{\pi}\)лЎњ м •н•њл‹¤кі  л°›м•„л“¤мќґл©ґ лђњл‹¤. мѓЃн™©мќ к°Ђм№ н•Ём€ \(V^{\pi}\)лЏ„ л§€м°¬к°Ђм§Ђл‹¤. мѓЃн™© \(s\)м—ђм„њ м •м±… \(\pi\)лҐј л”°лќј л¬ґм€нћ€ л§ЋмќЂ лЇёлћлҐј л§Њл“¤кі  л€„м Ѓ ліґмѓЃмќ нЏ‰к· мќ„ кі„м‚°н•њл‹¤. л‹¤л§Њ мІмќЊ н–‰лЏ™к№Њм§Ђ м •м±… \(\pi\)м—ђкІЊ л§ЎкІЁ лІ„л¦°л‹¤лЉ” м°Ёмќґк°Ђ мћ€л‹¤.</p>

<p>\(s\)м™Ђ \(a\)лЉ” мЎ°к±ґмњјлЎњ мЈјм–ґм§ЂлЉ” лЌ°мќґн„°лќјлЉ” м ђмќ„ м§љкі  л„м–ґк°Ђм•ј н•њл‹¤. м •м±… \(\pi\)лҐј кі м€н•  л•Њ н„м‹¤м ЃмњјлЎњ л§ћл‹ҐлњЁл¦ґ мќјмќґ м—†лЉ” \(s\)м™Ђ \(a\)м—ђ лЊЂн•ґм„њлЏ„ мќґлЎ мѓЃ \(Q^{\pi}(s, a)\) лЉ” м •мќлђњл‹¤. к°•м›ђлЏ„ н† л°•мќґ мІ м€(\(\pi\))к°Ђ м„ёл ќкІЊн‹° н•њліµнЊђ(\(s\))м—ђм„њ м‚јкІ№м‚ґмќ„ кµ¬м›ЊлЁ№лЉ”кІЊ(\(a\)) м–јл§€л‚ н„лЄ…н•њ мќјмќём§Ђ(\(Q^{\pi}(s, a)\)) м§ђмћ‘мќґм•ј н•ґ ліј м€ мћ€л‹¤лЉ” л§ђмќґл‹¤. л¬јлЎ  \(Q^{\pi}(s, a)\)лҐј м¶”м •н•ґліґкі  м‹¶л‹¤л©ґ м„ёл ќкІЊн‹° м‚јкІ№м‚ґ нЊЊн‹°м—ђ мІ м€лҐј лЌ°л ¤л‹¤ л†“кі  лЇёлћлҐј кґЂм°°н•ґліґм•„м•ј н•њл‹¤. н„мћ¬лЎњм„њлЉ” мњ мќјн•њ л°©лІ•мќґл‹¤. нѓЂмћ„лЁём‹ мќґ мћ€лЉ” кІЅмљ°м—ђлЉ” м—¬лџ¬лІ€ кґЂм°°н•ґліґл©ґ лЌ” мў‹л‹¤.</p>

<p>л§Њм•Ѕ м‹ кІЅл§ќмњјлЎњ \(Q^{\pi}(s, a)\)лҐј н‘њн„н•кі  м‹¶л‹¤л©ґ к°„л‹Ён•л‹¤. \(s\)м™Ђ \(a\)лҐј мћ…л ҐмњјлЎњ, мќґн›„ м€л§ЋмќЂ лЇёлћм—ђм„њ кі„м‚°н•њ л€„м Ѓ ліґмѓЃл“¤мќ нЏ‰к· мќ„ м •л‹µмњјлЎњ л‘ђкі  н•™мЉµм‹њн‚¤л©ґ лђњл‹¤. лЇёлћлҐј м—¬лџ¬ лІ€ кґЂмёЎн•  м€ мћ€л‹¤л©ґ л‹Ём€њн•њ м§ЂлЏ„н•™мЉµм—ђ л¶€кіјн•л‹¤. к·ёлџ¬л‚ мќёкіјмњЁмќ„ мЎґм¤‘н•мћђ. лЇёлћлҐј м—¬лџ¬ лІ€ кґЂмёЎн•  м€лЉ” м—†л‹¤. к·ёлџ¬л‹€ н„м‹¤м ЃмњјлЎњлЉ” л‹Ё н•л‚мќ лЇёлћм—ђм„њ м–»мќЂ л€„м Ѓ ліґмѓЃмќ„ м •л‹µмњјлЎњ м‚¬мљ©н•  м€ л°–м—ђ м—†л‹¤. м •л§ђлЎњ к·ёл ‡кІЊ кµ¬н„н•њл‹¤. к·ёлћлЏ„ мћ‘лЏ™мќ„ нЌЅ мћ н•њл‹¤. (м‚¬мЎ±: м•Ѕк°„мќЂ м¶”мѓЃм Ѓмќё мќґм•јкё°м§Ђл§Њ м–ґм°Ён”ј нЏ‰к·  м њкі± м¤м°Ёк°Ђ мћ‘м•„м§ЂлЏ„лЎќ н•™мЉµм‹њн‚¤л©ґ к°ЂлЉҐн•њ к°’л“¤мќґ мќґлЈЁлЉ” л¶„нЏ¬мќ кё°лЊ“к°’мќ„ н•™мЉµн•лЉ” м…€мќґл‹€ кґњм°®л‹¤. <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">мќґ кµђмћ¬</a>мќ 18 нЋмќґм§ЂлҐј м°ёкі н•мћђ.)</p>

<p>м •м±… \(\pi\)лҐј л”°лҐј л•Њмќ к°Ђм№ н•Ём€ \(Q^{\pi}\)лҐј м•Њкі  м‹¶мќЂ мѓЃн™©мќґл‹¤. к·ёлџ¬л‹€ м •л‹µмќ„ кі„м‚°н•  л•Њ н•„мљ”н•њ лЇёлћлЉ” л°л“њм‹њ \(\pi\)к°Ђ л§Њл“  лЇёлћм—¬м•јл§Њ н•њл‹¤лЉ” м‚¬м‹¤м—ђ мњ мќн•мћђ. л‹¤лҐё м •м±… \(\pi^{\beta}\)к°Ђ л§Њл“  лЇёлћлҐј мќґмљ©н•ґ м •л‹µмќ„ кі„м‚°н•њл‹¤л©ґ \(Q^{\pi^{\beta}}\)лҐј м•ЊкІЊ лђлЉ” м…€мќґл‹¤. м•„л¬ґлџ° мќлЇёк°Ђ м—†л‹¤.</p>

<p>к·ёл ‡л‹¤л©ґ н–‰мњ„мћђм™Ђ м •м±… \(\pi\)к°Ђ м–ґл–»кІЊ лЇёлћлҐј л§Њл“¤м–ґ к°ЂлЉ”м§Ђ л”°м ёліј н•„мљ”к°Ђ мћ€л‹¤. мњ„м—ђм„њ мќґм•јкё°н–€л“Їмќґ \(s_t\)м—ђм„њ \(a_t\)лҐј н•лЉ”к±ґ мЎ°к±ґмќґл‹¤. н–‰мњ„мћђлҐј \(s_t\)лЎњ лЃЊм–ґм™Ђ к°•м њлЎњ \(a_t\)лҐј м‹њмјњліёл‹¤. \(s_t\)м—ђм„њ \(a_t\)лҐј н–€мќ„ л•Њ м–ґл–¤ лЇёлћ \(s_{t+1}\)лҐј л§€мЈјн•кІЊ лђ лЉ”м§ЂлЉ” лЄЁлҐёл‹¤. к·ёлџ¬л‚ м Ѓм–ґлЏ„ н–‰мњ„мћђмќ м†ЊкґЂмќЂ м•„л‹€л‹¤. н–‰мњ„мћђлЉ” мќґлЇё н–‰лЏ™мќ„ м–µм§ЂлЎњ н•ґ лІ„л¦° мѓЃн™©мќґл‹¤. н–‰мњ„мћђлЉ” м†ђмќ„ л†“кі  ліґмѓЃ \(r_t\)м™Ђ н•Ёк» \(s_{t+1}\)лҐј кІён—€нћ€ л§ћмќґн•њл‹¤. к·ё мќґн›„л‚ лђм–ґм•ј л№„лЎњм†Њ н–‰мњ„мћђмќ мќґм„±мќґ к°њмћ…н•њл‹¤. н–‰мњ„мћђлЉ” м •м±… \(\pi\)м—ђ л”°лќј \(a_{t+1}\)лҐј кІ°м •н•њл‹¤. м™њ н–‰мњ„мћђк°Ђ \(a_{t+1}\)лҐј кІ°м •н•лЉ”к°Ђ? \(Q^{\pi}(s_t, a_t)\)мќ м •мќлЉ” \(s_t\)м—ђм„њ \(a_t\)лҐј н–€мќ„ л•Њ, м •м±… \(\pi\)лҐј л”°лќјк°Ђл©ґ лЇёлћм—ђ л°›мќ„ м€ мћ€лЉ” л€„м Ѓ ліґмѓЃмќ кё°лЊ“к°’мќґлќјкі  н–€м—€л‹¤. м •мќм—ђ м¶©м‹¤н•кІЊ \(s_t\)м™Ђ \(a_t\) мќґн›„лЎњ м •м±… \(\pi\)м—ђ л”°лќј н–‰лЏ™н•кі  мћ€л‹¤.</p>

<p>\(Q^{\pi}(s_t, a_t)\)лҐј н•™мЉµм‹њн‚¤кё° мњ„н•ґ л§Њл“¤м–ґм§€ лЌ°мќґн„°лҐј м‹њк°„ м€њм„њм—ђ л”°лќј мЌЁ ліґмћђ.</p>

<p>\(s_t\) \(\rightarrow\) \(a_t\) \(\rightarrow\) \({\color{#642EFE} {r_t}}\) \(\rightarrow\) \(s_{t+1}\) \(\rightarrow\) \(a_{t+1}\) \(\rightarrow\) \({\color{#642EFE} {r_{t+1}}}\) \(\rightarrow\) \(s_{t+2}\) \(\rightarrow\) \(\dots\)</p>

<p>\(s_t\)м™Ђ \(a_t\)м—ђм„њ м‹њмћ‘н•лЉ” н•л‚мќ лЇёлћл§Њ кі л ¤н•ґ м •л‹µмќ„ л§Њл“¤кё°лЎњ н–€м—€л‹¤. к·ёлџ¬л‹€ мћ…л ҐмќЂ \(s_t\)м™Ђ \(a_t\), м •л‹µмќЂ \(\sum_{t'=t}r_{t'}\)лЎњ мћЎкі  м‹ кІЅл§ќмќ„ н›€л Ём‹њн‚¤л©ґ м¶©л¶„н•л‹¤.</p>

<details>
<summary>Talk is cheap. Show me the code. в”“ </summary>
<div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"CartPole-v1"</span><span class="p">)</span>

<span class="n">STATE_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">ACTION_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.0005</span>

<span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">array</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">calculate_returns</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span>
    <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">rewards</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">R</span> <span class="o">+</span> <span class="n">r</span>
        <span class="n">returns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">returns</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="s">"""
Note that it's handy if we implement Q in the form of Q(s)[a] when the action space is discrete. 
We could still implement Q as Q(s, a), but it's unnecessary. (one-hot encode actions or whatever.)
This sutblety will be elaborated further later. It's key difference between DQN-like and DDPQ-like algorithms.
"""</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">))</span>

<span class="c1"># You need separate optimizers for actor and critic respectively.
</span><span class="n">optimizer_actor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">optimizer_critic</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">LEARNING_RATE</span><span class="p">)</span>

<span class="n">track_performance</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">STATE_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ACTION_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">REWARD_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="c1"># Deactivate PyTorch autograd engine as you don't need to compute gradient here.
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span> 
        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action_probs</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
            <span class="n">sampled_action</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">action_probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">sampled_action</span><span class="p">)</span>

            <span class="n">STATE_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">ACTION_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sampled_action</span><span class="p">)</span>
            <span class="n">REWARD_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="n">STATE_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">STATE_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">ACTION_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ACTION_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">RETURN_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">calculate_returns</span><span class="p">(</span><span class="n">REWARD_MEMORY</span><span class="p">),</span>\
                                  <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="s">"""
    Critic (State-action value) learning phase
    """</span>
    <span class="c1"># Q(s)[a] в‰€ cumulative rewards after a in s.
</span>    <span class="n">expected_state_action_values</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ACTION_TENSORS</span><span class="p">)</span>
    <span class="n">single_true_cumulative_rewards</span> <span class="o">=</span> <span class="n">RETURN_TENSORS</span>
    <span class="n">critic_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">expected_state_action_values</span> <span class="o">-</span> <span class="n">single_true_cumulative_rewards</span><span class="p">).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">optimizer_critic</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">critic_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_critic</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="s">"""
    Actor (Policy) learning phase
    """</span>
    <span class="n">likelihoods</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ACTION_TENSORS</span><span class="p">)</span>
    <span class="n">log_likelihoods</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">likelihoods</span><span class="p">)</span> 

    <span class="c1"># Log-likelihoods are now weighted with the state-action values calculated by q. i.e. Q(s)[a]
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">assigned_credits</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ACTION_TENSORS</span><span class="p">)</span>

    <span class="n">weighted_log_likelihoods</span> <span class="o">=</span> <span class="n">log_likelihoods</span> <span class="o">*</span> <span class="n">assigned_credits</span>

    <span class="n">pseudo_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">weighted_log_likelihoods</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">optimizer_actor</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">pseudo_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_actor</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Keeping track of performace of the algorithm.
</span>    <span class="n">track_performance</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">REWARD_MEMORY</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">f"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">th Trial -&gt; </span><span class="si">{</span><span class="n">track_performance</span><span class="o">/</span><span class="mi">100</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="n">track_performance</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div>    </div>

  </div>
</details>

<h2 id="мІњм§Ђ-к°њлІЅ">мІњм§Ђ к°њлІЅ</h2>

<p>\(Q^{\pi}(s_t, a_t)\)лҐј м¶”м •н•кі  м‹¶л‹¤л©ґ мќґ кёёкі  кёґ м‚¬мЉ¬кіј к°™мќЂ лЌ°мќґн„°лЉ” н–‰мњ„мћђк°Ђ л§Њл“¤м—€м–ґм•ј н•њл‹¤кі  к°•мЎ°н–€л‹¤. к·ёлџ¬л‚ н–‰мњ„мћђк°Ђ лЌ°мќґн„°м—ђ к°њмћ…н•њ нќ”м ЃмќЂ \(a_{t+1}\)л¶Ђн„° л“њлџ¬л‚њл‹¤. к·ёлџ¬л‹€ к·ё м „к№Њм§ЂлЉ” н–‰мњ„мћђм™Ђ л¬ґкґЂн•њ лЌ°мќґн„°л‹¤. л§Њм•Ѕ \(Q^{\pi}\)лҐј н•™мЉµм‹њн‚¤кё° мњ„н•њ м–ґл–¤ л°©лІ•лЎ мќґ \(a_{t+1}\) мќґн›„мќ лЌ°мќґн„°лҐј мљ”кµ¬н•м§Ђ м•ЉлЉ”л‹¤л©ґ мќґлЎ  мѓЃ н–‰мњ„мћђм™Ђ л¬ґкґЂн•њ лЌ°мќґн„°лЎњлЏ„ н•™мЉµмќґ к°ЂлЉҐн•л‹¤. лЊЂл‹Ёнћ€ м¤‘мљ”н•њ кґЂм°°мќґл‹¤. мІњм§Ђк°Ђ к°њлІЅн•  мќјмќґл‹¤. м •м±… \(\pi\)мќ к°Ђм№ н•Ём€лҐј \(\pi\)мќ нќ”м Ѓмќґ м—†лЉ” лЌ°мќґн„°лЎњлЏ„ м•Њм•„л‚ј м€ мћ€л‹¤л‹€. мІ м€к°Ђ л§Њл“њлЉ” к°Ђм№лҐј мЃнќ¬мќ н–‰лЏ™л§ЊмњјлЎњ м•Њм•„л‚ј м€ мћ€л‹¤л‹€! мЃл¦¬н•њ лЏ…мћђлќјл©ґ мќкµ¬м‹¬мќґ л“¤м–ґм•ј м •мѓЃмќґл‹¤. мќґ мќґм•јкё°к°Ђ л“±мћҐн•л ¤л©ґ мЎ°кё€ лЌ” кё°л‹¤л ¤м•ј н•њл‹¤. к·ёлџ¬л‹€ мћ к№ђ мћЉкі  м›ђлћ мќґм•јкё°лҐј мќґм–ґк°Ђмћђ.</p>

<h2 id="л°°мљ°-м€м—…">л°°мљ° м€м—…</h2>

<p><a href="/rl-story-1/">м§Ђл‚њ мќґм•јкё°</a>м—ђм„њ л§ђн–€л“Їмќґ к°Ђм№ н•Ём€ \(Q^{\pi}\)лҐј м•Њкі  мћ€л‹¤л©ґ н–‰мњ„мћђмќ м •м±… \(\pi\)лҐј кµђм •н•  л•Њ к°Ѓ н–‰лЏ™л“¤мќ кё°м—¬лЏ„лҐј лЌ” н•©л¦¬м ЃмњјлЎњ нЊђл‹Ён•  м€ мћ€л‹¤. мќґлҐј м•Њкі л¦¬м¦мњјлЎњ мЌЁ ліґмћђ.</p>

<ol>
  <li>м •м±… \(\pi\)лҐј мќґмљ©н•ґ лЌ°мќґн„°лҐј лЄЁмќЂл‹¤.</li>
  <li>м •м±… \(\pi\)мќ к°Ђм№ н•Ём€ \(Q^{\pi}\)лҐј м¶”м •н•њл‹¤.</li>
  <li>м¶”м •н•њ к°Ђм№н•Ём€ \(Q^{\pi}\)лҐј к·јк±°лЎњ м •м±… \(\pi\)мќ н–‰лЏ™мќ„ кµђм •н•њл‹¤.</li>
</ol>

<p>к·ёлџ°лЌ° 3л‹Ёкі„м—ђм„њ м •м±… \(\pi\)к°Ђ мЎ°кё€мќґлќјлЏ„ ліЂн•лЉ” м€њк°„ к°Ђм№ н•Ём€ \(Q^{\pi}\)лЉ” л¬ґмљ©м§Ђл¬јмќґ лђњл‹¤. м •м±…мќґ ліЂн–€мњјл‹€ к°Ђм№ н•Ём€лҐј л‹¤м‹њ м°ѕм•„м•ј н•њл‹¤. м €л§ќм Ѓмќґл‹¤. к·ёлћлЏ„ кІЅм‚¬ н•к°•лІ•мќЂ м •м±…мќ„ нЃ¬кІЊ л°”кѕём§Ђ м•Љмњјл‹€ л‹¤н–‰мќґл‹¤. лЌ•л¶„м—ђ \(\pi\)м™Ђ \(Q^{\pi}\)лҐј н•њ лІ€м”© лІ€к°€м•„ н•™мЉµм‹њн‚¤кё°л§Њ н•ґлЏ„ м¶©л¶„н•л‹¤. л°”лЂЊкё° м „ м •м±…м—ђ лЊЂн•њ к°Ђм№ н•Ём€лҐј мґ€к№ѓк°’ м‚јм•„ л°”лЂђ н›„мќ м •м±…м—ђ лЊЂн•њ к°Ђм№ н•Ём€лҐј м°ѕлЉ” м…€мќґл‹¤. мќґлџ° лЇёл¬н•њ л¶Ђл¶„л“¤к№Њм§Ђ кјјкјјнћ€ м§љкі  л„м–ґк°Ђм•ј м•€ н—·к°€л¦°л‹¤.</p>

<p>м •м±… \(\pi\)м™Ђ к°Ђм№ н•Ём€ \(Q^{\pi}\)мќ„ н‘њн„н•лЉ”лЌ°лЉ” лЏ…л¦Ѕлђњ л‘ђ м‹ кІЅл§ќмќ„ м‚¬мљ©н•њл‹¤. к·ёлџ¬л‹€ нѓњмѓќм ЃмњјлЎњ л¶€м•€м • н•  м€ л°–м—ђ м—†л‹¤. м§Ђл‚њ мќґм•јкё°м—ђм„њ л…јн–€лЌлЊЂлЎњ \(V^{\pi}\) к№Њм§Ђ мћ€л‹¤л©ґ лЌ” л‚мќЂ м‹ лў° н• л‹№мќ„ н•  м€ мћ€л‹¤. \(V^{\pi}\)лҐј н‘њн„н•лЉ” м„ё лІ€м§ё м‹ кІЅл§ќмќ„ лЏ„мћ…н•л©ґ лђњл‹¤.</p>

<details id="inside">
<summary>Talk is cheap. Show me the code. в”“ </summary>
<div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Note that we NEVER do this in practice. 
Just a pedagogical practice. :)
"""</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"CartPole-v1"</span><span class="p">)</span>

<span class="n">STATE_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">ACTION_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.0001</span>

<span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">array</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">calculate_returns</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span>
    <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">rewards</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">R</span> <span class="o">+</span> <span class="n">r</span>
        <span class="n">returns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">returns</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># We now need 3 neural nets and optimizers. 
</span><span class="n">policy</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">))</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">optimizer_actor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">optimizer_critic_q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">optimizer_critic_v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">LEARNING_RATE</span><span class="p">)</span>

<span class="n">track_performance</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">STATE_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ACTION_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">REWARD_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>


    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action_probs</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
            <span class="n">sampled_action</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">action_probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">sampled_action</span><span class="p">)</span>

            <span class="n">STATE_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">ACTION_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sampled_action</span><span class="p">)</span>
            <span class="n">REWARD_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="n">STATE_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">STATE_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">ACTION_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ACTION_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">RETURN_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">calculate_returns</span><span class="p">(</span><span class="n">REWARD_MEMORY</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="s">"""
    Critic Q Learning Phase
    """</span>
    <span class="n">expected_state_action_values</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ACTION_TENSORS</span><span class="p">)</span>
    <span class="n">single_true_cumulative_rewards</span> <span class="o">=</span> <span class="n">RETURN_TENSORS</span>
    <span class="n">critic_q_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">expected_state_action_values</span> <span class="o">-</span> <span class="n">single_true_cumulative_rewards</span><span class="p">).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="n">optimizer_critic_q</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">critic_q_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_critic_q</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="s">"""
    Critic V Learning Phase
    """</span>
    <span class="c1"># Cumulative-rewards weighted version of negative log likelihoods.
</span>    <span class="n">expected_state_action_values</span> <span class="o">=</span> <span class="n">v</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">)</span>
    <span class="n">single_true_cumulative_rewards</span> <span class="o">=</span> <span class="n">RETURN_TENSORS</span>
    <span class="n">critic_v_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">expected_state_action_values</span> <span class="o">-</span> <span class="n">single_true_cumulative_rewards</span><span class="p">).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">optimizer_critic_v</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">critic_v_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_critic_v</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="s">"""
    Actor (Policy) Learning Phase
    """</span>
    <span class="n">likelihoods</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ACTION_TENSORS</span><span class="p">)</span>
    <span class="n">log_likelihoods</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">likelihoods</span><span class="p">)</span> 

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">assignmented_credits</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ACTION_TENSORS</span><span class="p">)</span> <span class="o">-</span> <span class="n">v</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">)</span>

    <span class="n">weighted_log_likelihoods</span> <span class="o">=</span> <span class="n">log_likelihoods</span> <span class="o">*</span> <span class="n">assignmented_credits</span>

    <span class="n">pseudo_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">weighted_log_likelihoods</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">optimizer_actor</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">pseudo_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_actor</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>


    <span class="n">track_performance</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">REWARD_MEMORY</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">f"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">th Trial -&gt; </span><span class="si">{</span><span class="n">track_performance</span><span class="o">/</span><span class="mi">100</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="n">track_performance</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div>    </div>

  </div>
</details>

<p>к·ёлџ¬л‚ \(Q^{\pi}\)м™Ђ \(V^{\pi}\)лҐј н‘њн„н•лЉ” м‹ кІЅл§ќмќ„ л”°лЎњ к°Ђм§Ђкі  мћ€лЉ”л‹¤л©ґ л‚­л№„л‹¤. л‹¤м†Њ л¶Ђм •н™•н• м§ЂлќјлЏ„ \(Q^{\pi}\)лЎњ \(V^{\pi}\)лҐј н‘њн„н•к±°л‚ \(V^{\pi}\)лҐј \(Q^{\pi}\)лЎњ н‘њн„н•ґ л°©лІ•лЎ мќ„ л‹Ём€њн•кІЊ л§Њл“¤м–ґм•ј мў‹л‹¤. мќґлџ° л§ҐлќЅм—ђм„њлЉ” \(Q^{\pi}\)лҐј \(V^{\pi}\)мќ кґЂм ђмњјлЎњ н‘њн„н•њл‹¤. мќґл ‡кІЊ л§ђмќґл‹¤.</p>

\[Q^{\pi}(s_t, a_t) \approx r_t + V^{\pi}(s_{t+1})\]

<p>мўЊліЂкіј мљ°ліЂмќЂ л‹¤лҐё м–‘мќґл‹¤. \(s_t\)м—ђм„њ \(a_t\)лҐј н–€мќ„м§ЂлќјлЏ„ л†“мќґкІЊ лђлЉ” л‹¤мќЊ мѓЃнѓњ \(s_{t+1}\)мќЂ л‹¬лќјм§€ м€ мћ€кё° л•Њл¬ёмќґл‹¤. м •н™•нћ€лЉ” мќґл ‡кІЊ кё°м€ н•ґм•ј міл‹¤.</p>

\[Q^{\pi}(s_t, a_t) = \mathbb{E}_{s_{t+1}} \left[ r_t + V^{\pi}(s_{t+1})\right]\]

<p>лЏ…мћђл“¤ мЉ¤мЉ¤лЎњ мќґ л“±м‹ќкіј к·јм‚¬м‹ќмќ н•ЁмќлҐј кі м°°н•ґліґкёё л°”лћЂл‹¤. \(Q^{\pi}\)м™Ђ \(V^{\pi}\)мќ м •мќлҐј мѓЃкё°н•л©° л‘ђ н•Ём€к°Ђ м–ґл–¤ кґЂкі„лЎњ м—®м—¬мћ€мќ„м§Ђ м°¬м°¬нћ€ мѓќк°Ѓн•ґліґл©ґ лђњл‹¤. \(V^{\pi}\)лҐј \(Q^{\pi}\)мќ кё°лЊ“к°’мњјлЎњ н‘њн„н•  м€лЏ„ мћ€л‹¤. мќґлџ° кґЂм ђмќЂ мќґлЇё <a href="/rl-story-1/">м§Ђл‚њ мќґм•јкё°</a>м—ђм„њ л“±мћҐн–€м—€л‹¤.</p>

<details id="inside">
<summary>Talk is cheap. Show me the code. в”“ </summary>
<div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"CartPole-v1"</span><span class="p">)</span>

<span class="n">STATE_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">ACTION_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.0001</span>

<span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">array</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">calculate_returns</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span>
    <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">rewards</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">R</span> <span class="o">+</span> <span class="n">r</span>
        <span class="n">returns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">returns</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">optimizer_actor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">optimizer_critic</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">LEARNING_RATE</span><span class="p">)</span>

<span class="n">track_performance</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">STATE_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">NEXT_STATE_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ACTION_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">REWARD_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">DONE_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Interacting with the environment. (i.e. Generating single trajectory.)
</span>    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
        <span class="n">sampled_action</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">action_probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">sampled_action</span><span class="p">)</span>

        <span class="n">STATE_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">NEXT_STATE_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
        <span class="n">ACTION_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sampled_action</span><span class="p">)</span>
        <span class="n">REWARD_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
        <span class="n">DONE_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">done</span> <span class="o">*</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="c1"># Concatenation to tensors.
</span>    <span class="n">STATE_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">STATE_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">NEXT_STATE_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">NEXT_STATE_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">ACTION_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ACTION_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">REWARD_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">REWARD_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">RETURN_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">calculate_returns</span><span class="p">(</span><span class="n">REWARD_MEMORY</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">DONE_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">DONE_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="s">"""
    Critic V Learning Phase
    """</span>
    <span class="c1"># Cumulative-rewards weighted version of negative log likelihoods.
</span>    <span class="n">expected_state_action_values</span> <span class="o">=</span> <span class="n">v</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">)</span>
    <span class="n">single_true_cumulative_rewards</span> <span class="o">=</span> <span class="n">RETURN_TENSORS</span>
    <span class="n">critic_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">expected_state_action_values</span> <span class="o">-</span> <span class="n">single_true_cumulative_rewards</span><span class="p">).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    <span class="c1"># Optimization as usual.
</span>    <span class="n">optimizer_critic</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">critic_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_critic</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="s">"""
    Actor (Policy) Learning Phase
    """</span>
    <span class="c1"># Cumulative-rewards weighted version of negative log likelihoods.
</span>    <span class="n">likelihoods</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ACTION_TENSORS</span><span class="p">)</span>
    <span class="n">log_likelihoods</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">likelihoods</span><span class="p">)</span> <span class="c1"># As if it were a supervised learning problem.
</span>    <span class="n">assignmented_credits</span> <span class="o">=</span> <span class="n">REWARD_TENSORS</span> <span class="o">+</span> <span class="n">v</span><span class="p">(</span><span class="n">NEXT_STATE_TENSORS</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">DONE_TENSORS</span><span class="p">)</span> <span class="o">-</span> <span class="n">v</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">)</span>
    <span class="n">weighted_log_likelihoods</span> <span class="o">=</span> <span class="n">log_likelihoods</span> <span class="o">*</span> <span class="n">assignmented_credits</span>

    <span class="c1"># Optimization as usual.
</span>    <span class="n">pseudo_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">weighted_log_likelihoods</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">optimizer_actor</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">pseudo_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_actor</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Keeping track of performace of the algorithm.
</span>    <span class="n">track_performance</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">REWARD_MEMORY</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">f"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">th Trial -&gt; </span><span class="si">{</span><span class="n">track_performance</span><span class="o">/</span><span class="mi">100</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="n">track_performance</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div>    </div>

  </div>
</details>

<p>м •л¦¬н•ґліґмћђ. кґ„нё м•€м—ђ л¬ґм—‡мќ„ л„Јм–ґм•ј лЌ” л‚мќЂ кё°м—¬лЏ„ н• л‹№мќґ к°ЂлЉҐн• м§Ђ мѓќк°Ѓн•ґ ліґм•л‹¤.</p>

\[\sum_t (\ \ \ ) \log \pi_{\theta}(a_t|s_t)\]

<p>мІ« лІ€м§ёлЎњлЉ” л€„м Ѓ ліґмѓЃмќ кё°лЊ“к°’мќё \(Q^{\pi}(s_t, a_t)\)к°Ђ мћ€м—€кі , л‘ђ лІ€м§ёлЎњлЉ” н–‰мњ„мќ мў‹кі  л‚мЃЁмќ„ мѓЃлЊЂм ЃмњјлЎњ кі л ¤н•  м€ мћ€лЏ„лЎќ \(V^{\pi}(s_t)\)м™Ђмќ м°ЁмќґлҐј кі„м‚°н•њ \(Q^{\pi}(s_t, a_t)\) \(- V^{\pi}(s_t)\)к°Ђ мћ€м—€л‹¤. л‘ђ лІ€м§ё м–‘мќ„ Advantageлќј л¶ЂлҐґкі  мќґ м–‘мќ„ мќґмљ©н•њ Policy Gradient л°©лІ•лЎ м—ђлЉ” Advantage Actor-Critic (A2C)лќјлЉ” мќґл¦„мќґ л¶™м–ґмћ€л‹¤. A2CлҐј CPUмќ м—¬лџ¬ м“°л €л“њм—ђм„њ лі‘л ¬м Ѓ + л№„лЏ™кё°м ЃмњјлЎњ лЏЊл¦¬мћђлЉ” м•„мќґл””м–ґк°Ђ к·ё мњ лЄ…н•њ <a href="https://arxiv.org/abs/160201783">A3C</a> лђм‹њкІ л‹¤.</p>

\[A^{\pi}(s_t, a_t) = Q^{\pi}(s_t, a_t)  - V^{\pi}(s_t)\]

<p>м‹ кІЅл§ќмќ„ \(Q^{\pi}\) л”°лЎњ \(V^{\pi}\) л”°лЎњ м‚¬мљ©н•  м€ м—†мњјл‹€ Advantageмќ к·јм‚¬лЎњм„њлЉ” мќјл°м ЃмњјлЎњ \(r_t + V^{\pi}(s_{t+1}) - V^{\pi}(s_{t})\)лҐј м‚¬мљ©н•њл‹¤кі  н–€л‹¤. мќґлҐј мЎ°кё€ лЌ” мќјл°н™”н•м—¬ м—¬лџ¬ л°©м‹ќмњјлЎњ AdvantageлҐј н‘њн„н•  м€ мћ€л‹¤. кґЂм‹¬мћ€лЉ” лЏ…мћђлЉ” <a href="https://arxiv.org/abs/1506.02438">GAE</a>лҐј мќЅм–ґліґл©ґ мў‹л‹¤. мќґн›„ л§ҐлќЅм—ђм„њ \({\hat{A_t}}\)лќјлЉ” кё°нёк°Ђ л‚мЁл‹¤л©ґ л¬ґм–ёк°Ђ н•њ к°Ђм§Ђ л°©лІ•мњјлЎњ AdvantageлҐј м¶”м •н•њ к°’мќґлќј мќґн•ґн•л©ґ лђњл‹¤.</p>

<p>к°Ђм№ н•Ём€лҐј лЏ„мћ…н–€л‹¤кі  м§Ђл‚њ мќґм•јкё°мќ л§ђлЇём—ђ л“±мћҐн–€лЌ л¬ём њм ђл“¤мќґ н•ґкІ°лђлЉ”к±ґ м•„л‹€л‹¤. л°›лЉ” ліґмѓЃмќґ м—†мњјл©ґ м •л‹µлЏ„ н•­мѓЃ \(0\)мќґл‹¤. к·ёлџ¬л©ґ к°Ђм№ н•Ём€лЉ” \(0\)мќ„ л±‰лЉ” л°©лІ•мќ„ н•™мЉµн•њл‹¤. м§„м •н•њ л¬ґк°Ђм№ н•Ём€лЎњ к±°л“­л‚њл‹¤. м‡јнЋњн•мљ°м–ґк°Ђ л”°лЎњ м—†л‹¤. н№м‹њ л¬ґк°Ђм№ н•Ём€к°Ђ н•„мљ”н•л‹¤л©ґ м‹њлЏ„н•ґ ліґмћђ.</p>
:ET