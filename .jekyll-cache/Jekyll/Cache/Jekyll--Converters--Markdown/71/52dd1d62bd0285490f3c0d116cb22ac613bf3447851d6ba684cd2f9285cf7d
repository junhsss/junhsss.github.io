I"1<p>가치 함수에는 강력한 재귀적 구조가 존재한다. 흙먼지 아래에 묻어두기는 아까운 구조다. 계산 자원의 낭비이다. 심미적 관점에서도 틀리다. 가치 함수의 아름다움을 모독하는 행동이다. 꺼내 써야 한다.</p>

<p>상황에 대한 가치함수 \(V^{\pi}\)의 경우를 생각해보자. 행위자가 상황 \(s\)에서 행동 \(a\)를 하면 환경으로부터 보상 \(r\)이 주어지고 \(s'\)라는 상황에 놓인다. 그렇다면 원래 상태의 가치 \(V^{\pi}(s)\)와 바뀐 상태의 가치 \(V^{\pi}(s')\) 사이에는 어떤 관계가 존재할까? 두 상태가 시간적으로 매우 가까우므로 크게 다르지 않을 것 같다. 정확히는 현재 상황에서 보상 \(r\)을 받고 다음 상황 \(s'\)로 넘어간 것이니 상황들의 가치 함수의 관계는 이렇게 쓸 수 있다.</p>

\[V^{\pi}(s) \approx r + V^{\pi}(s')\]

<p>아침에 가만히 앉아 오늘부터 삶의 마지막까지 누릴 수 있는 행복의 총량을 추산해 본다. 하루를 열심히 살고 다음날 아침에 가만히 앉아 같은 양을 추산한다. 전자를 \(V(s)\), 후자를 \(V(s')\)라 생각하자. 시간적 관계를 고려해 보았을 때 \(V(s)\)는 하루동안 누렸던 행복 \(r\)에 \(V(s')\)를 더한 양과 비슷해야 말이 된다.</p>

<p>그러나 세상 일은 뜻대로 되지 않는다. 우연히 길에서 귀여운 고양이와 마주칠 수도 있고, 간발의 차로 지하철을 놓칠수도 있는 노릇이다. 타임머신이 있어 하루를 여러번 살 수 있다고 해도 매번 다른 하루를 마주하게 될 것이다. 머릿속의 행복 추산기가 정확하다면, 그리고 타임머신이 있다면 오늘 추산한 행복 \(V(s)\)는 하루를 수 없이 살아보고 계산한 \(r + V(s')\) 들의 평균과 일치해야 할 것이다.</p>

\[V^{\pi}(s) = \mathbb{E}_{s'} \left[ r + V^{\pi}(s') \right]\]

<p>그럼에도 불구하고 \(V^{\pi}(s)\)와 하나의 미래에 대한 \(r + V^{\pi}(s')\)는 비슷할 것이다. 하루가 아무리 다사다난해봐야 얼마나 달라질 수 있겠는가. (사족: 물론 이 진술은 일반적으로 참이 아니다. 이러한 논의가 불편하다면 단순히 기댓값을 하나의 표본으로 추정했다고 생각해도 좋다. 분산이 얼마나 클지는 몰라도 적어도 불편추정량이다. 사실 더 좋은것은 기댓값이 아닌 전체 분포를 논하는 방식이다. 이런 분야를 Distributional RL이라 부르고 2020년 현재 SOTA로 간주되는 방법론들이 이에 속한다.) 이 방정식에는 벨만 기대 방정식이라는 이름이 붙어있다. 이는 사실 가치 함수의 정의를 변형해 간단히 유도할 수 있다.</p>

\[V^{\pi}(s) =\mathbb{E}_{\pi} \left[  \sum_{t'=t} r_{t'} \mid s_t = s \right]\]

\[=\mathbb{E}_{\pi} \left[  r_t + \sum_{t'=t+1} r_{t'} \mid s_t = s \right]\]

\[= \mathbb{E}_{\pi} \left[ r + V^{\pi}(s') \mid s_t = s \right]\]

<p>두 번째 줄에서 세 번째 줄로 넘어가는 10px 사이에서는 많은 일이 일어난다. 정의에서는 \(s\)에서 시작하는 아주 긴 미래들에 대해서 논한다면 마지막 식에서는 \(s\)에서 시작하는 단 한 단계의 미래들에 대해서만 논한다. 상황 \(s\)에서의 가치를 판단하고 싶다면 우선 한 단계 미래에서 처할 수 있는 상황들에 대해 고려한다. 그렇게 처할 수 있는 상황들의 가치를 알고 있다면 이를 참조하여 현재 상황 \(s\)의 가치를 표현할 수 있어야 한다. 말이 된다.</p>

<p>그러나 잠깐만, 이는 순환 논리가 아닌가? 현재 상황의 가치를 몰라 이를 판단하고 싶은 상황인데 미래에 처하게 되는 상황의 가치인들 알고 있겠는가?</p>

<figure class="image">
    <img src="/images/funcoolsexy.gif" alt="'그것이 가치 함수이니까.'" />
    <figcaption>'그것이 가치 함수이니까.'</figcaption>
  </figure>

<p>사실 이미 정답을 말한 것이나 다름이 없다. 정의가 방정식이 되었기 때문이다. 행위자가 놓일 수 있는 상황이 총 \(n\)개라고 해 보자. 벨만 방정식의 좌변에 \(n\)개의 상황 중 하나를 넣으면 가치들, 즉 \(n\)개의 미지수들로 이루어진 1차 방정식을 하나 얻는다. 그렇게 해서 모든 상태를 넣어보면 \(n\)개의 1차 방정식을 얻게 되고, 미지수가 \(n\)개, 식이 \(n\)개가 되니 해를 구할 수 있다. (사족: 물론 말은 쉽다. 선형 방정식을 풀 때 발생하는 온갖 문제를 차치하더라도 역행렬을 계산해야 하는데 역행렬의 계산복잡도는 일반적으로 \(O(n^3)\)이다. 그 유명한 차원의 저주라는 용어가 리처드 벨만에 의해 이 때 처음으로 등장한다. 이 방정식을 효율적으로 근사해 풀기 위해 마찬가지로 리처드 벨만에 의해 제안된 알고리즘이 또 그 유명한 다이나믹 프로그래밍이다. 다이나믹 프로그래밍이라는 이름이 멋지지 않은가? 맞다. 그냥 멋져보이라고 지은 이름이라고 한다.)</p>

<p>여기까지 읽었다면 처음 두 문단을 제외하곤 잠시 잊어도 좋다. 으레 그렇듯이 실제로 구현할때는 단 하나의 미래만을 고려하는게 현실적인 선택이기 때문이다. 다시, 아래의 두 양은 비슷해야 한다.</p>

\[V^{\pi}(s) \approx r + V^{\pi}(s')\]

<p>머릿속의 행복 추산기가 아직 미숙한 상태라고 해 보자. 가령 \(V^{\pi}(s)=12\) 이었고, 하루가 지난 오늘 다시 따져 보았을 때 \(r=1\), \(V^{\pi}(s')=10\) 이라 해 보자. \(12&gt;1+10\) 이니 모르긴 몰라도 어제의 나는 너무 순진했던 것 같다. 오늘 다시 생각해 보았을 때, 이 각박한 세상에서 어제의 나는 조금은 비관적이어야 할 필요가 있다. \(V^{\pi}(s)\)가 약간 작아지도록 행복 추산기를 살짝 건드린다. 내일의 내가 오늘의 나를 교정한다.</p>

<p>행복 추산기가 미숙한 상태이기 때문에 \(V^{\pi}(s')=10\) 자체가 정확한 값이 아님에도 이를 신뢰하여 교정한다는 점이 재밌다. 이러한 아이디어는 강화 학습에서 시간차 학습 (Temporal Difference Learning) 이라는 이름으로 등장한다. 개인적으로는 자기 참조 학습 (Self-Referential Learning) 이라는 이름이 더 적절하지 않을까 싶다. 리처드 서튼의 말을 빌리자면 시간차 학습은 강화학습에서 등장하는 가장 신박하고 중요한 아이디어란다. 맞는 말이다. 가치 함수가 나오는 맥락이라면 시간차 학습은 무조건 등장한다.</p>

<p>일찍이 마빈 민스키는 본인의 저서 <a href="http://aurellem.org/society-of-mind/som-17.1.html">마음의 사회</a> (1991) 에서 인간의 발달 과정은 자기 자신을 가르치는 과정의 반복이라는 이야기를 한 바 있다. 완전히 다른 주제이긴 하지만, 같은 이야기가 Knowledge Distillation의 맥락에서도 등장한다는게 재밌다. 최근까지 Imagenet 분류의 <a href="https://arxiv.org/abs/1911.04252"> 왕좌를 차지했던 방법론</a>에서도 자가 학습의 아이디어를 차용하고 있다.</p>

<p>시간차 학습을 염두에 두었다면 이렇게 쓰는게 더 낫겠다.</p>

\[V^{\pi}(s) \leftarrow r + V^{\pi}(s')\]

<p>신경망으로 가치 함수를 표현하려는 상황이라면 아래 오차가 줄어들도록 파라미터를 건드리면 된다. 유의할 점은 학습 과정에서 \(r + V^{\pi}(s')\)는 상수로 취급한다는 점이다. PyTorch에서는 <code class="language-plaintext highlighter-rouge">detach</code>를 붙여 계산 그래프를 끊어주거나 <code class="language-plaintext highlighter-rouge">no_grad</code> 컨텍스트 매니저를 이용해 아예 계산 그래프를 만들지 못하게 해야한다. 내일의 내가 오늘의 나를 교정하는 것이지 쌍방 교정하는 것이 아니다.</p>

\[\left[ V^{\pi}(s) - (r + V^{\pi}(s')) \right]^2\]

<p>상황에 대한 가치함수를 시간차 학습으로 추정하기 위해 필요한 데이터는 \((s,\ r,\ s')\) 이다. 가치 함수는 정책 \(\pi\)에 종속되는 양이라 했는데 이 데이터에서 정책에 대한 종속성은 어디에서 확인할 수 있는가? 헷갈리게도 데이터에 직접적으로 드러나진 않는다. 데이터를 수집하는 과정에 대해 생각해보자. 상황 \(s\)에서 정책 \(\pi\)가 행동 \(a\)를 할 것을 결정한다. \(s\)에서 \(a\)을 했기 때문에 보상 \(r\)을 받고 \(s'\)에 놓인다. 결국 \(s'\)는 간접적으로 정책의 영향을 받아 관측된 데이터이다. 강조한다. \((s,\ r,\ s')\)는 가치 함수를 추정하고 싶은 정책 \(\pi\)로 모은 데이터여야만 한다. 만약 그렇지 않았다면? \(\pi\)가 아닌 이 데이터를 모은 미지의 정책 \(\pi ^{\beta}\)에 대한 가치 함수를 추정하는 꼴이 되어버릴 것이다. 가치 함수를 추정하기 위한 데이터를 모을 때 정책이 어디에서 어떻게 개입하는지 명확히 이해하는 것이 중요하다.</p>

<p><a href="/rl-story-2/">이전 포스팅</a>에서 이야기 했듯이 A2C 학습 과정의 얼개는 달라지지 않는다. 다만 가치 함수를 추정하는 방식이 달라질 뿐이다.</p>

<ol>
  <li>정책 \(\pi\)를 이용해 데이터를 모은다.</li>
  <li>정책 \(\pi\)에 대한 가치 함수 \(V^{\pi}\)를 시간차 학습으로 추정한다.</li>
  <li>추정한 가치함수 \(V^{\pi}\)를 근거로 정책 \(\pi\)의 행동을 교정한다.</li>
</ol>

<p>가치 함수를 신경망 등을 이용해 근사하려는 상황에서 시간차 학습이 발산할 수 있다는 것은 <a href="https://www.mit.edu/~jnt/Papers/J063-97-bvr-td.pdf">오래전부터 알려져 있던 사실</a>이고 어떤 이유로 그렇게 되는지는 이후 포스팅에서 이야기 해 볼 생각이다. 그러나 일반적으로 가치 함수를 추정할 때는 대체로 시간차 학습을 이용한다고 보면 된다. 먼 미래가 아닌 단 한 단계의 미래만을 고려하기 때문에 분산이 낮은 덕이다. (사족: 반드시 한 단계 미래와의 관계만을 볼 필요는 없다. 정의를 조금 더 만지면 일반적으로 \(n\)단계 미래와의 관계를 보도록 변형할 수 있다. 자주 쓰이는 테크닉이다.)</p>

<p>감가율 \(\gamma\)에 대해 이야기하고 마치자. 가치 함수는 기본적으로 먼 미래까지 받는 보상의 누계를 고려하는 양이기 때문에 한없이 커질 수 있다. 이 문제는 현재 시점으로부터 멀어질수록 받은 보상의 중요도를 낮추어 더하는 것으로 자연스럽게 해결할 수 있다. 현재 시점에서 한 단계씩 멀어질수록 \(\gamma\)를 곱한다. 그렇다면 가치함수의 정의와 벨만 기대 방정식은 아래와 같이 수정할 수 있다. 위에서 만든 알고리즘에 이를 반영하고 싶다면 다음 상태의 가치가 등장할 때마다 \(\gamma\)를 곱해 수정하면 된다.</p>

\[V^{\pi}(s) =\mathbb{E}_{\pi} \left[  \sum_{t'=t} \gamma^{t'-t} r_{t'} \mid s_t = s \right]\]

\[=\mathbb{E}_{\pi} \left[  r_t + \sum_{t'=t+1} \gamma^{t'-t} r_{t'} \mid s_t = s \right]\]

\[= \mathbb{E}_{\pi} \left[ r + \gamma V^{\pi}(s') \mid s_t = s \right]\]

<p>\(\gamma\)는 \(0.99\)처럼 \(1\)에 가까운 값으로 설정하는 것이 일반적이다. 직관적으로는 \(\frac{1}{1-\gamma}\)만큼 떨어진 미래까지의 보상만 고려한다고 받아들이자. \(\gamma\)는 원론적으로 사람이 정해주는 값이 아닌 문제 정의의 일부이나 현실적으로는 하이퍼 파라미터 취급을 받아 막 굴려지는 안타까운 녀석이다. 먼 미래까지 고려하는 것 보다는 가까운 미래까지 고려하는 것이 쉬운 문제이기에 가끔 구현체를 보다 보면 학습 초기에는 \(0.5\)처럼 낮은 값으로 설정했다가 학습이 진행될수록 점점 늘려나가는 통밥을 확인할 수 있다. 먼 미래를 무시할 수 있게 해주는 특성상 <a href="https://arxiv.org/abs/1506.02438">분산 감소를 위한 하이퍼 파라미터 취급을 받는 경우</a>도 있고, 메타 학습으로 아예 <a href="https://arxiv.org/abs/1805.09801">적절한 값을 학습 당하는 경우</a>도 있다. 57개의 Atari 2600 게임에서 모두 인간을 깨부쉈다는 Deepmind의 <a href="https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark">Agent57</a>에도 이런 아이디어가 구현되어 있다. 이후 포스팅에서는 \(\gamma = 0.99\) 쯤으로 고정된 상수라고 생각하자.</p>
:ET