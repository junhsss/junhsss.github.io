I"+<p>눈치를 키우자. 가치 함수를 도입했던 이유를 돌이켜보라. 가치 함수는 행위자의 행동이 불러올 미래가 얼마나 좋고 나쁜지 알려준다. 방황하는 정책을 구원해주던 절대자였다. 그러나 주객이 전도되었다. 가치 함수가 어떤 행동이 좋고 나쁜지 알려주니 그것으로 충분하다. 가치 함수가 곧 정책의 역할을 한다. 정책을 따로 들고 있다면 거추장스러울 뿐이다.</p>

<p><span style="color:#5680e9;"> \(Q^{\pi}\)만을 아는 상황에서 상황 \(s\)에 직면해 있다고 해 보자. 가장 나은 행동을 하고 싶다. 그러면 그저 \(Q^{\pi}(s, \cdot \ )\)에 가능한 행동들을 넣어 보면 된다. \(Q^{\pi}\)를 가장 크게 만드는 행동을 골라서 하면 된다</span> - 정말 그럴까? 결론부터 말하자면 그렇다. 언뜻 당연한 규범처럼 보인다. 그러나 까다로운 독자들은 많은 의문이 들어야 한다. 잠깐 글 읽기를 멈추고 어색한 부분을 스스로 따져 보았으면 한다.</p>

<p>복습하자. \(Q^{\pi}(s, a)\)의 정의를 되짚자. 행위자를  \(s\)로 끌고와 강제로 \(a\)를 시켜본다. 그 이후 \(\pi\)에 따라 행동했을 때 미래에 받을 누적 보상의 기댓값이 \(Q^{\pi}(s, a)\)이다. 두 가지 의문이 들어야 한다.</p>

<ol>
  <li>\(Q^{\pi}\)를 논하고 있다. 그럼 \(\pi\)는 어떤 정책인가?</li>
  <li><span style="color:#5680e9;">하늘색</span> 규범대로 행동하는 행위자의 정책은 \(\pi\)인가?</li>
</ol>

<p>더 근본적인 문제 의식을 제기해 보자. 단순히 어떤 정책 \(\pi\)의 가치 함수 \(Q^{\pi}\)를 추정하는건 큰 의미가 없다. 정책 \(\pi\)가 바보라면 그의 가치 함수를 알아 어디에 쓰겠는가? \(Q^{\pi}\)를 발판삼아 현재 정책 \(\pi\)보다 더 나은 정책 \(\pi'\)를 알아내는 매커니즘이 있어야만 한다. A2C가 그랬다. \(Q^{\pi}\)를 추정해 정책 \(\pi\)를 교정하는데 이용한다. 미세하게나마 개선된 정책 \(\pi'\)을 만들어낸다.</p>

<h2 id="헬리콥터-부모">헬리콥터 부모</h2>

<p><span style="color:#5680e9;">하늘색</span> 규범대로 행동하는 행위자의 정책은  \(\pi\)가 아니다. 가능한 행동이 \(a_1\)와 \(a_2\) 뿐이고 \(Q^{\pi}(s, a_1)\) \(&gt;\) \(Q^{\pi}(s, a_2)\)라 하자. 정책 \(\pi\)는 \(s\)에서 \(a_1\)와 \(a_2\) 중 무엇을 선호할까? 모른다. 답답하지만 \(\pi\)는 \(a_2\)를 더 선호할 수도 있다. 그러나 만약 우리가 행동을 강제할 수 있다면 당연히 \(a_1\)를 고른다. 물론 \(a_1\)가 절대적으로도 좋은 행위라는 말도 아니다. 적어도 그 이후에 \(\pi\)에 따라 행동한다면 \(a_1\)를 하는게 그나마 낫다는 말이다.</p>

<p>건강하지 못한 부모와 자식의 관계다. 부모는 자식 \(\pi\)의 가치 \(Q^{\pi}(s, a)\)를 추산하려 노력한다. 그러곤 자식 \(\pi\)에게 \(Q^{\pi}\)를 가장 크게 만드는 행동들만 할 것을 강요한다. 떨떠름하지만 부모의 명령을 따라 행동하는 것이 자식의 새로운 정책 \(\pi'\)이 된다. 부모는 다시금 \(Q^{\pi'}(s, a)\)를 추산해 자식의 행동을 조종한다.</p>

<p>이러한 방법론을 살사라 부른다. \(Q^{\pi}\)를 시간차 학습으로 추정하기 위해 필요한 데이터를 그대로 읽은 펀치라인이다. 윗 단락에서 제기했던 세 가지 의문이 자연스럽게 해소된다. 이를 알고리즘으로 써 보자.</p>

<ol>
  <li>\(Q^{\pi}(s, a)\)를 가장 크게 만드는 행동만을 하는 새로운 정책 \(\pi'\)를 이용해 데이터를 모은다.</li>
  <li>\(\pi'\)의 가치함수 \(Q^{\pi'}(s, a)\)를 추정한다.</li>
  <li>\(\pi\) \(\leftarrow\) \(\pi'\)</li>
</ol>

<p>정책 \(\pi'\)를 표현하는 신경망 따위가 없다는 것에 주목하자. 이전 정책 \(\pi\)의 가치 함수 \(Q^{\pi}\)가 간접적으로 \(\pi'\)를 정의한다. 마찬가지로 \(\pi'\)의 가치 함수 \(Q^{\pi'}\)는 새로운 정책 \(\pi''\)을 정의한다. 반복한다.</p>

<p>일부러 비극적인 비유를 들어 보았다. 살사는 현실적으로 사용하는 방법론이 아니기 때문이다. 훌닭는 부모 아래 자식의 삶은 불행하다. 더 나은 방법론은 다음 이야기에서 다룬다.</p>
:ET