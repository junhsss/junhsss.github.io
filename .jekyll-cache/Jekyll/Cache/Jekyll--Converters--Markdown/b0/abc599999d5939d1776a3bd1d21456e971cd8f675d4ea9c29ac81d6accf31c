I"ôõ<p>ë„ëŒ€ì²´ ì´ê²Œ ë¬´ìŠ¨ íƒìƒ ê³µë¡ ì¸ê°€? ë¯¸ë˜ì— ë°›ì„ ìˆ˜ ìˆëŠ” ëˆ„ì  ë³´ìƒì˜ ê¸°ëŒ“ê°’ì´ë¼ë‹ˆ. ê°€ëŠ¥í•œ ë¯¸ë˜ë“¤ì„ ì „ë¶€ ì—´ê±°í•´ ë³´ê¸°ë¼ë„ í•˜ê² ë‹¤ëŠ” ë§ì¸ê°€? <a href="/rl-story-1/">ì§€ë‚œ ì´ì•¼ê¸°</a>ë¥¼ ì½ìœ¼ë©° ì´ëŸ° ìƒê°ì´ ë“¤ì—ˆë‹¤ë©´ ì •ìƒì´ë‹¤. ê°€ì¹˜ í•¨ìˆ˜ê°€ ë°”ë¡œ ê·¸ë ‡ê²Œ ì •ì˜ë˜ëŠ” ì–‘ì´ë‹¤. ì •ì±… \(\pi\)ê°€ ì–´ë–¤ í–‰ë™ì„ í•´ì•¼ í•˜ëŠ”ì§€ ì•Œë ¤ì¤€ë‹¤ë©´ ê°€ì¹˜ í•¨ìˆ˜ëŠ” ê·¸ëŸ¬í•œ í–‰ë™ ì´í›„ì— ì–¼ë§ˆë§Œí¼ì˜ ëˆ„ì  ë³´ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆëŠ”ì§€ ì•Œë ¤ì¤€ë‹¤. ì •ì˜ë¥¼ ë˜ì§šì–´ë³´ì.</p>

<ul>
  <li>ìƒíƒœ \(s\)ì—ì„œ í–‰ìœ„ \(a\)ë¥¼ í–ˆì„ ë•Œ ë¯¸ë˜ì— ë°›ì„ ìˆ˜ ìˆëŠ” ëˆ„ì  ë³´ìƒì˜ ê¸°ëŒ“ê°’ì„ \(Q(s, a)\),</li>
  <li>ìƒíƒœ \(s\)ì— ë†“ì—¬ ìˆì„ ë•Œ ë¯¸ë˜ì— ë°›ì„ ìˆ˜ ìˆëŠ” ëˆ„ì  ë³´ìƒì˜ ê¸°ëŒ“ê°’ì„ \(V(s)\)ë¼ í•œë‹¤.</li>
</ul>

<p>ì´ ì •ì˜ì—ì„œ ë¯¸ë˜ë¼ëŠ” ë§ì€ ëª¨í˜¸í•˜ë‹¤. ëˆ„ê°€ ë§Œë“œëŠ” ë¯¸ë˜ì¸ê°€? ê°™ì€ ë™ë„¤ì—ì„œ ë‚˜ê³  ìëë”ë¼ë„ ë‚´ê°€ ë§Œë“œëŠ” ë¯¸ë˜ì™€ ì˜†ì§‘ ì² ìˆ˜ê°€ ë§Œë“œëŠ” ë¯¸ë˜ëŠ” ë‹¤ë¥´ë‹¤. ê·¸ë˜ì„œ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ì •ì˜í•  ë• í–‰ìœ„ìì˜ ì •ì±… \(\pi\)ê°€ ê°œì…í•œë‹¤. ìœ— ì²¨ìë¡œ ì–´ë–¤ ì •ì±…ì„ ë”°ë¥¼ ë•Œì˜ ê°€ì¹˜ í•¨ìˆ˜ì¸ì§€ ì•Œë ¤ ì£¼ì–´ì•¼ ëª…í™•í•˜ë‹¤. \(Q^{\pi}(s, a)\), \(V^{\pi}(s)\) ì´ë ‡ê²Œ ì“´ë‹¤.</p>

\[Q^{\pi}(s, a)=\mathbb{E}_{\pi} \left[  \sum_{t'=t} r_{t'} \mid s_t = s, a_t = a \right]\]

\[V^{\pi}(s) =\mathbb{E}_{\pi} \left[  \sum_{t'=t} r_{t'} \mid s_t = s \right]\]

<p>ìƒí™©ê³¼ í–‰ë™ì˜ ê°€ì¹˜ í•¨ìˆ˜ \(Q^{\pi}\)ì˜ ê²½ìš°ë¥¼ ìƒê°í•´ ë³´ì. ì‹œì  \(t\)ì—ì„œ ìƒí™© \(s\)ì— ë†“ì—¬ ìˆë‹¤. ê·¸ ë•Œ \(a\)ë¼ëŠ” í–‰ë™ì„ í•˜ë¼ëŠ” ì¡°ê±´ì´ ì£¼ì–´ì¡Œë‹¤. ê±°ê¸°ì„œë¶€í„° í–‰ìœ„ìëŠ” ì •ì±… \(\pi\)ë¥¼ ë”°ë¼ ë¬´ìˆ˜íˆ ë§ì€ ë¯¸ë˜ë“¤ì„ ë§Œë“¤ì–´ë³¸ë‹¤. (ì´í›„ë¡œëŠ” í–‰ìœ„ìì™€ ì •ì±… \(\pi\)ë¥¼ ë™ì¼ì‹œí•˜ê² ë‹¤.) ê·¸ë ‡ê²Œ ë§Œë“¤ì–´ì§„ ì—¬ëŸ¬ ë¯¸ë˜ë“¤ì—ì„œ ê³„ì‚°ë˜ëŠ” ëˆ„ì  ë³´ìƒì˜ í‰ê· ì„ \(Q^{\pi}\)ë¡œ ì •í•œë‹¤ê³  ë°›ì•„ë“¤ì´ë©´ ëœë‹¤. ìƒí™©ì˜ ê°€ì¹˜ í•¨ìˆ˜ \(V^{\pi}\)ë„ ë§ˆì°¬ê°€ì§€ë‹¤. ìƒí™© \(s\)ì—ì„œ ì •ì±… \(\pi\)ë¥¼ ë”°ë¼ ë¬´ìˆ˜íˆ ë§ì€ ë¯¸ë˜ë¥¼ ë§Œë“¤ê³  ëˆ„ì  ë³´ìƒì˜ í‰ê· ì„ ê³„ì‚°í•œë‹¤. ë‹¤ë§Œ ì²˜ìŒ í–‰ë™ê¹Œì§€ ì •ì±… \(\pi\)ì—ê²Œ ë§¡ê²¨ ë²„ë¦°ë‹¤ëŠ” ì°¨ì´ê°€ ìˆë‹¤.</p>

<p>\(s\)ì™€ \(a\)ëŠ” ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” ë°ì´í„°ë¼ëŠ” ì ì„ ì§šê³  ë„˜ì–´ê°€ì•¼ í•œë‹¤. ì •ì±… \(\pi\)ë¥¼ ê³ ìˆ˜í•  ë•Œ í˜„ì‹¤ì ìœ¼ë¡œ ë§ë‹¥ëœ¨ë¦´ ì¼ì´ ì—†ëŠ” \(s\)ì™€ \(a\)ì— ëŒ€í•´ì„œë„ ì´ë¡ ìƒ \(Q^{\pi}(s, a)\) ëŠ” ì •ì˜ëœë‹¤. ê°•ì›ë„ í† ë°•ì´ ì² ìˆ˜(\(\pi\))ê°€ ì„¸ë ê²Œí‹° í•œë³µíŒ(\(s\))ì—ì„œ ì‚¼ê²¹ì‚´ì„ êµ¬ì›Œë¨¹ëŠ”ê²Œ(\(a\)) ì–¼ë§ˆë‚˜ í˜„ëª…í•œ ì¼ì¸ì§€(\(Q^{\pi}(s, a)\)) ì§ì‘ì´ì•¼ í•´ ë³¼ ìˆ˜ ìˆë‹¤ëŠ” ë§ì´ë‹¤. ë¬¼ë¡  \(Q^{\pi}(s, a)\)ë¥¼ ì¶”ì •í•´ë³´ê³  ì‹¶ë‹¤ë©´ ì„¸ë ê²Œí‹° ì‚¼ê²¹ì‚´ íŒŒí‹°ì— ì² ìˆ˜ë¥¼ ë°ë ¤ë‹¤ ë†“ê³  ë¯¸ë˜ë¥¼ ê´€ì°°í•´ë³´ì•„ì•¼ í•œë‹¤. í˜„ì¬ë¡œì„œëŠ” ìœ ì¼í•œ ë°©ë²•ì´ë‹¤. íƒ€ì„ë¨¸ì‹ ì´ ìˆëŠ” ê²½ìš°ì—ëŠ” ì—¬ëŸ¬ë²ˆ ê´€ì°°í•´ë³´ë©´ ë” ì¢‹ë‹¤.</p>

<p>ë§Œì•½ ì‹ ê²½ë§ìœ¼ë¡œ \(Q^{\pi}(s, a)\)ë¥¼ í‘œí˜„í•˜ê³  ì‹¶ë‹¤ë©´ ê°„ë‹¨í•˜ë‹¤. \(s\)ì™€ \(a\)ë¥¼ ì…ë ¥ìœ¼ë¡œ, ì´í›„ ìˆ˜ë§ì€ ë¯¸ë˜ì—ì„œ ê³„ì‚°í•œ ëˆ„ì  ë³´ìƒë“¤ì˜ í‰ê· ì„ ì •ë‹µìœ¼ë¡œ ë‘ê³  í•™ìŠµì‹œí‚¤ë©´ ëœë‹¤. ë¯¸ë˜ë¥¼ ì—¬ëŸ¬ ë²ˆ ê´€ì¸¡í•  ìˆ˜ ìˆë‹¤ë©´ ë‹¨ìˆœí•œ ì§€ë„í•™ìŠµì— ë¶ˆê³¼í•˜ë‹¤. ê·¸ëŸ¬ë‚˜ ì¸ê³¼ìœ¨ì„ ì¡´ì¤‘í•˜ì. ë¯¸ë˜ë¥¼ ì—¬ëŸ¬ ë²ˆ ê´€ì¸¡í•  ìˆ˜ëŠ” ì—†ë‹¤. ê·¸ëŸ¬ë‹ˆ í˜„ì‹¤ì ìœ¼ë¡œëŠ” ë‹¨ í•˜ë‚˜ì˜ ë¯¸ë˜ì—ì„œ ì–»ì€ ëˆ„ì  ë³´ìƒì„ ì •ë‹µìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ë°–ì— ì—†ë‹¤. ì •ë§ë¡œ ê·¸ë ‡ê²Œ êµ¬í˜„í•œë‹¤. ê·¸ë˜ë„ ì‘ë™ì„ í½ ì˜ í•œë‹¤. (ì‚¬ì¡±: ì•½ê°„ì€ ì¶”ìƒì ì¸ ì´ì•¼ê¸°ì§€ë§Œ ì–´ì°¨í”¼ í‰ê·  ì œê³± ì˜¤ì°¨ê°€ ì‘ì•„ì§€ë„ë¡ í•™ìŠµì‹œí‚¤ë©´ ê°€ëŠ¥í•œ ê°’ë“¤ì´ ì´ë£¨ëŠ” ë¶„í¬ì˜ ê¸°ëŒ“ê°’ì„ í•™ìŠµí•˜ëŠ” ì…ˆì´ë‹ˆ ê´œì°®ë‹¤. <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">ì´ êµì¬</a>ì˜ 18 í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì.)</p>

<p>ì •ì±… \(\pi\)ë¥¼ ë”°ë¥¼ ë•Œì˜ ê°€ì¹˜ í•¨ìˆ˜ \(Q^{\pi}\)ë¥¼ ì•Œê³  ì‹¶ì€ ìƒí™©ì´ë‹¤. ê·¸ëŸ¬ë‹ˆ ì •ë‹µì„ ê³„ì‚°í•  ë•Œ í•„ìš”í•œ ë¯¸ë˜ëŠ” ë°˜ë“œì‹œ \(\pi\)ê°€ ë§Œë“  ë¯¸ë˜ì—¬ì•¼ë§Œ í•œë‹¤ëŠ” ì‚¬ì‹¤ì— ìœ ì˜í•˜ì. ë‹¤ë¥¸ ì •ì±… \(\pi^{\beta}\)ê°€ ë§Œë“  ë¯¸ë˜ë¥¼ ì´ìš©í•´ ì •ë‹µì„ ê³„ì‚°í•œë‹¤ë©´ \(Q^{\pi^{\beta}}\)ë¥¼ ì•Œê²Œ ë˜ëŠ” ì…ˆì´ë‹¤. ì•„ë¬´ëŸ° ì˜ë¯¸ê°€ ì—†ë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ í–‰ìœ„ìì™€ ì •ì±… \(\pi\)ê°€ ì–´ë–»ê²Œ ë¯¸ë˜ë¥¼ ë§Œë“¤ì–´ ê°€ëŠ”ì§€ ë”°ì ¸ë³¼ í•„ìš”ê°€ ìˆë‹¤. ìœ„ì—ì„œ ì´ì•¼ê¸°í–ˆë“¯ì´ \(s_t\)ì—ì„œ \(a_t\)ë¥¼ í•˜ëŠ”ê±´ ì¡°ê±´ì´ë‹¤. í–‰ìœ„ìë¥¼ \(s_t\)ë¡œ ëŒì–´ì™€ ê°•ì œë¡œ \(a_t\)ë¥¼ ì‹œì¼œë³¸ë‹¤. \(s_t\)ì—ì„œ \(a_t\)ë¥¼ í–ˆì„ ë•Œ ì–´ë–¤ ë¯¸ë˜ \(s_{t+1}\)ë¥¼ ë§ˆì£¼í•˜ê²Œ ë ëŠ”ì§€ëŠ” ëª¨ë¥¸ë‹¤. ê·¸ëŸ¬ë‚˜ ì ì–´ë„ í–‰ìœ„ìì˜ ì†Œê´€ì€ ì•„ë‹ˆë‹¤. í–‰ìœ„ìëŠ” ì´ë¯¸ í–‰ë™ì„ ì–µì§€ë¡œ í•´ ë²„ë¦° ìƒí™©ì´ë‹¤. í–‰ìœ„ìëŠ” ì†ì„ ë†“ê³  ë³´ìƒ \(r_t\)ì™€ í•¨ê»˜ \(s_{t+1}\)ë¥¼ ê²¸í—ˆíˆ ë§ì´í•œë‹¤. ê·¸ ì´í›„ë‚˜ ë˜ì–´ì•¼ ë¹„ë¡œì†Œ í–‰ìœ„ìì˜ ì´ì„±ì´ ê°œì…í•œë‹¤. í–‰ìœ„ìëŠ” ì •ì±… \(\pi\)ì— ë”°ë¼ \(a_{t+1}\)ë¥¼ ê²°ì •í•œë‹¤. ì™œ í–‰ìœ„ìê°€ \(a_{t+1}\)ë¥¼ ê²°ì •í•˜ëŠ”ê°€? \(Q^{\pi}(s_t, a_t)\)ì˜ ì •ì˜ëŠ” \(s_t\)ì—ì„œ \(a_t\)ë¥¼ í–ˆì„ ë•Œ, ì •ì±… \(\pi\)ë¥¼ ë”°ë¼ê°€ë©´ ë¯¸ë˜ì— ë°›ì„ ìˆ˜ ìˆëŠ” ëˆ„ì  ë³´ìƒì˜ ê¸°ëŒ“ê°’ì´ë¼ê³  í–ˆì—ˆë‹¤. ì •ì˜ì— ì¶©ì‹¤í•˜ê²Œ \(s_t\)ì™€ \(a_t\) ì´í›„ë¡œ ì •ì±… \(\pi\)ì— ë”°ë¼ í–‰ë™í•˜ê³  ìˆë‹¤.</p>

<p>\(Q^{\pi}(s_t, a_t)\)ë¥¼ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§ˆ ë°ì´í„°ë¥¼ ì‹œê°„ ìˆœì„œì— ë”°ë¼ ì¨ ë³´ì.</p>

<p>\(s_t\) \(\rightarrow\) \(a_t\) \(\rightarrow\) \({\color{#642EFE} {r_t}}\) \(\rightarrow\) \(s_{t+1}\) \(\rightarrow\) \(a_{t+1}\) \(\rightarrow\) \({\color{#642EFE} {r_{t+1}}}\) \(\rightarrow\) \(s_{t+2}\) \(\rightarrow\) \(\dots\)</p>

<p>\(s_t\)ì™€ \(a_t\)ì—ì„œ ì‹œì‘í•˜ëŠ” í•˜ë‚˜ì˜ ë¯¸ë˜ë§Œ ê³ ë ¤í•´ ì •ë‹µì„ ë§Œë“¤ê¸°ë¡œ í–ˆì—ˆë‹¤. ê·¸ëŸ¬ë‹ˆ ì…ë ¥ì€ \(s_t\)ì™€ \(a_t\), ì •ë‹µì€ \(\sum_{t'=t}r_{t'}\)ë¡œ ì¡ê³  ì‹ ê²½ë§ì„ í›ˆë ¨ì‹œí‚¤ë©´ ì¶©ë¶„í•˜ë‹¤.</p>

<details>
<summary>Talk is cheap. Show me the code. â”“ </summary>
<div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"CartPole-v1"</span><span class="p">)</span>

<span class="n">STATE_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">ACTION_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.0005</span>

<span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">array</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">calculate_returns</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span>
    <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">rewards</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">R</span> <span class="o">+</span> <span class="n">r</span>
        <span class="n">returns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">returns</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="s">"""
Note that it's handy if we consider Q(s, a) -&gt; Q(s)[a] if the action space is discrete. 
We could still implement in the form of Q(s, a)
"""</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">))</span>

<span class="n">optimizer_actor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">optimizer_critic</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">LEARNING_RATE</span><span class="p">)</span>

<span class="n">track_performance</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">STATE_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ACTION_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">REWARD_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Interacting with the environment. (i.e. Generating single trajectory.)
</span>    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
        <span class="n">sampled_action</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">action_probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">sampled_action</span><span class="p">)</span>

        <span class="n">STATE_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">ACTION_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sampled_action</span><span class="p">)</span>
        <span class="n">REWARD_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="c1"># Concatenation to tensors.
</span>    <span class="n">STATE_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">STATE_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">ACTION_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ACTION_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">RETURN_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">calculate_returns</span><span class="p">(</span><span class="n">REWARD_MEMORY</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="s">"""
    Critic (Value) Learning Phase
    """</span>
    <span class="c1"># Cumulative-rewards weighted version of negative log likelihoods.
</span>    <span class="n">expected_state_action_values</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ACTION_TENSORS</span><span class="p">)</span>
    <span class="n">single_true_cumulative_rewards</span> <span class="o">=</span> <span class="n">RETURN_TENSORS</span>
    <span class="n">critic_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">expected_state_action_values</span> <span class="o">-</span> <span class="n">single_true_cumulative_rewards</span><span class="p">).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    <span class="c1"># Optimization as usual.
</span>    <span class="n">optimizer_critic</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">critic_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_critic</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="s">"""
    Actor (Policy) Learning Phase
    """</span>
    <span class="c1"># Cumulative-rewards weighted version of negative log likelihoods.
</span>    <span class="n">likelihoods</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ACTION_TENSORS</span><span class="p">)</span>
    <span class="n">log_likelihoods</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">likelihoods</span><span class="p">)</span> <span class="c1"># As if it were a supervised learning problem.
</span>    <span class="n">assignmented_credits</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ACTION_TENSORS</span><span class="p">)</span>
    <span class="n">weighted_log_likelihoods</span> <span class="o">=</span> <span class="n">log_likelihoods</span> <span class="o">*</span> <span class="n">assignmented_credits</span>

    <span class="c1"># Optimization as usual.
</span>    <span class="n">pseudo_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">weighted_log_likelihoods</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">optimizer_actor</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">pseudo_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_actor</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Keeping track of performace of the algorithm.
</span>    <span class="n">track_performance</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">REWARD_MEMORY</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">f"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">th Trial -&gt; </span><span class="si">{</span><span class="n">track_performance</span><span class="o">/</span><span class="mi">100</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="n">track_performance</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div>    </div>

  </div>
</details>

<h2 id="ì²œì§€-ê°œë²½">ì²œì§€ ê°œë²½</h2>

<p>\(Q^{\pi}(s_t, a_t)\)ë¥¼ ì¶”ì •í•˜ê³  ì‹¶ë‹¤ë©´ ì´ ê¸¸ê³  ê¸´ ì‚¬ìŠ¬ê³¼ ê°™ì€ ë°ì´í„°ëŠ” í–‰ìœ„ìê°€ ë§Œë“¤ì—ˆì–´ì•¼ í•œë‹¤ê³  ê°•ì¡°í–ˆë‹¤. ê·¸ëŸ¬ë‚˜ í–‰ìœ„ìê°€ ë°ì´í„°ì— ê°œì…í•œ í”ì ì€ \(a_{t+1}\)ë¶€í„° ë“œëŸ¬ë‚œë‹¤. ê·¸ëŸ¬ë‹ˆ ê·¸ ì „ê¹Œì§€ëŠ” í–‰ìœ„ìì™€ ë¬´ê´€í•œ ë°ì´í„°ë‹¤. ë§Œì•½ \(Q^{\pi}\)ë¥¼ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ì–´ë–¤ ë°©ë²•ë¡ ì´ \(a_{t+1}\) ì´í›„ì˜ ë°ì´í„°ë¥¼ ìš”êµ¬í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ì´ë¡  ìƒ í–‰ìœ„ìì™€ ë¬´ê´€í•œ ë°ì´í„°ë¡œë„ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤. ëŒ€ë‹¨íˆ ì¤‘ìš”í•œ ê´€ì°°ì´ë‹¤. ì²œì§€ê°€ ê°œë²½í•  ì¼ì´ë‹¤. ì •ì±… \(\pi\)ì˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ \(\pi\)ì˜ í”ì ì´ ì—†ëŠ” ë°ì´í„°ë¡œë„ ì•Œì•„ë‚¼ ìˆ˜ ìˆë‹¤ë‹ˆ. ì² ìˆ˜ê°€ ë§Œë“œëŠ” ê°€ì¹˜ë¥¼ ì˜í¬ì˜ í–‰ë™ë§Œìœ¼ë¡œ ì•Œì•„ë‚¼ ìˆ˜ ìˆë‹¤ë‹ˆ! ì˜ë¦¬í•œ ë…ìë¼ë©´ ì˜êµ¬ì‹¬ì´ ë“¤ì–´ì•¼ ì •ìƒì´ë‹¤. ì´ ì´ì•¼ê¸°ê°€ ë“±ì¥í•˜ë ¤ë©´ ì¡°ê¸ˆ ë” ê¸°ë‹¤ë ¤ì•¼ í•œë‹¤. ê·¸ëŸ¬ë‹ˆ ì ê¹ ìŠê³  ì›ë˜ ì´ì•¼ê¸°ë¥¼ ì´ì–´ê°€ì.</p>

<h2 id="ë°°ìš°-ìˆ˜ì—…">ë°°ìš° ìˆ˜ì—…</h2>

<p><a href="/rl-story-1/">ì§€ë‚œ ì´ì•¼ê¸°</a>ì—ì„œ ë§í–ˆë“¯ì´ ê°€ì¹˜ í•¨ìˆ˜ \(Q^{\pi}\)ë¥¼ ì•Œê³  ìˆë‹¤ë©´ í–‰ìœ„ìì˜ ì •ì±… \(\pi\)ë¥¼ êµì •í•  ë•Œ ê° í–‰ë™ë“¤ì˜ ê¸°ì—¬ë„ë¥¼ ë” í•©ë¦¬ì ìœ¼ë¡œ íŒë‹¨í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¨ ë³´ì.</p>

<ol>
  <li>ì •ì±… \(\pi\)ë¥¼ ì´ìš©í•´ ë°ì´í„°ë¥¼ ëª¨ì€ë‹¤.</li>
  <li>ì •ì±… \(\pi\)ì˜ ê°€ì¹˜ í•¨ìˆ˜ \(Q^{\pi}\)ë¥¼ ì¶”ì •í•œë‹¤.</li>
  <li>ì¶”ì •í•œ ê°€ì¹˜í•¨ìˆ˜ \(Q^{\pi}\)ë¥¼ ê·¼ê±°ë¡œ ì •ì±… \(\pi\)ì˜ í–‰ë™ì„ êµì •í•œë‹¤.</li>
</ol>

<p>ê·¸ëŸ°ë° 3ë‹¨ê³„ì—ì„œ ì •ì±… \(\pi\)ê°€ ì¡°ê¸ˆì´ë¼ë„ ë³€í•˜ëŠ” ìˆœê°„ ê°€ì¹˜ í•¨ìˆ˜ \(Q^{\pi}\)ëŠ” ë¬´ìš©ì§€ë¬¼ì´ ëœë‹¤. ì •ì±…ì´ ë³€í–ˆìœ¼ë‹ˆ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ë‹¤ì‹œ ì°¾ì•„ì•¼ í•œë‹¤. ì ˆë§ì ì´ë‹¤. ê·¸ë˜ë„ ê²½ì‚¬ í•˜ê°•ë²•ì€ ì •ì±…ì„ í¬ê²Œ ë°”ê¾¸ì§€ ì•Šìœ¼ë‹ˆ ë‹¤í–‰ì´ë‹¤. ë•ë¶„ì— \(\pi\)ì™€ \(Q^{\pi}\)ë¥¼ í•œ ë²ˆì”© ë²ˆê°ˆì•„ í•™ìŠµì‹œí‚¤ê¸°ë§Œ í•´ë„ ì¶©ë¶„í•˜ë‹¤. ë°”ë€Œê¸° ì „ ì •ì±…ì— ëŒ€í•œ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ì´ˆê¹ƒê°’ ì‚¼ì•„ ë°”ë€ í›„ì˜ ì •ì±…ì— ëŒ€í•œ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ì°¾ëŠ” ì…ˆì´ë‹¤. ì´ëŸ° ë¯¸ë¬˜í•œ ë¶€ë¶„ë“¤ê¹Œì§€ ê¼¼ê¼¼íˆ ì§šê³  ë„˜ì–´ê°€ì•¼ ì•ˆ í—·ê°ˆë¦°ë‹¤.</p>

<p>ì •ì±… \(\pi\)ì™€ ê°€ì¹˜ í•¨ìˆ˜ \(Q^{\pi}\)ì„ í‘œí˜„í•˜ëŠ”ë°ëŠ” ë…ë¦½ëœ ë‘ ì‹ ê²½ë§ì„ ì‚¬ìš©í•œë‹¤. ê·¸ëŸ¬ë‹ˆ íƒœìƒì ìœ¼ë¡œ ë¶ˆì•ˆì • í•  ìˆ˜ ë°–ì— ì—†ë‹¤. ì§€ë‚œ ì´ì•¼ê¸°ì—ì„œ ë…¼í–ˆë˜ëŒ€ë¡œ \(V^{\pi}\) ê¹Œì§€ ìˆë‹¤ë©´ ë” ë‚˜ì€ ì‹ ë¢° í• ë‹¹ì„ í•  ìˆ˜ ìˆë‹¤. \(V^{\pi}\)ë¥¼ í‘œí˜„í•˜ëŠ” ì„¸ ë²ˆì§¸ ì‹ ê²½ë§ì„ ë„ì…í•˜ë©´ ëœë‹¤.</p>

<details id="inside">
<summary>Talk is cheap. Show me the code. â”“ </summary>
<div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"CartPole-v1"</span><span class="p">)</span>

<span class="n">STATE_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">ACTION_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.0001</span>

<span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">array</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">calculate_returns</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span>
    <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">rewards</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">R</span> <span class="o">+</span> <span class="n">r</span>
        <span class="n">returns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">returns</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">))</span>


<span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">optimizer_actor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">optimizer_critic_q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">optimizer_critic_v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">LEARNING_RATE</span><span class="p">)</span>

<span class="n">track_performance</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">STATE_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ACTION_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">REWARD_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Interacting with the environment. (i.e. Generating single trajectory.)
</span>    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
        <span class="n">sampled_action</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">action_probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">sampled_action</span><span class="p">)</span>

        <span class="n">STATE_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">ACTION_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sampled_action</span><span class="p">)</span>
        <span class="n">REWARD_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="c1"># Concatenation to tensors.
</span>    <span class="n">STATE_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">STATE_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">ACTION_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ACTION_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">RETURN_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">calculate_returns</span><span class="p">(</span><span class="n">REWARD_MEMORY</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="s">"""
    Critic Q Learning Phase
    """</span>
    <span class="c1"># Cumulative-rewards weighted version of negative log likelihoods.
</span>    <span class="n">expected_state_action_values</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ACTION_TENSORS</span><span class="p">)</span>
    <span class="n">single_true_cumulative_rewards</span> <span class="o">=</span> <span class="n">RETURN_TENSORS</span>
    <span class="n">critic_q_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">expected_state_action_values</span> <span class="o">-</span> <span class="n">single_true_cumulative_rewards</span><span class="p">).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    <span class="c1"># Optimization as usual.
</span>    <span class="n">optimizer_critic_q</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">critic_q_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_critic_q</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="s">"""
    Critic V Learning Phase
    """</span>
    <span class="c1"># Cumulative-rewards weighted version of negative log likelihoods.
</span>    <span class="n">expected_state_action_values</span> <span class="o">=</span> <span class="n">v</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">)</span>
    <span class="n">single_true_cumulative_rewards</span> <span class="o">=</span> <span class="n">RETURN_TENSORS</span>
    <span class="n">critic_v_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">expected_state_action_values</span> <span class="o">-</span> <span class="n">single_true_cumulative_rewards</span><span class="p">).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    <span class="c1"># Optimization as usual.
</span>    <span class="n">optimizer_critic_v</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">critic_v_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_critic_v</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="s">"""
    Actor (Policy) Learning Phase
    """</span>
    <span class="c1"># Cumulative-rewards weighted version of negative log likelihoods.
</span>    <span class="n">likelihoods</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ACTION_TENSORS</span><span class="p">)</span>
    <span class="n">log_likelihoods</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">likelihoods</span><span class="p">)</span> <span class="c1"># As if it were a supervised learning problem.
</span>    <span class="n">assignmented_credits</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ACTION_TENSORS</span><span class="p">)</span> <span class="o">-</span> <span class="n">v</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">)</span>
    <span class="n">weighted_log_likelihoods</span> <span class="o">=</span> <span class="n">log_likelihoods</span> <span class="o">*</span> <span class="n">assignmented_credits</span>

    <span class="c1"># Optimization as usual.
</span>    <span class="n">pseudo_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">weighted_log_likelihoods</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">optimizer_actor</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">pseudo_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_actor</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Keeping track of performace of the algorithm.
</span>    <span class="n">track_performance</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">REWARD_MEMORY</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">f"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">th Trial -&gt; </span><span class="si">{</span><span class="n">track_performance</span><span class="o">/</span><span class="mi">100</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="n">track_performance</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div>    </div>

  </div>
</details>

<p>ê·¸ëŸ¬ë‚˜ \(Q^{\pi}\)ì™€ \(V^{\pi}\)ë¥¼ í‘œí˜„í•˜ëŠ” ì‹ ê²½ë§ì„ ë”°ë¡œ ê°€ì§€ê³  ìˆëŠ”ë‹¤ë©´ ë‚­ë¹„ë‹¤. ë‹¤ì†Œ ë¶€ì •í™•í• ì§€ë¼ë„ \(Q^{\pi}\)ë¡œ \(V^{\pi}\)ë¥¼ í‘œí˜„í•˜ê±°ë‚˜ \(V^{\pi}\)ë¥¼ \(Q^{\pi}\)ë¡œ í‘œí˜„í•´ ë°©ë²•ë¡ ì„ ë‹¨ìˆœí•˜ê²Œ ë§Œë“¤ì–´ì•¼ ì¢‹ë‹¤. ì´ëŸ° ë§¥ë½ì—ì„œëŠ” \(Q^{\pi}\)ë¥¼ \(V^{\pi}\)ì˜ ê´€ì ìœ¼ë¡œ í‘œí˜„í•œë‹¤. ì´ë ‡ê²Œ ë§ì´ë‹¤.</p>

\[Q^{\pi}(s_t, a_t) \approx r_t + V^{\pi}(s_{t+1})\]

<p>ì¢Œë³€ê³¼ ìš°ë³€ì€ ë‹¤ë¥¸ ì–‘ì´ë‹¤. \(s_t\)ì—ì„œ \(a_t\)ë¥¼ í–ˆì„ì§€ë¼ë„ ë†“ì´ê²Œ ë˜ëŠ” ë‹¤ìŒ ìƒíƒœ \(s_{t+1}\)ì€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì •í™•íˆëŠ” ì´ë ‡ê²Œ ê¸°ìˆ í•´ì•¼ ì˜³ë‹¤.</p>

\[Q^{\pi}(s_t, a_t) = \mathbb{E}_{s_{t+1}} \left[ r_t + V^{\pi}(s_{t+1})\right]\]

<p>ë…ìë“¤ ìŠ¤ìŠ¤ë¡œ ì´ ë“±ì‹ê³¼ ê·¼ì‚¬ì‹ì˜ í•¨ì˜ë¥¼ ê³ ì°°í•´ë³´ê¸¸ ë°”ë€ë‹¤. \(Q^{\pi}\)ì™€ \(V^{\pi}\)ì˜ ì •ì˜ë¥¼ ìƒê¸°í•˜ë©° ë‘ í•¨ìˆ˜ê°€ ì–´ë–¤ ê´€ê³„ë¡œ ì—®ì—¬ìˆì„ì§€ ì°¬ì°¬íˆ ìƒê°í•´ë³´ë©´ ëœë‹¤. \(V^{\pi}\)ë¥¼ \(Q^{\pi}\)ì˜ ê¸°ëŒ“ê°’ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ë„ ìˆë‹¤. ì´ëŸ° ê´€ì ì€ ì´ë¯¸ <a href="/rl-story-1/">ì§€ë‚œ ì´ì•¼ê¸°</a>ì—ì„œ ë“±ì¥í–ˆì—ˆë‹¤.</p>

<details id="inside">
<summary>Talk is cheap. Show me the code. â”“ </summary>
<div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"CartPole-v1"</span><span class="p">)</span>

<span class="n">STATE_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">ACTION_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.0001</span>

<span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">array</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">calculate_returns</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span>
    <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">rewards</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">R</span> <span class="o">+</span> <span class="n">r</span>
        <span class="n">returns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">returns</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">),</span>
                       <span class="n">nn</span><span class="p">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">optimizer_actor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">optimizer_critic</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">LEARNING_RATE</span><span class="p">)</span>

<span class="n">track_performance</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">STATE_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">NEXT_STATE_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ACTION_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">REWARD_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">DONE_MEMORY</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Interacting with the environment. (i.e. Generating single trajectory.)
</span>    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
        <span class="n">sampled_action</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">action_probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">sampled_action</span><span class="p">)</span>

        <span class="n">STATE_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">NEXT_STATE_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
        <span class="n">ACTION_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sampled_action</span><span class="p">)</span>
        <span class="n">REWARD_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
        <span class="n">DONE_MEMORY</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">done</span> <span class="o">*</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="c1"># Concatenation to tensors.
</span>    <span class="n">STATE_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">STATE_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">NEXT_STATE_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">NEXT_STATE_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">ACTION_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ACTION_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">REWARD_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">REWARD_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">RETURN_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">calculate_returns</span><span class="p">(</span><span class="n">REWARD_MEMORY</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">DONE_TENSORS</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">DONE_MEMORY</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="s">"""
    Critic V Learning Phase
    """</span>
    <span class="c1"># Cumulative-rewards weighted version of negative log likelihoods.
</span>    <span class="n">expected_state_action_values</span> <span class="o">=</span> <span class="n">v</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">)</span>
    <span class="n">single_true_cumulative_rewards</span> <span class="o">=</span> <span class="n">RETURN_TENSORS</span>
    <span class="n">critic_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">expected_state_action_values</span> <span class="o">-</span> <span class="n">single_true_cumulative_rewards</span><span class="p">).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    <span class="c1"># Optimization as usual.
</span>    <span class="n">optimizer_critic</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">critic_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_critic</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="s">"""
    Actor (Policy) Learning Phase
    """</span>
    <span class="c1"># Cumulative-rewards weighted version of negative log likelihoods.
</span>    <span class="n">likelihoods</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ACTION_TENSORS</span><span class="p">)</span>
    <span class="n">log_likelihoods</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">likelihoods</span><span class="p">)</span> <span class="c1"># As if it were a supervised learning problem.
</span>    <span class="n">assignmented_credits</span> <span class="o">=</span> <span class="n">REWARD_TENSORS</span> <span class="o">+</span> <span class="n">v</span><span class="p">(</span><span class="n">NEXT_STATE_TENSORS</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">DONE_TENSORS</span><span class="p">)</span> <span class="o">-</span> <span class="n">v</span><span class="p">(</span><span class="n">STATE_TENSORS</span><span class="p">)</span>
    <span class="n">weighted_log_likelihoods</span> <span class="o">=</span> <span class="n">log_likelihoods</span> <span class="o">*</span> <span class="n">assignmented_credits</span>

    <span class="c1"># Optimization as usual.
</span>    <span class="n">pseudo_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">weighted_log_likelihoods</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">optimizer_actor</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">pseudo_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_actor</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Keeping track of performace of the algorithm.
</span>    <span class="n">track_performance</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">REWARD_MEMORY</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">f"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">th Trial -&gt; </span><span class="si">{</span><span class="n">track_performance</span><span class="o">/</span><span class="mi">100</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="n">track_performance</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div>    </div>

  </div>
</details>

<p>ì •ë¦¬í•´ë³´ì. ê´„í˜¸ ì•ˆì— ë¬´ì—‡ì„ ë„£ì–´ì•¼ ë” ë‚˜ì€ ê¸°ì—¬ë„ í• ë‹¹ì´ ê°€ëŠ¥í• ì§€ ìƒê°í•´ ë³´ì•˜ë‹¤.</p>

\[\sum_t (\ \ \ ) \log \pi_{\theta}(a_t|s_t)\]

<p>ì²« ë²ˆì§¸ë¡œëŠ” ëˆ„ì  ë³´ìƒì˜ ê¸°ëŒ“ê°’ì¸ \(Q^{\pi}(s_t, a_t)\)ê°€ ìˆì—ˆê³ , ë‘ ë²ˆì§¸ë¡œëŠ” í–‰ìœ„ì˜ ì¢‹ê³  ë‚˜ì¨ì„ ìƒëŒ€ì ìœ¼ë¡œ ê³ ë ¤í•  ìˆ˜ ìˆë„ë¡ \(V^{\pi}(s_t)\)ì™€ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•œ \(Q^{\pi}(s_t, a_t)\) \(- V^{\pi}(s_t)\)ê°€ ìˆì—ˆë‹¤. ë‘ ë²ˆì§¸ ì–‘ì„ Advantageë¼ ë¶€ë¥´ê³  ì´ ì–‘ì„ ì´ìš©í•œ Policy Gradient ë°©ë²•ë¡ ì—ëŠ” Advantage Actor-Critic (A2C)ë¼ëŠ” ì´ë¦„ì´ ë¶™ì–´ìˆë‹¤. A2Cë¥¼ CPUì˜ ì—¬ëŸ¬ ì“°ë ˆë“œì—ì„œ ë³‘ë ¬ì  + ë¹„ë™ê¸°ì ìœ¼ë¡œ ëŒë¦¬ìëŠ” ì•„ì´ë””ì–´ê°€ ê·¸ ìœ ëª…í•œ <a href="https://arxiv.org/abs/160201783">A3C</a> ë˜ì‹œê² ë‹¤.</p>

\[A^{\pi}(s_t, a_t) = Q^{\pi}(s_t, a_t)  - V^{\pi}(s_t)\]

<p>ì‹ ê²½ë§ì„ \(Q^{\pi}\) ë”°ë¡œ \(V^{\pi}\) ë”°ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë‹ˆ Advantageì˜ ê·¼ì‚¬ë¡œì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ \(r_t + V^{\pi}(s_{t+1}) - V^{\pi}(s_{t})\)ë¥¼ ì‚¬ìš©í•œë‹¤ê³  í–ˆë‹¤. ì´ë¥¼ ì¡°ê¸ˆ ë” ì¼ë°˜í™”í•˜ì—¬ ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ Advantageë¥¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. ê´€ì‹¬ìˆëŠ” ë…ìëŠ” <a href="https://arxiv.org/abs/1506.02438">GAE</a>ë¥¼ ì½ì–´ë³´ë©´ ì¢‹ë‹¤. ì´í›„ ë§¥ë½ì—ì„œ \({\hat{A_t}}\)ë¼ëŠ” ê¸°í˜¸ê°€ ë‚˜ì˜¨ë‹¤ë©´ ë¬´ì–¸ê°€ í•œ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ Advantageë¥¼ ì¶”ì •í•œ ê°’ì´ë¼ ì´í•´í•˜ë©´ ëœë‹¤.</p>

<p>ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ë„ì…í–ˆë‹¤ê³  ì§€ë‚œ ì´ì•¼ê¸°ì˜ ë§ë¯¸ì— ë“±ì¥í–ˆë˜ ë¬¸ì œì ë“¤ì´ í•´ê²°ë˜ëŠ”ê±´ ì•„ë‹ˆë‹¤. ë°›ëŠ” ë³´ìƒì´ ì—†ìœ¼ë©´ ì •ë‹µë„ í•­ìƒ \(0\)ì´ë‹¤. ê·¸ëŸ¬ë©´ ê°€ì¹˜ í•¨ìˆ˜ëŠ” \(0\)ì„ ë±‰ëŠ” ë°©ë²•ì„ í•™ìŠµí•œë‹¤. ì§„ì •í•œ ë¬´ê°€ì¹˜ í•¨ìˆ˜ë¡œ ê±°ë“­ë‚œë‹¤. ì‡¼íœí•˜ìš°ì–´ê°€ ë”°ë¡œ ì—†ë‹¤. í˜¹ì‹œ ë¬´ê°€ì¹˜ í•¨ìˆ˜ê°€ í•„ìš”í•˜ë‹¤ë©´ ì‹œë„í•´ ë³´ì.</p>
:ET