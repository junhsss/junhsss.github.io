I"&<p>신경망 훈련은 트레이닝 데이터에 대한 가능도를 높이는 방향으로 이루어진다. 가령 신경망이 호랑이의 형상이 담겨있는 픽셀 덩어리를 먹었다고 하자. 이 때 신경망이 뱉는 확률 분포에서 호랑이에 해당하는 값이 가능도이고, 가능도가 늘어나도록 파라미터를 건드리는 것이 소위 신경망의 훈련이다. 이를 통해 신경망은 적어도 먹어보았던 것이 무엇인지는 알게 된다. 물론 정화수를 떠놓고 일반화의 마법이 발생하기를 빌어야 한다는 사실은 비밀이다.</p>

<p>강화 학습에서는 이러한 학습 방식이 불가능하다. 먹은 픽셀 덩어리에 호랑이의 형상이 담겨있었다는 사실을 친절하게 알려줄 선생님이 없다. 과거의 판단들이 얼마나 타당했는지 학습 주체가 스스로 판단해 본인의 행동을 교정해야 한다. 물론 무엇이 옳고 그른지 판단할 근거가 전혀 없다면 행동 교정이 가능할 턱이 없다. 그래서 보상이라는 신호가 주어진다.</p>

<p>강화 학습은 시간이 존재하는 상황에서의 의사 결정을 논하는 분야이다. 잠깐 강화 학습의 언어를 복습하고 넘어가자. 행위자는 자신이 놓인 상황에서 최적일 것이라 판단되는 행동을 한다. 그래서 행동을 하면 상황이 달라지고 보상을 받는다. 보상은 행동을 한 시점에서 그 행동이 얼마나 좋고 나빴는지 알려준다. 행위자는 시간에 따른 누적 보상이 가장 커지도록 행동하는 법을 배워야 한다.</p>

<p>호랑이를 보고 떡볶이라고 판단했는가? 괜찮다. 떡볶이는 맛있으니 보상을 +1만큼 주자. 다시보니 형체가 조금 더 가까워졌다. 이제는 호랑이가 아니라 고양이 같다. 고양이는 귀여우니 보상을 +10만큼 주자. 그러나 이렇게 잘못된 판단이 누적되면 언젠가는 호랑이의 식사가 되는 대가를 치른다. 행위자는 단기적인 보상이 커지도록 행동하면 안 된다.</p>

<p>이를 달성하는 방법은 간단하다. 행위자의 행동이 높은 누적 보상을 가져왔다면 당근을, 그렇지 못했다면 채찍을 주는 것으로 행동 정책을 교정한다. 경험을 통해 정책을 교정할 올바른 방향을 판단한다. 그래서 이러한 철학을 공유하는 방법론들에 Policy Gradient라는 이름이 붙어있다.</p>

<p>정답이 제공되어 있다고 가정하자. 다시 말해 상황 \(s\) 에서 행동 \(a\) 가 올바른 행위라는 사실을 인간 전문가가 알려주었다고 가정하자. 그렇다면 이 문제는 전형적인 지도 학습이다. 로그 가능도 \(\log \pi_{\theta} (a \mid s)\)를 높이는 방향으로 신경망을 학습시키면 간단하다. (현실적으로는 불충분하다. <a href="http://proceedings.mlr.press/v15/ross11a/ross11a.pdf">참고</a>) 만약 이러한 판단들이 순차적으로 제공되었다면 로그 가능도들을 시간에 따라 전부 더해주면 된다. 이 양이 늘어나도록 행위자를 장려하면 된다.</p>

\[\sum_t  \log \pi_{\theta} (a_t \mid s_t)\]

<p>그러나 앞서 말했다시피 강화학습에서는 \(s_t\) 에서 \(a_t\) 를 하도록 판단한 주체가 인간 전문가가 아니라 행위자이므로 위 방법론을 사용하는 것은 의미가 없다. 그러나 이를 자연스럽게 수정해 볼 수는 있다. \(s_t\)에서 했던 \(a_t\)가 정답은 아니지만, 얼마나 좋고 나쁜 행위인지 정량화할 수 있다면 이것을 가중치의 형태를 빌어 행동 교정의 신호로서 제공하고 싶다.</p>

\[\sum_t (\ \ \ ) \log \pi_{\theta}(a_t|s_t)\]

<p>행위자의 행동들은 순차적으로 누적되어 하나의 미래를 만들어 낸다. 미래는 좋을수도 있고 나쁠수도 있다. \(a_t\)가 그 미래에 얼마나 기여했는지 따지는 것은 쉬운 문제가 아니다. 미래는 아주 좋았지만 \(a_t\) 자체는 나쁜 행위일수도 있으며 그 반대일수도 있다. 어쩌면 \(s_t\)에서의 \(a_t\)가 좋은 미래를 만드는데 결정적인 요인이었을수도 있다. <a href="https://en.wikipedia.org/wiki/Causal_inference">인과 관계를 추론하는 것</a>이 뜨거운 주제인 이유가 있다. \(s_t\)에서의 \(a_t\)가 미래에 얼마나 기여했는지 판단하는 문제를 강화학습에서는 기여도 할당 문제라 부른다.</p>

<p>순진하게 생각하면 \(a_t\) 이후로 받은 보상을 모두 더해 행동 교정의 신호로 제공할 수 있다. 이 신호가 양수라면 그 행위를 강화하고 음수라면 그 행위를 약화하는 셈이다. 그래서 강화학습이라는 이름이 붙었다.</p>

\[\sum_t (\sum_{t'=t} r_{t'}) \log \pi_{\theta}(a_t|s_t)\]

<p>누적 보상이 얼마 이상이어야 비로소 ‘좋다’라고 할 수 있는가? 좋고 나쁨은 상대적인 개념이다. 가령 마지막 시점에는 항상 +100만큼의 보상이 주어지는 풍요로운 환경에 놓여있다면 가중치를 계산할 때 항상 -100은 빼 주어야 각 행위들의 좋고 나쁨을 조금 더 객관적인 눈으로 판단할 수 있을 것이다.</p>

\[\sum_t (\sum_{t'=t} r_{t'} - b) \log \pi_{\theta}(a_t|s_t)\]

<p>순차적인 의사결정을 해 나가는 상황에서 행위자가 했던 행동들을 일단은 정답이라 간주하고, 이후 얻은 보상의 합을 통해 그러한 행동들의 타당성을 사후적으로 판단하여 교정의 신호로 제공한다. 단순하면서도 직관적인 이 방법론은 REINFORCE라는 이름으로 1992년 처음 제안되었고 오늘날 정책 경사 방법론들의 효시가 되었다. 일견 말이 되는 방법론처럼 보이고 특정한 조건들을 만족하는 환경에서는 실제로도 작동을 잘 하는데, 가령 <a href="https://arxiv.org/abs/1611.01578">NAS</a>가 처음 제안되었을 때도 이 간단한 방법론만으로 재미를 톡톡히 보았다.</p>

<p>여기까지는 좋아보이지만 현실적으로 여러 문제가 있다. 가장 큰 문제는 보상을 받기가 매우 어려운 경우이다. 가령 복잡한 의사결정에 따른 행동을 반복해야 보상을 간신히 받는 환경에선 아직 미숙한 행위자가 아무리 행동해봐야 결국 받는 보상은 항상 0일테다. 따라서 학습이 일어나지 않는다. 이 문제는 REINFORCE 뿐만이 아니라 Model-Free 방법론들이 겪는 고질적인 문제이다. 환경에서 떨어지는 보상을 행동 교정의 신호로 사용하는것은 수동적이며 게으른 방식이다. 인간은 환경으로부터 보상을 받아보지 않고도 좋고 나쁨을 판단해 행위할 수 있다. 찍어먹어 보아야만 된장인줄 아는건 아니라는 말이다. 인간은 계획을 세워 행동한다는 말이고 이것은 세상이 어떻게 동작하는지 얼추 이해하고 있어야 가능하다. 이는 모델 기반 강화학습의 필연성을 시사한다. 도메인 지식을 가지고 보상 체계를 단계적으로 세심하게 디자인하거나, 호기심을 모델링해 그것을 보상의 신호로 이용하는 멋진 방법론들이 있기는 하지만 본질적인 해결책은 아니다.</p>

<p>분산이 높은 것도 심각한 문제중 하나이다. REINFORCE에서는 어떤 행동이 얼마나 좋았는지 판단하기 위해 해당 행동 이후에 받은 보상을 전부 더하고 있다. 만약 타임머신을 타고 그 행동을 했던 직후로 돌아갔다고 해 보자. 같은 행동을 했더라도 미래는 달라진다. 그것도 아주 많이. 발생 가능한 미래는 무한히 많고 우리가 관측한 미래는 그 중 하나에 불과하다. 그래서 행동 교정의 신호로 제공하기에 보상의 합이라는 신호는 매우 불안정하다.</p>

<p>만약 타임머신이 있어서 특정 행위 이후의 미래를 수십번 관찰할 수 있다면 각 미래들에서 받는 누적 보상의 평균을 강화 신호로 제공하면 되기야 하겠지만 타임머신이 있다 한들 어느 세월에 그러고 있겠는가. 그러나 가능하다면 특정 행위 이후에 발생하는 모든 미래의 누적 보상을 계산해 그것들의 평균을 강화의 신호로 제공하는 것이 최선이다. 이렇게 말이다.</p>

\[\sum_t Q(s_t, a_t) \log \pi_{\theta}(a_t|s_t)\]

<p>\(Q(s, a)\)는 상황 \(s\)에서 행동 \(a\)를 하고난 후 얻을 수 있는 누적 보상의 기댓값을 의미한다. 발생 가능한 모든 미래를 고려한다는 말이다. 위에서 언급했듯이 좋고 나쁨은 상대적이다. 가령 \(Q(s, a_1)=12\)이라고 해 보자. 이 수치는 좋은것이니 상황 \(s\)에서는 \(a_1\)을 하도록 행동을 강화해야 하는걸까? 그렇지 않다. 만약 가능한 다른 행위 \(a_2, a_3, ...\) 들을 했을 때 더 높은 누적 보상을 기대할 수 있다면 \(a_1\)는 나쁜 행동이다. 이를 고려해주기 위해 가능한 행위들에 대해 \(Q(s, a)\)의 기댓값 \(V(s)\)를 계산해 뺀다. 이를 Advantage Actor-Critic이라 부른다. \(Q\)와 \(V\)를 알아내는 방법은 다음 포스팅에서 다룬다.</p>

\[\sum_t \left[ Q(s_t, a_t)-V(s_t) \right] \log \pi_{\theta}(a_t|s_t)\]

<p>정책 경사 방법론들은 무의식적인 시행착오를 알고리즘의 형태로 구현해 놓은 것에 불과하다는 사실을 이해하자. <a href="https://www.youtube.com/watch?v=vGazyH6fQQ4">비둘기에게 탁구를 가르칠 수는 있지만</a> 그 뿐이다. 더 나은 알고리즘이 필요하다.</p>

<p>사족: 이해를 돕기 위해 인과를 비틀어 설명하였다. 조금 더 엄밀한 접근이 궁금한 독자들은 경사 추정 방법론들을 정리한 Deepmind의 <a href="https://arxiv.org/abs/1906.10652">이 논문</a>을 읽어보자. 참 잘 썼다.</p>
:ET