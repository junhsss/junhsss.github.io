I"Q<p>Q-Learning 계열의 방법론들은 최적의 정책이 할 행동을 상상한다. 그렇게 상상한 \(a'\)를 가지고 시간차 학습을 한다. \(Q(s, a)\)와 \(r+\gamma Q(s', a')\)가 비슷해지도록 학습해 나간다. \((s,\) \(a,\) \(r,\) \(s')\)는 진짜 데이터, \(a'\)는 상상한 데이터이기에 Off-Policy라는 성질이 발생한다. 그런데  최적의 정책이 할 행동을 상상하는 주체는 학습 중인 가치 함수 \(\hat{Q}\)다. 그러니 당연하게도 정확하지 않다. 이 오류가 어떤 문제를 야기하는지 따져보자.</p>

:ET