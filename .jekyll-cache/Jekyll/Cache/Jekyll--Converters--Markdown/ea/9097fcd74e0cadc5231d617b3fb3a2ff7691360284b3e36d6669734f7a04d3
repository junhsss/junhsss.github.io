I"×#<p>Q-Learning ê³„ì—´ì˜ ë°©ë²•ë¡ ë“¤ì€ ìµœì ì˜ ì •ì±…ì´ í•  í–‰ë™ì„ ìƒìƒí•œë‹¤. ê·¸ë ‡ê²Œ ìƒìƒí•œ \(a'\)ë¥¼ ê°€ì§€ê³  ì‹œê°„ì°¨ í•™ìŠµì„ í•œë‹¤. \(Q(s, a)\)ì™€ \(r+\gamma Q(s', a')\)ê°€ ë¹„ìŠ·í•´ì§€ë„ë¡ í•™ìŠµí•´ ë‚˜ê°„ë‹¤ëŠ” ë§ì´ë‹¤.</p>

<h2 id="ìŠ¤í†¡ë°ì¼-íŒ¨ëŸ¬ë…ìŠ¤">ìŠ¤í†¡ë°ì¼ íŒ¨ëŸ¬ë…ìŠ¤</h2>

<p>í•™ìŠµ ì¤‘ì¸ \(\hat{Q}\)ë¥¼ ê·¼ê±°ë¡œ ê°€ì¥ ì¢‹ì€ \(a'\)ë¥¼ ì„ íƒí•œë‹¤.</p>

\[a' = \mathrm{argmax}_a \hat{Q}(s', a)\]

<p>ê·¸ë ‡ê²Œ ì„ íƒí•œ \(a'\)ë¡œ \(r+\gamma \hat{Q}(s', a')\)ë¥¼ ê³„ì‚°í•´ ì´ê²ƒì´ ë§ˆì¹˜ ì •ë‹µì¸ ì–‘ \(\hat{Q}(s,a)\)ì™€ ê°€ê¹Œì›Œì§€ê²Œ ë§Œë“ ë‹¤. í•¨ì •ì€ \(a' = \mathrm{argmax}_a \hat{Q}(s', a)\)ì— ìˆë‹¤. í•œ ê°€ì§€ ì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ì. ê°€ë ¹ ê°€ëŠ¥í•œ í–‰ë™ë“¤ì´ \(a_1,\) \(a_2,\) \(a_3\) ì„¸ ê°€ì§€ì´ê³ , ìµœì  ì •ì±…ì˜ ê°€ì¹˜ í•¨ìˆ˜ê°€ \(s'\)ì—ì„œ ì•„ë˜ì™€ ê°™ë‹¤ê³  í•´ ë³´ì.</p>

\[Q^*(s', a_1)=2.6\]

\[Q^*(s', a_2)=2\]

\[Q^*(s', a_3)=2.5\]

<p>ê·¸ëŸ¬ë‹ˆ \(s'\)ì—ì„œ ìµœì ì˜ ì •ì±…ì€ \(a_1\)ë§Œì„ í•  í…Œë‹¤. ê·¸ ë•Œì˜ ê°€ì¹˜ëŠ” 2.6ì´ë‹¤. ê·¸ëŸ°ë° ë¬¸ì œëŠ” ìš°ë¦¬ê°€ \(Q^*\)ë¥¼ ì•Œì§€ ëª»í•œë‹¤ëŠ”ë° ìˆë‹¤. ì•„ì§ í•™ìŠµ ì¤‘ì´ê¸° ë•Œë¬¸ì¼ ìˆ˜ë„ ìˆì§€ë§Œ ì‹ ê²½ë§ì„ ì´ìš©í•´ \(Q^*\)ë¥¼ ê·¼ì‚¬í•˜ë ¤ í•˜ê³  ìˆëŠ” ìƒí™©ì´ê¸° ë•Œë¬¸ì— <a href="https://en.wikipedia.org/wiki/Uncertainty_quantification#Aleatoric_and_epistemic_uncertainty">ë³¸ì§ˆì ìœ¼ë¡œ ë¶€ì •í™•</a>í•  ìˆ˜ ë°–ì— ì—†ë‹¤. ê°€ë ¹ ì•½ê°„ì˜ ì˜¤ì°¨ê°€ ê°œì…í–ˆë‹¤ê³  í•´ ë³´ì.</p>

\[\hat{Q}(s', a_1)=2.59\]

\[\hat{Q}(s', a_2)=1.7\]

\[\hat{Q}(s', a_3)=2.64\]

<p>\(\hat{Q}\)ë¥¼ ê·¼ê±°ë¡œ íŒë‹¨í•œ ìµœì ì˜ í–‰ë™ì€ \(a_3\)ì´ê³ , ê·¸ ë•Œì˜ ì¶”ì • ê°€ì¹˜ëŠ” \(2.64\)ì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì˜¤ì°¨ê°€ ì—†ì—ˆë‹¤ë©´ ìµœì ì˜ í–‰ë™ì€ \(a_1\)ì´ê³ , ê·¸ ë•Œì˜ ê°€ì¹˜ëŠ” \(2.6\)ì´ë¼ ì¶”ì •í–ˆì–´ì•¼ í•œë‹¤. ì´ëŠ” \(\hat{Q}(s,a)\)ì˜ ì •ë‹µ ì—­í• ì„ í•  \(r+\gamma \hat{Q}(s', a')\)ë¥¼ ì°¸ê°’ë³´ë‹¤ í¬ê²Œ ì¶”ì •í•˜ëŠ” ì˜ëª»ëœ íŒë‹¨ìœ¼ë¡œ ì´ì–´ì§„ë‹¤. í•­ìƒ ì •ë‹µì„ ì°¸ê°’ë³´ë‹¤ í¬ê²Œ ì¶”ì •í•˜ë‹ˆ \(\hat{Q}\)ê°€ ì „ë°˜ì ìœ¼ë¡œ ì°¸ê°’ \(Q^*\)ì— ë¹„í•´ ë†’ê²Œ í•™ìŠµë˜ëŠ” í¸í–¥ì„ ì•¼ê¸°í•œë‹¤.</p>

<p>\(a_1\)ê³¼ \(a_2\)ì˜ ê°€ì¹˜ëŠ” ì°¸ê°’ë³´ë‹¤ ë‚®ê²Œ ì¶”ì •í–ˆë‹¤. ì „ì²´ì ì¸ ì˜¤ì°¨ëŠ” ë‚®ì€ ë°©í–¥ìœ¼ë¡œ ì‘ìš©í•œ ì…ˆì´ë‹¤.  ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  í¸í–¥ì´ ë°œìƒí•œë‹¤ëŠ” ì‚¬ì‹¤ì´ ì¬ë°Œë‹¤. ì´ëŸ¬í•œ í¸í–¥ì„ Maximization Biasë¼ ë¶€ë¥¸ë‹¤. ë¶ˆê³µí‰í•œ ë…€ì„ì´ë‹¤.</p>

<p>ì´ëŸ¬í•œ í¸í–¥ íƒ“ì— í•™ìŠµí•œ \(\hat{Q}\)ì™€ ì°¸ê°’ \(Q^{*}\)ì´ ë‹¬ë¼ì§„ë‹¤. ê·¸ëŸ¬ë‚˜ ìƒê°ë³´ë‹¤ í° ë¬¸ì œëŠ” ì•„ë‹ ìˆ˜ ìˆë‹¤. \(Q^{*}\)ê°€ ì•„ë‹Œ \(\pi^{*}\)ë¥¼ ì¶”ì •í•˜ëŠ”ê²Œ ìš°ë¦¬ì˜ ëª©ì ì´ê¸° ë•Œë¬¸ì´ë‹¤. í–‰ìœ„ìì—ê²ŒëŠ” í–‰ë™ì˜ ê·¼ê±°ê°€ ë  ì •ì±…ë§Œ ìˆìœ¼ë©´ ì¶©ë¶„í•˜ë‹¤. ë¬¼ë¡  \(Q^{*}\)ë¥¼ ì •í™•íˆ ì¶”ì •í•˜ë©´ ì¢‹ë‹¤. ê·¸ëŸ¬ë‚˜ ì°¸ê°’ë³´ë‹¤ ì¼ê´€ì ìœ¼ë¡œ ë†’ê²Œ í•™ìŠµí•œë‹¤ë©´, ì˜ˆì»¨ë° ì–´ë–¤ ìƒìˆ˜ \(C\)ì— ëŒ€í•´ \(Q^{*} + C\)ë¥¼ í•™ìŠµí•˜ëŠ” ì…ˆì´ë¼ë©´ ìƒê´€ì—†ë‹¤. ê²°êµ­ ê°€ì¹˜ í•¨ìˆ˜ë¡œë¶€í„° ìœ ë„ë˜ëŠ” ì •ì±…ì€ ê°™ë‹¤.</p>

\[\pi^{*} = \mathrm{argmax} _a Q^{*} (s, a) +C=\mathrm{argmax} _a Q^{*} (s, a)\]

<p>ë‹¹ì—°í•˜ê²Œë„ êµ³ì´ ì´ë ‡ê²Œ ì¼ê´€ì ìœ¼ë¡œ ì˜ëª»í•  ì´ìœ ê°€ ì—†ë‹¤. ê·¸ëŸ¬ë‹ˆ í•™ìŠµí•œ ê°€ì¹˜ í•¨ìˆ˜ë¡œë¶€í„° ìœ ë„ë˜ëŠ” ì •ì±…ì´ ìµœì  ì •ì±…ê³¼ ë‹¬ë¼ì§„ë‹¤. ë”°ë¼ì„œ Q-Learning ê¸°ë°˜ì˜ ë°©ë²•ë¡ ë“¤ì€ ì´ëŸ¬í•œ í¸í–¥ì„ í•´ê²°í•´ ì¤„ ì²˜ë°©ì´ í•„ìš”í•˜ë‹¤.</p>

<figure class="image">
    <img src="/images/rl-story-5-1.png" alt="van Hasselt et al., 2015" />
    <figcaption class="caption">van Hasselt et al., 2015</figcaption>
  </figure>

<p>ìœ„ ë„ì‹ì€ ê°€ì¹˜ í•¨ìˆ˜ê°€ ì¶”ì •í•œ ê°€ì¹˜, ì•„ë˜ ë„ì‹ì€ ê·¸ëŸ¬í•œ ê°€ì¹˜ í•¨ìˆ˜ë¡œë¶€í„° ì •ì±…ì„ ìœ ë„í•´ í–‰ë™í–ˆì„ ë•Œ ì‹¤ì œë¡œ ì–»ì€ ëˆ„ì  ë³´ìƒì„ ë‚˜íƒ€ë‚¸ë‹¤. ë§í–ˆë“¯ì´ ìµœì  ì •ì±…ì˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ì°¸ê°’ë³´ë‹¤ ë†’ê²Œ ì¸¡ì •í•œë‹¤ëŠ” ì‚¬ì‹¤ ìì²´ê°€ í° ë¬¸ì œëŠ” ì•„ë‹ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ê·¸ëŸ° í˜„ìƒì´ ì‹œì‘ë˜ëŠ” ìˆœê°„ë¶€í„° ì‹¤ì œ ì„±ëŠ¥ì€ ê°ì†Œí•˜ê¸° ì‹œì‘í•œë‹¤. ë‚™ê´€ì£¼ì˜ì˜ ë¹„ê·¹ì´ë‹¤.</p>

<p>ì´ì œ ë“±ì¥í•  Double DQNì˜ ê²½ìš°ì—ëŠ” ì´ëŸ¬í•œ í¸í–¥ì´ í˜„ì €íˆ ì ì–´ì§„ë‹¤. í¸í–¥ì´ ì ì„ ë¿ ì•„ë‹ˆë¼ ì‹¤ì œ í™˜ê²½ì—ì„œ ì–»ëŠ” ëˆ„ì  ë³´ìƒë„ ë†’ì•„ì§€ê³  í•™ìŠµì´ ì•ˆì •í™”ëœë‹¤. í¸í–¥ì„ í•´ê²°í•˜ë©´ ë‹¬ì½¤í•œ ë³´ìƒì´ ë”°ë¥´ë¦¬ë¼ëŠ” ì•”ì‹œë ¸ë‹¤.</p>

<h2 id="ìŒì„±ê³„">ìŒì„±ê³„</h2>

<p>í¸í–¥ì´ ì™œ ë°œìƒí•˜ëŠ”ì§€ ìƒê°í•´ë³´ì. \(\hat{Q}\)ëŠ” ë¬´ì—‡ì„ ê·¼ê±°ë¡œ \(a_3\)ì´ ìµœì ì˜ í–‰ë™ì´ë¼ê³  íŒë‹¨í–ˆëŠ”ê°€? ë§ì¥ë‚œ ê°™ì§€ë§Œ \(\hat{Q}\) ìì‹ ì´ë‹¤. ê·¸ëŸ¬ë‹ˆ \(\hat{Q}\)ëŠ” ì–µìš¸í•˜ë‹¤. ìµœì ì˜ í–‰ë™ì„ ë§í•˜ë˜ì„œ ë§í–ˆê³  ê·¸ ê·¼ê±°ë¥¼ ëŒ€ë˜ì„œ ëŒ”ë‹¤. í¸í–¥ì´ ë°œìƒí•˜ëŠ”ê±´ \(\hat{Q}\)ì˜ ì˜ëª»ì´ ì•„ë‹ˆë‹¤.</p>

<p>ë¹„ìœ ë¥¼ ë“¤ì–´ë³´ì. ê°€ë ¹ ë‘ ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚¨ë‹¤ê³  í•´ ë³´ì. ì‹ ê²½ë§ì´ ì‘ë™í•˜ê²Œë” í•˜ê¸° ìœ„í•´ ìš°ì„ ì€ í•™ìŠµ ë°ì´í„°ì— ë¼ì›Œ ë§ì¶°ì•¼ í•œë‹¤. ì´ë¯¸ í•™ìŠµ ë°ì´í„°ì— ë§ì¶”ì—ˆê¸° ë•Œë¬¸ì— ë‹¹ì—°íˆ ì¼ë°˜ì ì¸ ë°ì´í„°ë³´ë‹¤ í•™ìŠµ ë°ì´í„°ì— ë” ì˜ ë§ì„í…Œë‹¤. ê·¸ëŸ¬ë‹ˆ í•™ìŠµ ë°ì´í„°ë¥¼ ì§€í‘œë¡œ ì‚¬ìš©í•œë‹¤ë©´ ì‹¤ì œë³´ë‹¤ ë‚™ê´€ì ì¸ ê²°ë¡ ì— ì´ë¥¸ë‹¤. ê·¸ëŸ¬ë‹ˆ í•™ìŠµ ë°ì´í„°ì™€ ìƒí˜¸ ë°°íƒ€ì ì¸ ê²€ì¦ ë°ì´í„°ë¥¼ ë“¤ê³ ì™€ ëª¨í˜•ì„ ì„ íƒí•œë‹¤.</p>

<p>ê±°ì°½í•œ ë¹„ìœ ë¥¼ ë“¤ì—ˆìœ¼ë‚˜ ì‚¬ì‹¤ ë‹¹ì—°í•œ ì´ì•¼ê¸°ë‹¤. ë†€ì´ê³µì›ì—ì„œ ë¡¤ëŸ¬ì½”ìŠ¤í„°ë¥¼ íƒ€ëŠ” ì‚¬ëŒë“¤ì˜ í‚¤ í‰ê· ì€ í•œêµ­ì¸ í‰ê·  í‚¤ë³´ë‹¤ ë†’ë‹¤. ì• ì´ˆì— í‚¤ê°€ ì‘ì€ ì‚¬ëŒì€ ë¡¤ëŸ¬ì½”ìŠ¤í„°ë¥¼ íƒˆ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. í•œêµ­ì¸ í‰ê·  í‚¤ì˜ ì¶”ì •ì¹˜ê°€ í•„ìš”í•œ ìë¦¬ì— ì´ë¥¼ ì‚¬ìš©í•œë‹¤ë©´ <a href="https://en.wikipedia.org/wiki/Sampling_bias">í¸í–¥</a>ì´ ìƒê¸´ë‹¤.</p>

<p>ë§ˆì°¬ê°€ì§€ë‹¤. \(\hat{Q}\)ì—ê²Œ ìµœì ì˜ í–‰ë™ì„ ë¬¼ì—ˆë‹¤ë©´ ê·¸ê²ƒìœ¼ë¡œ ëë‚´ì•¼ í•œë‹¤. ê·¸ í–‰ë™ì˜ ê°€ì¹˜ë¥¼ ë‹¤ì‹œê¸ˆ \(\hat{Q}\)ì—ê²Œ ë¬¼ìœ¼ë©´ í•„ì—°ì ìœ¼ë¡œ ì°¸ê°’ë³´ë‹¤ ë†’ì€ ê°’ì„ ì´ì•¼ê¸°í•˜ëŠ” ê²½í–¥ì´ ë°œìƒí•œë‹¤. \(\hat{Q}\)ê°€ ìµœì ì´ë¼ê³  íŒë‹¨í–ˆë˜ í–‰ë™ì˜ ê°€ì¹˜ë¥¼ ê°ê´€ì ìœ¼ë¡œ ì¼ëŸ¬ì¤„ ì œ 2ì˜ ê°€ì¹˜ í•¨ìˆ˜ê°€ í•„ìš”í•˜ë‹¤.</p>

<details id="inside">
<summary>Talk is cheap. Show me the code. â”“ </summary>
<div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">copy</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"CartPole-v1"</span><span class="p">)</span>

<span class="n">STATE_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">ACTION_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">0.03</span>
<span class="n">TAU</span> <span class="o">=</span> <span class="mf">0.005</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">array</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">store</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)),</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">batch_indices</span><span class="p">]</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="s">'states'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span>
                 <span class="s">'actions'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                 <span class="s">'next_states'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span>
                 <span class="s">'rewards'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                 <span class="s">'dones'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)}</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span>

<span class="n">q1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">))</span>

<span class="n">q2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">))</span>

<span class="n">q1_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">q1</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">q2_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">q2</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>

<span class="nb">buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">EPSILON</span><span class="p">:</span>
            <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">q1</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)).</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
            
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">chosen_action</span><span class="p">)</span>

        <span class="nb">buffer</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">chosen_action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">buffer</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
        <span class="k">continue</span>
    
    <span class="n">batch</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
    <span class="n">q1_expected_state_action_values</span> <span class="o">=</span> <span class="n">q1</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s">'actions'</span><span class="p">])</span>
    <span class="n">q2_expected_state_action_values</span> <span class="o">=</span> <span class="n">q2</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s">'actions'</span><span class="p">])</span>

    <span class="n">augmented_next_actions</span> <span class="o">=</span> <span class="n">q1</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'next_states'</span><span class="p">]).</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'rewards'</span><span class="p">]</span> <span class="o">+</span> <span class="n">q2</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'next_states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">augmented_next_actions</span><span class="p">)</span> <span class="o">*</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">batch</span><span class="p">[</span><span class="s">'dones'</span><span class="p">])</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">q1_expected_state_action_values</span> <span class="o">-</span> <span class="n">target</span><span class="p">.</span><span class="n">detach</span><span class="p">()).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span> <span class="o">+</span>\
           <span class="p">(</span><span class="n">q2_expected_state_action_values</span> <span class="o">-</span> <span class="n">target</span><span class="p">.</span><span class="n">detach</span><span class="p">()).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
            
    <span class="n">q1_optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">q2_optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="n">q1_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">q2_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="s">"""
    for param, param_target in zip(q.parameters(), q_target.parameters()):
        param_target.data.copy_((1-TAU) * param_target.data+ TAU * param.data)
    """</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">performance</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">q1</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)).</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">chosen_action</span><span class="p">)</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
                <span class="n">performance</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">print</span><span class="p">(</span><span class="s">f"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">th Trial -&gt; </span><span class="si">{</span><span class="n">performance</span><span class="o">/</span><span class="mi">10</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div>    </div>

  </div>
</details>

<p>\(Q_1\)ì—ê²Œ ìµœì ì˜ í–‰ë™ì„ ê³ ë¥´ê²Œ ë‘ê³ , \(Q_2\)ë¡œ ê·¸ í–‰ë™ì— ëŒ€í•œ ê°€ì¹˜ë¥¼ ì¶”ì •í•˜ê²Œ ë§Œë“ ë‹¤. ê·¸ë ‡ê²Œ ê³„ì‚°í•œ ì •ë‹µìœ¼ë¡œ \(Q_1\)ê³¼ \(Q_2\)ë¥¼ ë™ì‹œì— í•™ìŠµì‹œí‚¨ë‹¤. ìƒê°ë³´ë‹¤ ë¬¸ì œê°€ ê°„ë‹¨íˆ í•´ê²°ëœë‹¤. ì›í•œë‹¤ë©´ \(Q_1\)ê³¼ \(Q_2\)ì´ ì—­í• ì„ ë²ˆê°ˆì•„ ë§¡ë„ë¡ êµ¬í˜„í•´ë„ ëœë‹¤. ììœ ë‹¤.</p>

<p>ì§€ë‚œ ì´ì•¼ê¸° ë§ë¯¸ì— ë“±ì¥í–ˆë˜ Target Networkë¥¼ ìŠì—ˆë‹¤. ì›ë˜ì˜ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ì²œì²œíˆ ë”°ë¼ì˜¤ëŠ” Target Networkë¡œ ì •ë‹µì„ ê³„ì‚°í•´ í•™ìŠµ ê³¼ì •ì„ ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¤ìê³  í–ˆë‹¤. ì´ ë˜í•œ êµ¬í˜„í•´ì£¼ì. ë‘ ì‹ ê²½ë§ ëª¨ë‘ì˜ Target Networkë¥¼ ë§Œë“¤ì–´ ì£¼ì–´ì•¼ í•œë‹¤. ë„¤ ì‹ ê²½ë§ì´ ì¡°í™”ë¥¼ ì´ë£¨ë©° ì‘ë™í•œë‹¤. ë‹¤ì†Œ ë¶€ë‹´ìŠ¤ëŸ½ë‹¤.</p>

<details id="inside">
<summary>Talk is cheap. Show me the code. â”“ </summary>
<div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">copy</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"CartPole-v1"</span><span class="p">)</span>

<span class="n">STATE_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">ACTION_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">0.03</span>
<span class="n">TAU</span> <span class="o">=</span> <span class="mf">0.005</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">array</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">store</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)),</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">batch_indices</span><span class="p">]</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="s">'states'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span>
                 <span class="s">'actions'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                 <span class="s">'next_states'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span>
                 <span class="s">'rewards'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                 <span class="s">'dones'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)}</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span>

<span class="n">q1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">))</span>

<span class="n">q1_target</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">q1</span><span class="p">)</span>

<span class="n">q2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">))</span>

<span class="n">q2_target</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">q2</span><span class="p">)</span>

<span class="n">q1_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">q1</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">q2_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">q2</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>

<span class="nb">buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">EPSILON</span><span class="p">:</span>
            <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">q1</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)).</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
            
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">chosen_action</span><span class="p">)</span>

        <span class="nb">buffer</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">chosen_action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">buffer</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
        <span class="k">continue</span>
    
    <span class="n">batch</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
    <span class="n">q1_expected_state_action_values</span> <span class="o">=</span> <span class="n">q1</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s">'actions'</span><span class="p">])</span>
    <span class="n">q2_expected_state_action_values</span> <span class="o">=</span> <span class="n">q2</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s">'actions'</span><span class="p">])</span>

    <span class="n">augmented_next_actions</span> <span class="o">=</span> <span class="n">q1_target</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'next_states'</span><span class="p">]).</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'rewards'</span><span class="p">]</span> <span class="o">+</span> <span class="n">q2_target</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'next_states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">augmented_next_actions</span><span class="p">)</span> <span class="o">*</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">batch</span><span class="p">[</span><span class="s">'dones'</span><span class="p">])</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">q1_expected_state_action_values</span> <span class="o">-</span> <span class="n">target</span><span class="p">.</span><span class="n">detach</span><span class="p">()).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span> <span class="o">+</span>\
           <span class="p">(</span><span class="n">q2_expected_state_action_values</span> <span class="o">-</span> <span class="n">target</span><span class="p">.</span><span class="n">detach</span><span class="p">()).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
            
    <span class="n">q1_optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">q2_optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="n">q1_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">q2_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">param_target</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">q1</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">q1_target</span><span class="p">.</span><span class="n">parameters</span><span class="p">()):</span>
        <span class="n">param_target</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">copy_</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">TAU</span><span class="p">)</span> <span class="o">*</span> <span class="n">param_target</span><span class="p">.</span><span class="n">data</span><span class="o">+</span> <span class="n">TAU</span> <span class="o">*</span> <span class="n">param</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">param_target</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">q2</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">q2_target</span><span class="p">.</span><span class="n">parameters</span><span class="p">()):</span>
        <span class="n">param_target</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">copy_</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">TAU</span><span class="p">)</span> <span class="o">*</span> <span class="n">param_target</span><span class="p">.</span><span class="n">data</span><span class="o">+</span> <span class="n">TAU</span> <span class="o">*</span> <span class="n">param</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">performance</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">q1</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)).</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">chosen_action</span><span class="p">)</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
                <span class="n">performance</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">print</span><span class="p">(</span><span class="s">f"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">th Trial -&gt; </span><span class="si">{</span><span class="n">performance</span><span class="o">/</span><span class="mi">10</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div>    </div>

  </div>
</details>

<p>Q-Learningì—ì„œ ë°œìƒí•˜ëŠ” ì´ëŸ° í¸í–¥ì€ <a href="https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf">ì˜¤ë˜ ì „</a>ë¶€í„° <a href="https://papers.nips.cc/paper/3964-double-q-learning">ì•Œë ¤ì ¸ ìˆë˜</a> í˜„ìƒì´ë‹¤.  DQNì—ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ ì´ëŸ° í˜„ìƒì´ ê´€ì¸¡ë˜ì—ˆê³ , ì–¼ë§ˆ ê°€ì§€ ì•Šì•„ ë”¥ë§ˆì¸ë“œì˜ ì—°êµ¬ìë“¤ì— ì˜í•´ <a href="https://arxiv.org/abs/1509.06461">Double DQN</a>ì´ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ê°œì„ ë˜ì—ˆë‹¤. ì´ ë•Œ ì œì•ˆëœ ë°©ë²•ë¡ ì€ ìœ„ì˜ êµ¬í˜„ì²´ë³´ë‹¤ëŠ” ë‹¨ìˆœí•˜ë‹¤. \(Q_2\)ë¥¼ ê·¸ëƒ¥ \(Q_1\)ì˜ Target Networkë¡œ ê°„ì£¼í•œë‹¤. ì„œë¡œ ì™„ì „íˆ ë…ë¦½ì ì´ì§€ëŠ” ì•Šì§€ë§Œ ì–´ì¨Œë“  ë‹¤ë¥¸ ì‹ ê²½ë§ì´ë‹ˆ ì œ 2ì˜ ì‹ ê²½ë§ê¹Œì§€ ë„ì…í•˜ì§„ ë§ìëŠ” ì•„ì´ë””ì–´ë‹¤.</p>

<p>ê·¸ë ‡ê²Œ í•  ìš”ëŸ‰ì´ë¼ë©´ ì§€ë‚œ ì´ì•¼ê¸°ì˜ DQN êµ¬í˜„ì²´ì—ì„œ ì •ë§ë¡œ ë‹¨ í•œ ë‹¨ì–´ë§Œ ì§€ìš°ë©´ ëœë‹¤. ì¼ë°˜ì ìœ¼ë¡œ Double DQNì´ë¼ê³  ë¶€ë¥´ëŠ”ê±´ ì•„ë˜ì˜ êµ¬í˜„ì²´ë‹¤. ê·¸ëŸ¬ë‚˜ ë” ì¼ë°˜ì ì¸ êµ¬í˜„ì²´ë¥¼ ë¨¼ì € ë§Œë“¤ì–´ë³¸ ì´ìœ ê°€ ìˆë‹¤. ì´í›„ ì´ì•¼ê¸°í•  TD3, ë‚´ì§€ëŠ” SACì˜ êµ¬í˜„ì²´ì™€ ì–¼ê°œê°€ ê°™ê¸° ë•Œë¬¸ì´ë‹¤.</p>

<details id="inside">
<summary>Talk is cheap. Show me the code. â”“ </summary>
<div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">copy</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"CartPole-v1"</span><span class="p">)</span>

<span class="n">STATE_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">ACTION_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">0.03</span>
<span class="n">TAU</span> <span class="o">=</span> <span class="mf">0.005</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">array</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">store</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)),</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">batch_indices</span><span class="p">]</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="s">'states'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span>
                 <span class="s">'actions'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                 <span class="s">'next_states'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span>
                 <span class="s">'rewards'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                 <span class="s">'dones'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)}</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span>

<span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">))</span>

<span class="n">q_target</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="nb">buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">EPSILON</span><span class="p">:</span>
            <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)).</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
            
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">chosen_action</span><span class="p">)</span>

        <span class="nb">buffer</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">chosen_action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">buffer</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
        <span class="k">continue</span>
    
    <span class="n">batch</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
    <span class="n">expected_state_action_values</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s">'actions'</span><span class="p">])</span>

    <span class="n">augmented_next_actions</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'next_states'</span><span class="p">]).</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'rewards'</span><span class="p">]</span> <span class="o">+</span> <span class="n">q_target</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'next_states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">augmented_next_actions</span><span class="p">)</span> <span class="o">*</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">batch</span><span class="p">[</span><span class="s">'dones'</span><span class="p">])</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">expected_state_action_values</span> <span class="o">-</span> <span class="n">target</span><span class="p">.</span><span class="n">detach</span><span class="p">()).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">param_target</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">q_target</span><span class="p">.</span><span class="n">parameters</span><span class="p">()):</span>
        <span class="n">param_target</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">copy_</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">TAU</span><span class="p">)</span> <span class="o">*</span> <span class="n">param_target</span><span class="p">.</span><span class="n">data</span><span class="o">+</span> <span class="n">TAU</span> <span class="o">*</span> <span class="n">param</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">performance</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)).</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">chosen_action</span><span class="p">)</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
                <span class="n">performance</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">print</span><span class="p">(</span><span class="s">f"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">th Trial -&gt; </span><span class="si">{</span><span class="n">performance</span><span class="o">/</span><span class="mi">10</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div>    </div>

  </div>
</details>

<h2 id="unplugged-ì‘ì„±ì¤‘">Unplugged (ì‘ì„±ì¤‘)</h2>

<p>ìš°ë¦¬ì˜ ëª©í‘œì¸ ì˜¤í”„ë¼ì¸ ê°•í™”í•™ìŠµì—ì„œëŠ” ì´ëŸ¬í•œ í¸í–¥ì´ ë” ì•„í”„ê²Œ ë‹¤ê°€ì˜¨ë‹¤. ëŒ€ë¶€ë¶„ì˜ ì˜¤í”„ë¼ì¸ ê°•í™”í•™ìŠµ ë°©ë²•ë¡ ë“¤ì€ Q-Learning ê³„ì—´ì˜ ë°©ë²•ë¡ ë“¤ì„ ê³¨ìë¡œ í•œë‹¤. Q-Learningì€ Off-Policyì´ë‹ˆ ì˜¤í”„ë¼ì¸ ê°•í™”í•™ìŠµì„ ìœ„í•œ ì‘ë‹¹ í•©ë¦¬ì ì¸ ì„ íƒì§€ì¸ ë“¯ ë³´ì¸ë‹¤. ê·¸ëŸ¼ Q-Learningì„ ë°”ë¡œ ì ìš©í•˜ë©´ ì•ˆ ë˜ëŠ”ê±¸ê¹Œ?</p>

<p>ê·¸ëŸ¬ë‹ˆ ë¯¸ì§€ì˜ ì •ì±… \(\pi^{\beta}\)ê°€ ë§Œë“  ìˆœì„œìŒ \((s,\) \(a,\) \(r,\) \(s')\)ë“¤ì„ ì´ìš©í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê¸° ê°„ë‹¨í•œ ë°ì´í„°ë‹¤.</p>
:ET