I"<p>강화학습에서 가장 핵심이 되는 양은 가치 함수이다. 정의는 아래와 같다.</p>

<ul>
  <li>상태 \(s\)에서 행위 \(a\)를 했을 때 미래에 받을 수 있는 누적 보상의 기댓값을 \(Q(s, a)\),</li>
  <li>상태 \(s\)에 놓여 있을 때 미래에 받을 수 있는 누적 보상의 기댓값을 \(V(s)\)라 한다.</li>
</ul>

<p>가치 함수를 정의하는데는 정책 \(\pi\)가 암묵적으로 개입한다. 어떤 상태에 놓여있건, 혹은 어떤 상태에서 어떤 행위를 하건 발생 가능한 미래의 생김새는 학습 주체가 어떤 정책을 고수하는지 따라 달라지기 때문이다. 당연한 이야기지만 어느날 갑자기 복권에 당첨되어도 방탕한 삶을 살아간다면 미래 가치는 낮을테고, 체계적으로 재산을 불리고자 하는 삶을 산다면 미래 가치는 높을 것이다. 같은 상태에 놓여있어도 두 정책의 가치는 얼마든지 다를 수 있다는 말이다. 따라서 가치 함수를 이야기 할때는 윗 첨자로 정책에 대한 종속성을 알려주는 것이 좋다. \(Q^{\pi}(s, a)\), \(V^{\pi}(s)\) 이렇게 쓴다.</p>

\[Q^{\pi}(s, a)=\mathbb{E}_{\pi} \left[  \sum_{t'=t} r_t' \mid s_t = s, a_t = a \right]\]

\[V^{\pi}(s) =\mathbb{E}_{\pi} \left[  \sum_{t'=t} r_t' \mid s_t = s \right]\]

<p>위 정의는 살짝 애매하기에 조금 더 뜯어 생각해 볼 필요가 있다. 가령 \(Q\)의 경우를 생각해 보자. 상태 \(s\)에서 행위 \(a\)를 했다는 것이 조건으로 주어졌다. 그 시점 이후로 발생 가능한 미래는 무수히 많다. 앞서 말했듯 미래의 생김새는 행위자의 정책 \(\pi\) 에 따라 달라진다. (이후로는 행위자와 정책 \(\pi\)를 동일시하겠다.) 정책 \(\pi\)가 만드는 무수히 많은 미래들에서 계산되는 누적 보상의 평균을 \(Q\)로 정하겠다는 말이다.</p>

<p>당연한 이야기지만 한 가지 짚고 넘어가야 할 것은 \(s\)와 \(a\)는 조건으로 주어지는 데이터라는 점이다. 정책 \(\pi\)를 고수할 때 현실적으로 맞닥뜨릴 일이 없는 \(s\)와 \(a\)에 대해서도 이론상 \(Q^{\pi}(s, a)\) 는 정의된다. 강원도 토박이 철수 (\(\pi\))가 세렝게티 한복판(\(s\))에서 삼겹살을 구워먹는(\(a\)) 것이 얼마나 현명한 일인지 (\(Q^{\pi}(s, a)\)) 짐작이야 해 볼 수 있다는 말이다. 물론 \(Q^{\pi}(s, a)\)를 추정해보고 싶다면? 세렝게티 삼겹살 파티에 철수를 데려다 놓고 미래를 관찰해보는 것이 현재로서는 유일한 방법이다. 타임머신이 있는 경우에는 여러번 관찰해보면 더 좋다. 불쌍한 철수.</p>

<p>기댓값이라는 말의 속내에는 발생 가능한 모든 미래를 고려하겠다는 뜻이 담겨 있다. 상태</p>
:ET