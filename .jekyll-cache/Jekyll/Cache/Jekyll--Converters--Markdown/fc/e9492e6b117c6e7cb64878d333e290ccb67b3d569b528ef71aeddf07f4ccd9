I"z<p>정책 \(\pi\)의 행동을 교정하기 위해 가치 함수 \(Q^{\pi}(s, a)\)와 \(V^{\pi}(s)\)를 소개했다. 낭비를 줄이기 위해 \(V^{\pi}\)만 이용하기로 했다. 그래서 \(V^{\pi}\)를 시간차 학습으로 추정하는 방법에 대해 이야기했다. \(Q^{\pi}(s, a)\) 역시 시간차 학습으로 추정할 수 있다.</p>

\[Q^{\pi}(s, a) \approx r + Q^{\pi}(s', a')\]

<p>필요한 데이터는 \((s,\ a,\ r,\ s',\ a')\)다. 행위자가 개입한 흔적은 \(a'\)에서만 드러난다. 지난 이야기에서 꼼꼼히 따져 봤다.</p>

<p>그러나 잘 생각해보자. 가치 함수 \(Q^{\pi}\)만 가지고도 해야 할 행동이 무엇인지 충분히 추론할 수 있다. 가장 높은 누적 보상을 기대할 수 있는 행동을 하면 된다. 그러나 그렇게 행동하는 정책은 \(Q^{\pi}\)를 정의하는 정책 \(\pi\)와 다르다는 점이 재밌다.</p>

<h2 id="헬리콥터-부모">헬리콥터 부모</h2>

<p>가능한 행동이 \(a_1\)와 \(a_2\) 뿐이고 \(Q^{\pi}(s, a_1)\) \(&gt;\) \(Q^{\pi}(s, a_2)\)라 하자. 정책 \(\pi\)는 \(s\)에서 \(a_1\)와 \(a_2\) 중 무엇을 선호할까? 모른다. \(\pi\)는 \(a_2\)를 더 선호할 수도 있다. 그러나 만약 우리가 행동을 강제할 수 있다면 당연히 \(a_1\)를 고른다. 물론 \(a_1\)가 정말로 좋은 행위인지는 모른다. 적어도 그 이후에 \(\pi\)에 따라 행동한다면 \(a_1\)를 하는게 가장 좋다는 말이다.</p>

<p>전형적인 부모 자식의 관계다. 부모는 자식의 정책 \(\pi\)의 가치 \(Q^{\pi}(s, a)\)를 추산하려 무던히도 노력한다. 자식 \(\pi\)에게 \(Q^{\pi}\)가 가장 높은 행동들만 할 것을 강요한다. 부모의 명령을 따라 행동하는 것이 자식의 새로운 정책 \(\pi'\)가 된다. 부모는 \(Q^{\pi'}(s, a)\)를 추산해 다시금 자식의 행동을 교정한다.</p>

<p>이러한 방법론을 살사라 부른다. \(Q^{\pi}\)를 시간차 학습으로 추정하기 위해 필요한 데이터를 그대로 읽은 펀치라인이다. 이를 알고리즘으로 써 보자.</p>

<ol>
  <li>\(Q^{\pi}(s, a)\)를 가장 크게 만드는 행동만을 하는 새로운 정책 \(\pi'\)를 이용해 데이터를 모은다.</li>
  <li>\(\pi'\)의 가치함수 \(Q^{\pi'}(s, a)\)를 추정한다.</li>
  <li>\(\pi\) \(\leftarrow\) \(\pi'\)</li>
</ol>

<p>정책 \(\pi'\)를 표현하는 신경망 따위가 없다는 것에 주목하자. 이전 정책의 가치 함수 \(Q^{\pi}\)가 간접적으로 \(\pi'\)를 정의한다. 마찬가지로 \(\pi'\)의 가치 함수 \(Q^{\pi'}\)는 새로운 정책 \(\pi''\)을 정의한다. 반복한다.</p>

<p>일부러 비극적인 비유를 들어 보았다. 살사는 현실적으로 사용하는 방법론이 아니기 때문이다. 부모의 판단에 의존하는 자식의 삶은 불행하다. 더 나은 방법론은 다음 이야기에서 다룬다.</p>
:ET