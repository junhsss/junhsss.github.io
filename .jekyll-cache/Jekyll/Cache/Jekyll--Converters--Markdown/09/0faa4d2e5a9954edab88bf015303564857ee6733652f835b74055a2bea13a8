I"<p>강화학습에서 가장 핵심이 되는 양은 가치 함수이다. 정의는 아래와 같다.</p>

<ul>
  <li>상태 \(s\)에서 행위 \(a\)를 했을 때 미래에 받을 수 있는 누적 보상의 기댓값을 \(Q(s, a)\),</li>
  <li>상태 \(s\)에 놓여 있을 때 미래에 받을 수 있는 누적 보상의 기댓값을 \(V(s)\)라 한다.</li>
</ul>

<p>가치 함수를 정의하는데는 정책 \(\pi\)가 암묵적으로 개입한다. 어떤 상태에 놓여있건, 혹은 어떤 상태에서 어떤 행위를 하건 발생 가능한 미래의 생김새는 학습 주체가 어떤 정책을 고수하는지 따라 달라지기 때문이다. 당연한 이야기지만 어느날 갑자기 복권에 당첨되어도 방탕한 삶을 살아간다면 미래 가치는 낮을테고, 체계적으로 재산을 불리고자 하는 삶을 산다면 미래 가치는 높을 것이다. 같은 상태에 놓여있어도 두 정책의 가치는 얼마든지 다를 수 있다는 말이다. 따라서 가치 함수를 이야기 할때는 윗 첨자로 정책에 대한 종속성을 알려주는 것이 좋다. \(Q^{\pi}(s, a)\), \(V^{\pi}(s)\) 이렇게 쓴다.</p>

\[Q^{\pi}(s, a)=\mathbb{E}_{\pi} \left[  \sum_{t'=t} r_{t'} \mid s_t = s, a_t = a \right]\]

\[V^{\pi}(s) =\mathbb{E}_{\pi} \left[  \sum_{t'=t} r_{t'} \mid s_t = s \right]\]

<p>위 정의는 살짝 애매하기에 조금 더 뜯어 생각해 볼 필요가 있다. 가령 \(Q\)의 경우를 생각해 보자. 상태 \(s\)에서 행위 \(a\)를 했다는 것이 조건으로 주어졌다. 그 시점 이후로 발생 가능한 미래는 무수히 많다. 앞서 말했듯 미래의 생김새는 행위자의 정책 \(\pi\) 에 따라 달라진다. (이후로는 행위자와 정책 \(\pi\)를 동일시하겠다.) 정책 \(\pi\)가 만드는 무수히 많은 미래들에서 계산되는 누적 보상의 평균을 \(Q\)로 정하겠다는 말이다.</p>

<p>기댓값이라는 말의 속내에는 발생 가능한 모든 미래를 고려하겠다는 뜻이 담겨 있다. 위 정의대로라면 어떤 \(s\)와 \(a\)에 대해 \(Q(s, a)\)를 계산할 수 있는 유일한 방법은 \(s\), \(a\)에서 시작하는 미래를 여러번 관찰해보고, 각 미래에서 떨어지는 누적 보상의 평균을 계산하는 것이다.</p>

<p>당연한 이야기지만 한 가지 짚고 넘어가야 할 것은 \(s\)와 \(a\)는 조건으로 주어지는 데이터라는 점이다. 정책 \(\pi\)를 고수할 때 현실적으로 맞닥뜨릴 일이 없는 \(s\)와 \(a\)에 대해서도 이론상 \(Q^{\pi}(s, a)\) 는 정의된다. 강원도 토박이 철수 (\(\pi\))가 세렝게티 한복판(\(s\))에서 삼겹살을 구워먹는(\(a\)) 것이 얼마나 현명한 일인지 (\(Q^{\pi}(s, a)\)) 짐작이야 해 볼 수 있다는 말이다. 물론 \(Q^{\pi}(s, a)\)를 추정해보고 싶다면? 세렝게티 삼겹살 파티에 철수를 데려다 놓고 미래를 관찰해보는 것이 현재로서는 유일한 방법이다. 타임머신이 있는 경우에는 여러번 관찰해보면 더 좋다. 불쌍한 철수.</p>

<p>만약 신경망으로 \(Q^{\pi}(s, a)\)를 표현하고 싶다면 간단하다. \(s\), \(a\)를 입력으로, 실제로 관측한 미래의 누적 보상을 정답으로 두고 학습시키면 된다. 단순한 지도학습이니 쉽다. 계속 말하듯이 타임머신이 있다면 여러 미래에서 떨어지는 누적 보상의 평균을 정답으로 두어도 되지만 현실적으로 그럴 필요까지는 없다. 정책 \(\pi\)를 따를 때의 가치함수 \(Q^{\pi}\)를 알고 싶은 상황이니 정답을 계산할 때 필요한 미래는 \(\pi\)가 만든 미래여야만 한다.</p>

<p>지난 포스팅에서 이야기 했듯이 \(Q^{\pi}\)를 알고 있다면 더 나은 신뢰 할당이 가능하다. 이를 알고리즘으로 써 보자.</p>

<ol>
  <li>정책 \(\pi\)를 이용해 데이터를 모은다.</li>
  <li>정책 \(\pi\)에 대한 가치 함수 \(Q^{\pi}\)를 추정한다.</li>
  <li>추정한 가치함수 \(Q^{\pi}\)를 근거로 정책 \(\pi\)의 행동을 교정한다.</li>
</ol>

<p>그런데 3단계에서 정책 \(\pi\)가 조금이라도 변하는 순간 가치 함수 \(Q^{\pi}\)는 무용지물이 된다. 원칙적으로는 2단계에서 바뀐 정책에 대한 가치함수를 처음부터 새롭게 추정해야 하겠지만, 경사 하강법을 사용하는 경우에는 3단계에서 정책이 크게 변하지 않으므로 그냥 \(\pi\)와 \(Q^{\pi}\)를 번갈아 학습시키는것으로 충분하다.</p>

<p>정책 \(\pi\)와 가치 함수 \(Q^{\pi}\)을 표현하는데는 독립된 두 신경망을 사용한다. 그러니 학습이 불안정 할 수밖에 없다. 세 번째 신경망을 도입하면 \(V^{\pi}\) 까지 표현해 더 나은 신뢰 할당을 할 수 있다.</p>

<ol>
  <li>정책 \(\pi\)를 이용해 데이터를 모은다.</li>
  <li>정책 \(\pi\)에 대한 가치 함수 \(Q^{\pi}\)와 \(V^{\pi}\)를 추정한다.</li>
  <li>추정한 가치함수 \(Q^{\pi}\)와 \(V^{\pi}\)를 근거로 정책 \(\pi\)의 행동을 교정한다.</li>
</ol>

<p>그러나 낭비다. 다소 부정확할지라도 \(Q\)로 \(V\)를 표현하거나 \(V\)를 \(Q\)로 표현해 단순화 해야한다.</p>
:ET