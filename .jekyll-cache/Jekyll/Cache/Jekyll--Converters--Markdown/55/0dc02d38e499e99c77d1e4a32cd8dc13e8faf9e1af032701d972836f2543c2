I"$+<p>강화학습에서 가장 핵심이 되는 양은 가치 함수이다. 정의는 아래와 같다.</p>

<ul>
  <li>상태 \(s\)에서 행위 \(a\)를 했을 때 미래에 받을 수 있는 누적 보상의 기댓값을 \(Q(s, a)\),</li>
  <li>상태 \(s\)에 놓여 있을 때 미래에 받을 수 있는 누적 보상의 기댓값을 \(V(s)\)라 한다.</li>
</ul>

<p>가치 함수를 정의하는데는 정책 \(\pi\)가 암묵적으로 개입한다. 어떤 상태에 놓여있건, 혹은 어떤 상태에서 어떤 행위를 하건 발생 가능한 미래의 생김새는 학습 주체가 어떤 정책을 고수하는지 따라 달라지기 때문이다. 당연한 이야기지만 어느날 갑자기 복권에 당첨되어도 방탕한 삶을 살아간다면 미래 가치는 낮을테고, 체계적으로 재산을 불리고자 하는 삶을 산다면 미래 가치는 높아진다. 같은 상태에 놓여있어도 두 정책의 가치는 얼마든지 다를 수 있다는 말이다. 따라서 가치 함수를 이야기 할때는 윗 첨자로 정책에 대한 종속성을 알려주는 것이 좋다. \(Q^{\pi}(s, a)\), \(V^{\pi}(s)\) 이렇게 쓴다.</p>

\[Q(s, a)=\mathbb{E}_{\tau \sim p(\tau)} \left[  \sum_{t'=t} r_t' \mid s_t, a_t \right]\]

\[V(s) =\mathbb{E}_{\tau \sim p(\tau)} \left[ r(\tau) \right]\]

<p>기댓값이라는 말의 속에는 발생 가능한 모든 미래를 고려하겠다는 뜻이 담겨 있다. 상태</p>

<p>대개 신경망 훈련은 트레이닝 데이터에 대한 가능도 (likelihood)를 높이는 방향으로 이루어진다. 고양이라는 제목이 달려있는 픽셀 덩어리가 신경망에 입력되었을 때 신경망이 유도하는 확률 분포에서 고양이에 해당하는 값이 가능도이고, 이 가능도가 늘어나도록 파라미터를 건드리는 것이 소위 신경망 훈련이다.</p>

<p>그러나 강화학습에서는 가능도를 논하는 것이 부자연스럽다. 강화학습에서 학습 주체는 외부에서 제공되는 정답이 아니라 보상을 통해 학습하기 때문에 가능도를 계산할 정답이 주어지지 않기 때문이다. 학습 주체는 환경 속에서 획득하는 누적 보상이 가장 커지도록 행위하는 방법을 배워야 한다. 가장 단순한 방법은 학습 주체의 어떤 행위가 장기적으로 높은 누적 보상을 가져왔다면 당근을, 그렇지 못했다면 채찍을 주는 것이다. 이런 철학을 공유하는 방법론들을 정책 경사 (Policy Gradient) 알고리즘이라 부른다.</p>

<p>정답이 제공되어 있다고 가정하자. 다시 말해 상태 \(s\) 에서는 행위 \(a\) 가 올바른 행위라 인간 전문가가 알려주었다고 가정하자. 그렇다면 이 문제는 지도학습이므로 단순히 로그 가능도 \(\log \pi_{\theta} (a \mid s)\)를 높이는 방향으로 신경망을 훈련시키면 된다. (현실적으로는 불충분하다. <a href="http://proceedings.mlr.press/v15/ross11a/ross11a.pdf">참고</a>) 만약 이 데이터가 시간에 따라 여러 쌍이 모였다면 전부 더해주면 된다.</p>

\[\sum_t  \log \pi_{\theta} (a_t \mid s_t)\]

<p>PyTorch 코드는 대강 아래와 같이 쓸 수 있을 것이다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">probs</span> <span class="o">=</span> <span class="n">pi</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span></code></pre></figure>

<p>그러나 앞서 말했다시피 강화학습에서는 \(s_t\) 에서 행위 \(a_t\) 를 하도록 판단한 주체가 인간 전문가가 아니라 학습 주체이므로 위 방법론을 사용할 순 없다. 그러나 이를 크게 수정하지 않고 싶다면 이런 접근을 취해볼 수도 있겠다. \(s_t\)에서 했던 행위 \(a_t\)가 정답은 아니지만, 얼마나 좋고 나쁜 행위인지 정량화할 수 있다면 이걸 일종의 가중치로 제공하고 싶다.</p>

\[\sum_t (\ \ \ ) \log \pi_{\theta}(a_t|s_t)\]

<p>\((s_t, a_t)\) 가 얼마나 좋고 나쁜 쌍인지 판단하는 문제를 강화학습에서는 신뢰 할당 (Credit Assignment) 문제라 부른다. 단순하게  \((s_t, a_t)\) 이후로 받은 보상을 모두 더해 가중치로 줄 수 있다.</p>

\[\sum_t (\sum_{t'=t} r_{t'}) \log \pi_{\theta}(a_t|s_t)\]

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">probs</span> <span class="o">=</span> <span class="n">pi</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">cumulative_sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span></code></pre></figure>

<p>그러나 누적 보상이 얼마 이상이어야 ‘좋다’라고 할 수 있는가? ‘좋고 나쁨’은 상대적이며 잘 정의되는 개념이 아니다. 가령 마지막 시점에는 항상 +100만큼의 보상이 주어지는 풍요로운 환경에 놓여있다면, 가중치를 계산할 때 항상 -100은 빼 주어야 각 행위들의 좋고 나쁨을 조금 더 객관적인 눈으로 판단할 수 있을 것이다.</p>

\[\sum_t (\sum_{t'=t} r_{t'} - b) \log \pi_{\theta}(a_t|s_t)\]

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">probs</span> <span class="o">=</span> <span class="n">pi</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">cumulative_sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights</span> <span class="o">-</span> <span class="n">baseline</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span></code></pre></figure>

<p>순차적인 의사결정을 해 나가는 상황에서 학습 주체가 했던 행위들을 일단은 정답이라 간주하고, 이후 얻은 보상의 합을 통해 그러한 행위들의 타당성을 사후적으로 판단하자는 말이다. 단순하면서도 직관적인 이 방법론은 REINFORCE라는 이름으로 1992년 처음 제안되었고 Policy Gradient 방법론들의 효시가 되었다. 일견 말이 되는 방법론처럼 보이고 실제로도 작동을 잘 하는데, 가령 최근 <a href="https://arxiv.org/abs/1611.01578">Neural Architecture Search</a>가 처음 제안되었을 때도 이 간단한 방법론을 사용해 재미를 톡톡히 봤다.</p>

<p>여기까지는 좋아보이지만 현실적으로 여러 문제가 있다. 가장 큰 문제는 보상을 받기가 매우 어려운 경우이다. 가령 복잡한 의사결정에 따른 행동을 반복해야 보상을 간신히 받는 환경에선 아직 미숙한 학습 주체가 아무리 행위해봐야 결국 받는 보상은 항상 0일테다. 따라서 학습을 할 수 없다. 이 문제는 REINFORCE 뿐만이 아니라 Model-Free 방법론들이 겪는 고질적인 문제이다. 환경에서 떨어지는 보상을 행동 강화의 신호로 사용하는것은 수동적이며 게으른 방식이다. 인간은 환경으로부터 보상을 받아보지 않고도 좋고 나쁨을 판단해 행위할 수 있다. 찍어먹어 보아야만 된장인줄 아는건 아니라는 말이다. 계획을 세워 행동한다는 말인데, 이는 환경 즉 세상이 어떻게 동작하는지 얼추 이해하고 있어야 가능하다. 이는 <a href="/Not-Hello-World/">모델 기반의 강화학습</a>이 필연적임을 시사한다. 도메인 지식을 가지고 보상 체계를 단계적으로 세심하게 디자인하거나, 호기심을 모델링해 그것을 보상의 신호로 이용하는 방법론들은 있지만 본질적인 해결책은 아니다.</p>

<p>분산이 높은 것도 문제이다. 어떤 행위가 얼마나 좋았는지 판단하기 위해 해당 행위 이후에 받은 보상을 전부 더하고 있다. 만약 타임머신을 타고 그 행위를 했던 직후로 돌아갔다고 해 보자. 같은 행위를 했더라도 미래는 달라진다. 그것도 아주 많이. 발생 가능한 미래는 무한히 많고 우리가 관측한 미래는 그 중 하나에 불과하다. 그래서 행동 강화의 신호로 제공하기에 보상의 합은 매우 불안정하다. 만약 타임머신이 있어서 특정 행위 이후의 미래를 수십번 관찰할 수 있다면 각 미래들에서 받는 누적 보상의 평균을 강화 신호로 제공하면 되기야 하겠지만 타임머신이 있다 한들 어느 세월에 그러고 있겠는가.</p>

<p>어찌되었든 가능하다면 특정 행위 이후에 발생하는 모든 미래의 누적 보상을 계산해 그것들의 평균을 강화의 신호로 제공하는 것이 최선이다. 이렇게 말이다.</p>

\[\sum_t Q(s_t, a_t) \log \pi_{\theta}(a_t|s_t)\]

<p>\(Q(s, a)\)는 상태 \(s\)에서 행위 \(a\)를 하고난 후 얻을 수 있는 누적 보상의 기댓값을 의미한다. 발생 가능한 모든 미래를 고려하자는 말이다. 위에서 언급했듯이 좋고 나쁨은 상대적이다. 가령 \(Q(s, a_1)=12\)이라고 해 보자. 이 수치는 좋은것이니 상태 \(s\)에서는 \(a_1\)을 하도록 행동을 강화해야 하는걸까? 그렇지 않다. 만약 가능한 다른 행위 \(a_2, a_3, ...\) 들을 했을 때 더 높은 누적 보상을 기대할 수 있다면 \(a_1\)는 나쁜 행동이다. 이를 고려해주기 위해 가능한 행위들에 대해 \(Q(s, a)\)의 기댓값을 계산해 뺀다.</p>

\[\sum_t \left[ Q(s_t, a_t)-V(s_t) \right] \log \pi_{\theta}(a_t|s_t)\]

<p>이를 Advantage Actor-Critic, 줄여서 A2C라 부른다. 이제 관심사는 현실적으로 \(Q\), 내지는 \(V\)를 알아내는 방법으로 옮겨간다.</p>
:ET