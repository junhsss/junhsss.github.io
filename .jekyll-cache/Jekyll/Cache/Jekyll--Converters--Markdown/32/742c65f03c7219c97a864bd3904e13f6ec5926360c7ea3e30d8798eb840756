I"<p>여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행여태까지는 행동이 이산적이라는 가정 하에 알고
리즘을 고민해 왔다. 그렇기에 \(Q(s, a)\)를 \(Q(s)[a]\)의 형태로 구현하면 되었다. 그래서 \(\mathbb{argmax}_{a} Q(s', a )\)가 전혀 어렵지 않았다. 본질은 \(\mathbb{argmax}_{a} Q(s')[a]\)이기에 그냥 리스트 \(Q(s')\)에 <code class="language-plaintext highlighter-rouge">argmax</code> 연산자를 적용하면 되었다. 그러나 연속적인 행동을 다룰 요량이라면 말이 다르다. 상황과 행동을 입력으로 받아 숫자를 하나 뱉게끔 정직하게 구현해야 한다. 그렇게 되면 \(\mathbb{argmax}_{a} Q(s', a )\) 자체가 문제로 떠오른다. 어떤 입력을 넣어야 신경망의 출력값을 가장 키울 수 있을 것인가? 전형적인 최적화 문제다. 이런 이야기를 풀어 나가자. 그러나 알고리즘의 얼개는 위에서 보았던 DQN과 정확히 동일하니 그 쌍대 관계에 주목하면 쉽다.</p>
:ET