I"U&<p>미래에 받을 수 있는 누적 보상의 기댓값이라니. 도대체 이게 무슨 탁상 공론인가? 가능한 미래들을 모두 열거해 보기라도 하겠다는 말인가? 경우의 수가 얼마나 많을 줄 알고? <a href="/rl-story-1/">지난 이야기</a>를 읽으며 이런 생각이 들었다면 정상이다. 가치 함수가 바로 그렇게 정의되는 양이다. 정책 \(\pi\)가 어떤 행동을 해야 하는지 알려준다면 가치 함수는 그러한 행동 이후에 얼마만큼의 누적 보상을 기대할 수 있는지 알려준다.</p>

<p><a href="/rl-story-1/">지난 이야기</a>에서는 정책 \(\pi\)의 행동을 교정하기 위해 강림한 비평가로서 가치 함수를 소개했다. 그러나 가치 함수를 통해 어떤 행동이 얼마나 좋은지 어림할 수 있다면 굳이 정책을 별도로 가지고 있을 필요가 없다. 가치 함수만으로도 해야 할 행동이 무엇인지 충분히 추론할 수 있다. 가장 높은 누적 보상을 기대할 수 있는 행동을 하면 된다. 그러니 가치 함수에 대체로 더 많은 정보량이 들어있다고 생각해도 좋다.</p>

<p>대부분의 강화 학습 방법론들은 정책과 가치 함수를 동시에 사용한다. 인간의 경우에도 항상 얼마만큼의 보상을 얻을 수 있을지 셈해가며 행동하는 것은 아니다. 마땅히 해야 할 행동을 아는데 그 행동이 얼만큼 좋은지 계산하는건 지적 낭비다. 아무튼 인간은 둘 다 가능하다. 말 한 필이 끄는 마차보다는 쌍두 마차가 더 안정적인 법이다. 가치 함수의 정의를 되짚어보자.</p>

<ul>
  <li>상태 \(s\)에서 행위 \(a\)를 했을 때 미래에 받을 수 있는 누적 보상의 기댓값을 \(Q(s, a)\),</li>
  <li>상태 \(s\)에 놓여 있을 때 미래에 받을 수 있는 누적 보상의 기댓값을 \(V(s)\)라 한다.</li>
</ul>

<p>이 정의에서 미래라는 말은 살짝 모호하다. 누가 만드는 미래인가? 같은 동네에서 나고 자랐더라도 내가 만드는 미래와 옆집 철수가 만드는 미래는 다른 법이다. 따라서 가치 함수의 정의에는 행위자가 고수하는 정책 \(\pi\)가 개입한다. 가치 함수를 쓸 때는 윗 첨자로 어떤 정책을 따라 행동할 때의 가치 함수인지 알려주는 것이 일반적이다. \(Q^{\pi}(s, a)\), \(V^{\pi}(s)\) 이렇게 쓴다.</p>

\[Q^{\pi}(s, a)=\mathbb{E}_{\pi} \left[  \sum_{t'=t} r_{t'} \mid s_t = s, a_t = a \right]\]

\[V^{\pi}(s) =\mathbb{E}_{\pi} \left[  \sum_{t'=t} r_{t'} \mid s_t = s \right]\]

<p>상황과 행동의 가치 함수 \(Q^{\pi}\)의 경우를 생각해 보자. 시점 \(t\)에서 상황 \(s\)에 놓여 있다. 그 때 \(a\)라는 행동을 하라는 조건이 주어졌다. 거기서부터 행위자는 정책 \(\pi\)를 따라 무수히 많은 미래들을 만들어본다. 이후로는 행위자와 정책 \(\pi\)를 동일시하겠다. 그렇게 만들어진 여러 미래들에서 계산되는 누적 보상의 평균을 \(Q^{\pi}\)로 정하는 것이라 받아들이면 된다. 상황의 가치 함수 \(V^{\pi}\)도 마찬가지다. 상황 \(s\)에서 정책 \(\pi\)를 따라 무수히 많은 미래를 만들고 누적 보상의 평균을 계산한다. 처음 행동까지 정책 \(\pi\)에게 맡겨 버린다.</p>

<p>당연한 이야기지만 한 가지 짚고 넘어가야 할 것은 \(s\)와 \(a\)는 조건으로 주어지는 데이터라는 점이다. 정책 \(\pi\)를 고수할 때 현실적으로 맞닥뜨릴 일이 없는 \(s\)와 \(a\)에 대해서도 이론상 \(Q^{\pi}(s, a)\) 는 정의된다. 강원도 토박이 철수(\(\pi\))가 세렝게티 한복판(\(s\))에서 삼겹살을 구워먹는게(\(a\)) 얼마나 현명한 일인지(\(Q^{\pi}(s, a)\)) 짐작이야 해 볼 수 있다는 말이다. 물론 \(Q^{\pi}(s, a)\)를 추정해보고 싶다면? 세렝게티 삼겹살 파티에 철수를 데려다 놓고 미래를 관찰해보는 것이 현재로서는 유일한 해법이다. 타임머신이 있는 경우에는 여러번 관찰해보면 더 좋다. 안타깝다.</p>

<p>만약 신경망으로 \(Q^{\pi}(s, a)\)를 표현하고 싶다면 간단하다. \(s\), \(a\)를 입력으로, 이후 수많은 미래에서 계산한 누적 보상들의 평균을 정답으로 두고 학습시키면 된다. 단순한 지도학습이다. 미래를 여러 번 관측할 수 있다면. 그러나 인과율을 존중하자. 타임 머신은 없다. 현실적으로는 단 하나의 미래에서 얻은 누적 보상을 정답으로 사용하는 것이 최선이다. 그래도 작동을 퍽 잘 한다.</p>

<p>정책 \(\pi\)를 따를 때의 가치 함수 \(Q^{\pi}\)를 알고 싶은 상황이다. 그러니 정답을 만들 때 필요한 미래는 반드시 \(\pi\)가 만든 미래여야만 한다는 사실에 유의하자. 가령 다른 정책 \(\pi^{\beta}\)가 만든 미래를 이용해 정답을 만든다면 \(Q^{\pi^{\beta}}\)를 알게 되는 셈이다. 아무런 의미가 없다.</p>

<p><a href="/rl-story-1/">이전 포스팅</a>에서 이야기 했듯이 \(Q^{\pi}\)를 알고 있다면 \(\pi\)를 교정할 때 더 나은 기여도 할당이 가능하다. 이를 알고리즘으로 써 보자.</p>

<ol>
  <li>정책 \(\pi\)를 이용해 데이터를 모은다.</li>
  <li>정책 \(\pi\)의 가치 함수 \(Q^{\pi}\)를 추정한다.</li>
  <li>추정한 가치함수 \(Q^{\pi}\)를 근거로 정책 \(\pi\)의 행동을 교정한다.</li>
</ol>

<p>그런데 3단계에서 정책 \(\pi\)가 살짝이라도 변하는 순간 가치 함수 \(Q^{\pi}\)는 무용지물이 된다. 변한 정책의 가치 함수를 다시 찾아야 한다. 절망적이다. 그러나 경사 하강법을 사용하는 경우에는 정책이 크게 변하지 않는다. 그냥 \(\pi\)와 \(Q^{\pi}\)를 한 번씩 번갈아 학습시키면 충분하다. 바뀌기 전 정책에 대한 가치 함수를 초깃값 삼아 바뀐 후의 정책에 대한 가치 함수를 찾는 셈이다. 이런 미묘한 부분들을 꼼꼼히 짚고 넘어가야 안 헷갈린다.</p>

<p>정책 \(\pi\)와 가치 함수 \(Q^{\pi}\)을 표현하는데는 독립된 두 신경망을 사용한다. 그러니 태생적으로 불안정 할 수밖에 없다. 지난 이야기에서 논했던대로 \(V^{\pi}\) 까지 있다면 더 나은 신뢰 할당을 할 수 있다. 세 번째 신경망을 도입하면 된다. \(V^{\pi}\)를 표현하는 신경망을 어떻게 학습시킬지는 스스로 생각해보자. \(Q^{\pi}\)와 크게 다르지 않다.</p>

<ol>
  <li>정책 \(\pi\)를 이용해 데이터를 모은다.</li>
  <li>정책 \(\pi\)의 가치 함수 \(Q^{\pi}\)와 \(V^{\pi}\)를 추정한다.</li>
  <li>추정한 가치함수 \(Q^{\pi}\)와 \(V^{\pi}\)를 근거로 정책 \(\pi\)의 행동을 교정한다.</li>
</ol>

<p>그러나 \(Q^{\pi}\)와 \(V^{\pi}\)를 표현하는 신경망을 따로 가지고 있는 것은 낭비다. 다소 부정확할지라도 \(Q^{\pi}\)로 \(V^{\pi}\)를 표현하거나 \(V^{\pi}\)를 \(Q^{\pi}\)로 표현해 방법론을 단순하게 만드는 것이 좋다. 이런 맥락에서는 \(Q^{\pi}\)를 \(V^{\pi}\)로 표현하는 것이 일반적이다. 이렇게 말이다.</p>

\[Q^{\pi}(s_t, a_t) \approx r_t + V^{\pi}(s_{t+1})\]

<p>먼저 원칙적인 이야기를 해 보자. \(s_t\)에서 \(a_t\)를 했을 때 어떤 미래 \(s_{t+1}\)를 마주하게 될지는 모른다. 그러나 적어도 행위자의 소관은 아니다. 이미 행동을 했으므로 행위자는 손을 놓고 \(s_{t+1}\)를 맞이한다.</p>

<p>좌변과 우변은 다른 양이다. \(s_t\)에서 \(a_t\)를 했을지라도 놓이게 되는 다음 상태 \(s_{t+1}\)은 확률적으로 달라질 수 있기 때문이다. 정확히는 아래와 같이 기술해야 옳다.</p>

\[Q^{\pi}(s_t, a_t) = \mathbb{E}_{s_{t+1}} \left[ r_t + V^{\pi}(s_{t+1})\right]\]

<p>타임머신이 있다면 \(s_t\)에서 \(a_t\)를 했을 때 놓일 수 있는 다음 상태 \(s_{t+1}\)을 여러번 관찰한 후 \(r+V^{\pi}(s_{t+1})\)의 평균을 계산해 \(Q^{\pi}(s_t, a_t)\)를 근사하는데 이용해도 되지만 역시 그럴 필요까지는 없다.</p>

<p>정리해보자. 다음 괄호 안에 무엇을 넣으면 적절한 기여도 할당을 통해 더 나은 행동 교정이 가능할지 생각해 보았다.</p>

\[\sum_t (\ \ \ ) \log \pi_{\theta}(a_t|s_t)\]

<p>첫 번째로는 누적 보상의 기댓값인 \(Q^{\pi}(s_t, a_t)\)가 있었고, 두 번째로는 행위의 좋고 나쁨을 상대적으로 고려할 수 있도록 \(V^{\pi}(s_t)\)와의 차이를 계산한 \(Q^{\pi}(s_t, a_t)\) \(- V^{\pi}(s_t)\)가 있었다. 두 번째 양을 Advantage라 부르고 이 양을 이용한 Policy Gradient 방법론에는 Advantage Actor-Critic (A2C)라는 이름이 붙어있다. A2C를 CPU의 여러 쓰레드에서 병렬적 + 비동기적으로 돌리자는 아이디어가 그 유명한 <a href="https://arxiv.org/abs/160201783">A3C</a> 되시겠다.</p>

\[A^{\pi}(s_t, a_t) = Q^{\pi}(s_t, a_t)  - V^{\pi}(s_t)\]

<p>현실적으로는 Advantage의 근사로서 \(r_t + V(s_{t+1}) - V(s_{t})\)를 사용한다고 했다. 이를 조금 더 일반화하면 여러 방식으로 Advantage를 표현할 수 있다. 관심있는 독자는 <a href="https://arxiv.org/abs/1506.02438">GAE</a>를 읽어보면 좋다. 이후 맥락에서 \({\hat{A_t}}\)라는 기호가 나온다면 무언가 한 가지 방법으로 Advantage를 추정한 값이라 이해하면 된다.</p>

<p>다음 포스팅에서는 가치 함수를 추론하는 또 다른 방법에 대해 다룬다. 그러나 가치 함수를 추론하는 방식이 달라진다고 해서 A2C라는 방법론의 구조가 달라지는 것은 아니라는 사실을 기억하자.</p>
:ET