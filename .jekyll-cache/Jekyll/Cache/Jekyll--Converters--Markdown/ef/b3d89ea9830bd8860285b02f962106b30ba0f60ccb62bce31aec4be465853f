I"&<p>Q-Learning 계열의 방법론들은 최적의 정책이 할 행동을 상상한다. 그렇게 상상한 \(a'\)를 가지고 시간차 학습을 한다. \(Q(s, a)\)와 \(r+\gamma Q(s', a')\)가 비슷해지도록 학습해 나간다. \((s,\) \(a,\) \(r,\) \(s')\)는 진짜 데이터, \(a'\)는 상상한 데이터이기에 Off-Policy라는 성질이 발생한다.</p>

<p>그러나 한 가지 문제가 떠오른다. 최적의 정책이 할 행동을 상상하는 주체는 학습 중인 가치 함수 \(\hat{Q}\)이다. 그러니 당연하게도 정확하지 않다. 이 오차가 어떤 문제를 야기하는지 따져보자.</p>

<h2 id="스톡데일-패러독스">스톡데일 패러독스</h2>

<p>학습 중인 \(\hat{Q}\)를 근거로 가장 좋은 \(a'\)를 선택한다.</p>

\[a' = \mathrm{argmax}_a \hat{Q}(s', a)\]

<p>그렇게 선택한 \(a'\)로 \(r+\gamma \hat{Q}(s', a')\)를 계산해 이것이 마치 정답인 양 \(\hat{Q}(s,a)\)와 가까워지게 만든다. 함정은 \(a' = \mathrm{argmax}_a \hat{Q}(s', a)\)에 있다. 한 가지 예시를 들어보자. 가령 가능한 행동들이 \(a_1,\) \(a_2,\) \(a_3\) 세 가지이고, 최적 정책의 가치 함수가 \(s'\)에서 아래와 같다고 해 보자.</p>

\[Q^*(s', a_1)=2.6\]

\[Q^*(s', a_2)=2\]

\[Q^*(s', a_3)=2.5\]

<p>그러니 \(s'\)에서 최적의 정책은 \(a_1\)만을 할 테다. 그 때의 가치는 2.6이다. 그런데 문제는 우리가 \(Q^*\)를 알지 못한다는데 있다. 아직 학습 중이기 때문일 수도 있지만 신경망을 이용해 \(Q^*\)를 근사하려 하고 있는 상황이기 때문에 <a href="https://en.wikipedia.org/wiki/Uncertainty_quantification#Aleatoric_and_epistemic_uncertainty">본질적으로 부정확</a>할 수 밖에 없다. 가령 약간의 오차가 개입했다고 해 보자.</p>

\[\hat{Q}(s', a_1)=2.59\]

\[\hat{Q}(s', a_2)=1.7\]

\[\hat{Q}(s', a_3)=2.64\]

<p>\(\hat{Q}\)를 근거로 판단한 최적의 행동은 \(a_3\)이고, 그 때의 추정 가치는 \(2.64\)이다. 그러나 오차가 없었다면 최적의 행동은 \(a_1\)이고, 그 때의 가치는 \(2.6\)이라 추정했어야 한다. 이는 \(\hat{Q}(s,a)\)의 정답 역할을 할 \(r+\gamma \hat{Q}(s', a')\)를 참값보다 크게 추정하는 잘못된 판단으로 이어진다. 항상 정답을 참값보다 크게 추정하니 \(\hat{Q}\)가 전반적으로 참값 \(Q^*\)에 비해 높게 학습되는 편향을 야기한다.</p>

<p>\(a_1\)과 \(a_2\)의 가치는 참값보다 낮게 추정했다. 전체적인 오차는 낮은 방향으로 작용한 셈이다.  그럼에도 불구하고 편향이 발생한다는 사실이 재밌다. 이러한 편향을 Maximization Bias라 부른다. 불공평한 녀석이다.</p>

<p>이러한 편향 탓에 학습한 \(\hat{Q}\)와 참값 \(Q^{*}\)이 달라진다. 그러나 생각보다 큰 문제는 아닐 수 있다. \(Q^{*}\)가 아닌 \(\pi^{*}\)를 추정하는게 우리의 목적이기 때문이다. 행위자에게는 행동의 근거가 될 정책만 있으면 충분하다. 물론 \(Q^{*}\)를 정확히 추정하면 좋다. 그러나 참값보다 일관적으로 높게 학습한다면, 예컨데 어떤 상수 \(C\)에 대해 \(Q^{*} + C\)를 학습하는 셈이라면 상관없다. 결국 가치 함수로부터 유도되는 정책은 같다.</p>

\[\pi^{*} = \mathrm{argmax} _a Q^{*} (s, a) +C=\mathrm{argmax} _a Q^{*} (s, a)\]

<figure class="image">
    <img src="/images/funcoolsexy.gif" alt="'그것이 가치 함수이니까.'" />
    <figcaption class="caption">'그것이 가치 함수이니까.'</figcaption>
  </figure>

<p>당연하게도 굳이 이렇게 일관적으로 잘못할 이유가 없다. 그러니 학습한 가치 함수로부터 유도되는 정책이 최적 정책과 달라진다. 따라서 Q-Learning 기반의 방법론들은 이러한 편향을 해결해 줄 처방이 필요하다.</p>

<figure class="image">
    <img src="/images/rl-story-5-1.png" alt="van Hasselt et al., 2015" />
    <figcaption class="caption">van Hasselt et al., 2015</figcaption>
  </figure>

<p>위 도식은 가치 함수가 추정한 가치, 아래 도식은 그러한 가치 함수로부터 정책을 유도해 행동했을 때 실제로 얻은 누적 보상을 나타낸다. 말했듯이 최적 정책의 가치 함수를 참값보다 높게 측정한다는 사실 자체가 큰 문제는 아닐 수 있다. 그러나 그런 현상이 시작되는 순간부터 실제 성능은 감소하기 시작한다. 낙관주의의 비극이다.</p>

<p>이제 등장할 Double DQN의 경우에는 이러한 편향이 현저히 적어진다. 편향이 적을 뿐 아니라 실제 환경에서 얻는 누적 보상도 높아지고 학습이 안정화된다. 편향을 해결하면 달콤한 보상이 따르리라는 암시렸다.</p>

<h2 id="쌍성계">쌍성계</h2>

<p>편향이 왜 발생하는지 생각해보자. \(\hat{Q}\)는 무엇을 근거로 \(a_3\)이 최적의 행동이라고 판단했는가? 말장난 같지만 \(\hat{Q}\) 자신이다. 그러니 \(\hat{Q}\)는 억울하다. 최적의 행동을 말하래서 말했고 그 근거를 대래서 댔다. 편향이 발생하는건 \(\hat{Q}\)의 잘못이 아니다.</p>

<p>비유를 들어보자. 가령 두 신경망을 학습시킨다고 해 보자. 신경망이 작동하게끔 하기 위해 우선은 학습 데이터에 끼워 맞춰야 한다. 이미 학습 데이터에 맞추었기 때문에 당연히 일반적인 데이터보다 학습 데이터에 더 잘 맞을테다. 그러니 학습 데이터를 지표로 사용한다면 실제보다 낙관적인 결론에 이른다. 그러니 학습 데이터와 상호 배타적인 검증 데이터를 들고와 모형을 선택한다.</p>

<p>거창한 비유를 들었으나 사실 당연한 이야기다. 놀이공원에서 롤러코스터를 타는 사람들의 키 평균은 한국인 평균 키보다 높다. 애초에 키가 작은 사람은 롤러코스터를 탈 수 없기 때문이다. 한국인 평균 키의 추정치가 필요한 자리에 이를 사용한다면 <a href="https://en.wikipedia.org/wiki/Sampling_bias">편향</a>이 생긴다.</p>

<p>마찬가지다. \(\hat{Q}\)에게 최적의 행동을 물었다면 그것으로 끝내야 한다. 그 행동의 가치를 다시금 \(\hat{Q}\)에게 물으면 필연적으로 참값보다 높은 값을 이야기하는 경향이 발생한다. \(\hat{Q}\)가 최적이라고 판단했던 행동의 가치를 객관적으로 일러줄 제 2의 가치 함수가 필요하다.</p>

<details id="inside">
<summary>Talk is cheap. Show me the code. ┓ </summary>
<div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">copy</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"CartPole-v1"</span><span class="p">)</span>

<span class="n">STATE_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">ACTION_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">0.03</span>
<span class="n">TAU</span> <span class="o">=</span> <span class="mf">0.005</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">array</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">store</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)),</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">batch_indices</span><span class="p">]</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="s">'states'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span>
                 <span class="s">'actions'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                 <span class="s">'next_states'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span>
                 <span class="s">'rewards'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                 <span class="s">'dones'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)}</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span>

<span class="n">q1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">))</span>

<span class="n">q2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">))</span>

<span class="n">q1_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">q1</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">q2_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">q2</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>

<span class="nb">buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">EPSILON</span><span class="p">:</span>
            <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">q1</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)).</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
            
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">chosen_action</span><span class="p">)</span>

        <span class="nb">buffer</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">chosen_action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">buffer</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
        <span class="k">continue</span>
    
    <span class="n">batch</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
    <span class="n">q1_expected_state_action_values</span> <span class="o">=</span> <span class="n">q1</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s">'actions'</span><span class="p">])</span>
    <span class="n">q2_expected_state_action_values</span> <span class="o">=</span> <span class="n">q2</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s">'actions'</span><span class="p">])</span>

    <span class="n">augmented_next_actions</span> <span class="o">=</span> <span class="n">q1</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'next_states'</span><span class="p">]).</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'rewards'</span><span class="p">]</span> <span class="o">+</span> <span class="n">q2</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'next_states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">augmented_next_actions</span><span class="p">)</span> <span class="o">*</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">batch</span><span class="p">[</span><span class="s">'dones'</span><span class="p">])</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">q1_expected_state_action_values</span> <span class="o">-</span> <span class="n">target</span><span class="p">.</span><span class="n">detach</span><span class="p">()).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span> <span class="o">+</span>\
           <span class="p">(</span><span class="n">q2_expected_state_action_values</span> <span class="o">-</span> <span class="n">target</span><span class="p">.</span><span class="n">detach</span><span class="p">()).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
            
    <span class="n">q1_optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">q2_optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="n">q1_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">q2_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="s">"""
    for param, param_target in zip(q.parameters(), q_target.parameters()):
        param_target.data.copy_((1-TAU) * param_target.data+ TAU * param.data)
    """</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">performance</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">q1</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)).</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">chosen_action</span><span class="p">)</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
                <span class="n">performance</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">print</span><span class="p">(</span><span class="s">f"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">th Trial -&gt; </span><span class="si">{</span><span class="n">performance</span><span class="o">/</span><span class="mi">10</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div>    </div>

  </div>
</details>

<p>\(Q_1\)에게 최적의 행동을 고르게 두고, \(Q_2\)로 그 행동에 대한 가치를 추정하게 만든다. 그렇게 계산한 정답으로 \(Q_1\)과 \(Q_2\)를 동시에 학습시킨다. 생각보다 문제가 간단히 해결된다. 원한다면 \(Q_1\)과 \(Q_2\)이 역할을 번갈아 맡도록 구현해도 된다. 자유다.</p>

<p>지난 이야기 말미에 등장했던 Target Network를 잊었다. 원래의 가치 함수를 천천히 따라오는 Target Network로 정답을 계산해 학습 과정을 안정적으로 만들자고 했다. 이 또한 구현해주자. 두 신경망 모두의 Target Network를 만들어 주어야 한다. 네 신경망이 조화를 이루며 작동한다. 다소 부담스럽다.</p>

<details id="inside">
<summary>Talk is cheap. Show me the code. ┓ </summary>
<div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">copy</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"CartPole-v1"</span><span class="p">)</span>

<span class="n">STATE_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">ACTION_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">0.03</span>
<span class="n">TAU</span> <span class="o">=</span> <span class="mf">0.005</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">array</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">store</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)),</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">batch_indices</span><span class="p">]</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="s">'states'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span>
                 <span class="s">'actions'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                 <span class="s">'next_states'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span>
                 <span class="s">'rewards'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                 <span class="s">'dones'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)}</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span>

<span class="n">q1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">))</span>

<span class="n">q1_target</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">q1</span><span class="p">)</span>

<span class="n">q2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">))</span>

<span class="n">q2_target</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">q2</span><span class="p">)</span>

<span class="n">q1_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">q1</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">q2_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">q2</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>

<span class="nb">buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">EPSILON</span><span class="p">:</span>
            <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">q1</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)).</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
            
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">chosen_action</span><span class="p">)</span>

        <span class="nb">buffer</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">chosen_action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">buffer</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
        <span class="k">continue</span>
    
    <span class="n">batch</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
    <span class="n">q1_expected_state_action_values</span> <span class="o">=</span> <span class="n">q1</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s">'actions'</span><span class="p">])</span>
    <span class="n">q2_expected_state_action_values</span> <span class="o">=</span> <span class="n">q2</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s">'actions'</span><span class="p">])</span>

    <span class="n">augmented_next_actions</span> <span class="o">=</span> <span class="n">q1_target</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'next_states'</span><span class="p">]).</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'rewards'</span><span class="p">]</span> <span class="o">+</span> <span class="n">q2_target</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'next_states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">augmented_next_actions</span><span class="p">)</span> <span class="o">*</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">batch</span><span class="p">[</span><span class="s">'dones'</span><span class="p">])</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">q1_expected_state_action_values</span> <span class="o">-</span> <span class="n">target</span><span class="p">.</span><span class="n">detach</span><span class="p">()).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span> <span class="o">+</span>\
           <span class="p">(</span><span class="n">q2_expected_state_action_values</span> <span class="o">-</span> <span class="n">target</span><span class="p">.</span><span class="n">detach</span><span class="p">()).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
            
    <span class="n">q1_optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">q2_optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="n">q1_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">q2_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">param_target</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">q1</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">q1_target</span><span class="p">.</span><span class="n">parameters</span><span class="p">()):</span>
        <span class="n">param_target</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">copy_</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">TAU</span><span class="p">)</span> <span class="o">*</span> <span class="n">param_target</span><span class="p">.</span><span class="n">data</span><span class="o">+</span> <span class="n">TAU</span> <span class="o">*</span> <span class="n">param</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">param_target</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">q2</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">q2_target</span><span class="p">.</span><span class="n">parameters</span><span class="p">()):</span>
        <span class="n">param_target</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">copy_</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">TAU</span><span class="p">)</span> <span class="o">*</span> <span class="n">param_target</span><span class="p">.</span><span class="n">data</span><span class="o">+</span> <span class="n">TAU</span> <span class="o">*</span> <span class="n">param</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">performance</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">q1</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)).</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">chosen_action</span><span class="p">)</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
                <span class="n">performance</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">print</span><span class="p">(</span><span class="s">f"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">th Trial -&gt; </span><span class="si">{</span><span class="n">performance</span><span class="o">/</span><span class="mi">10</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div>    </div>

  </div>
</details>

<p>Q-Learning에서 발생하는 이런 편향은 <a href="https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf">오래 전</a>부터 <a href="https://papers.nips.cc/paper/3964-double-q-learning">알려져 있던</a> 현상이다.  DQN에서도 마찬가지로 이런 현상이 관측되었고, 얼마 가지 않아 딥마인드의 연구자들에 의해 <a href="https://arxiv.org/abs/1509.06461">Double DQN</a>이라는 이름으로 개선되었다. 이 때 제안된 방법론은 위의 구현체보다는 단순하다. \(Q_2\)를 그냥 \(Q_1\)의 Target Network로 간주한다. 서로 완전히 독립적이지는 않지만 어쨌든 다른 신경망이니 제 2의 신경망까지 도입하진 말자는 아이디어다.</p>

<p>그렇게 할 요량이라면 지난 이야기의 DQN 구현체에서 정말로 단 한 단어만 지우면 된다. 일반적으로 Double DQN이라고 부르는건 아래의 구현체다. 그러나 더 일반적인 구현체를 먼저 만들어본 이유가 있다. 이후 이야기할 TD3, 내지는 SAC의 구현체와 얼개가 같기 때문이다.</p>

<details id="inside">
<summary>Talk is cheap. Show me the code. ┓ </summary>
<div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">copy</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"CartPole-v1"</span><span class="p">)</span>

<span class="n">STATE_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">ACTION_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">0.03</span>
<span class="n">TAU</span> <span class="o">=</span> <span class="mf">0.005</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">array</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">store</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)),</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">batch_indices</span><span class="p">]</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="s">'states'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span>
                 <span class="s">'actions'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                 <span class="s">'next_states'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span>
                 <span class="s">'rewards'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                 <span class="s">'dones'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)}</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span>

<span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">STATE_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ACTION_DIM</span><span class="p">))</span>

<span class="n">q_target</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="nb">buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">EPSILON</span><span class="p">:</span>
            <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)).</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
            
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">chosen_action</span><span class="p">)</span>

        <span class="nb">buffer</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">chosen_action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">buffer</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
        <span class="k">continue</span>
    
    <span class="n">batch</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
    <span class="n">expected_state_action_values</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s">'actions'</span><span class="p">])</span>

    <span class="n">augmented_next_actions</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'next_states'</span><span class="p">]).</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'rewards'</span><span class="p">]</span> <span class="o">+</span> <span class="n">q_target</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">'next_states'</span><span class="p">]).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">augmented_next_actions</span><span class="p">)</span> <span class="o">*</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">batch</span><span class="p">[</span><span class="s">'dones'</span><span class="p">])</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">expected_state_action_values</span> <span class="o">-</span> <span class="n">target</span><span class="p">.</span><span class="n">detach</span><span class="p">()).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">param_target</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">q_target</span><span class="p">.</span><span class="n">parameters</span><span class="p">()):</span>
        <span class="n">param_target</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">copy_</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">TAU</span><span class="p">)</span> <span class="o">*</span> <span class="n">param_target</span><span class="p">.</span><span class="n">data</span><span class="o">+</span> <span class="n">TAU</span> <span class="o">*</span> <span class="n">param</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">performance</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">chosen_action</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)).</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">chosen_action</span><span class="p">)</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
                <span class="n">performance</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">print</span><span class="p">(</span><span class="s">f"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">th Trial -&gt; </span><span class="si">{</span><span class="n">performance</span><span class="o">/</span><span class="mi">10</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div>    </div>

  </div>
</details>

<h2 id="unplugged-작성중">Unplugged (작성중)</h2>

<p>우리의 목표인 오프라인 강화학습에서는 이러한 편향이 더 아프게 다가온다. 대부분의 오프라인 강화학습 방법론들은 Q-Learning 계열의 방법론들을 골자로 한다. Q-Learning은 Off-Policy이니 오프라인 강화학습을 위한 응당 합리적인 선택지인 듯 보인다. 그럼 Q-Learning을 바로 적용하면 안 되는걸까?</p>

<p>그러니 미지의 정책 \(\pi^{\beta}\)가 만든 순서쌍 \((s,\) \(a,\) \(r,\) \(s')\)들을 이용한다. 일반적으로 수집하기 간단한 데이터다.</p>
:ET